{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Installing Codacy on Kubernetes \u00b6 This documentation guides you on how to install Codacy on Kubernetes or MicroK8s. To install Codacy you must complete these main steps: Setting up the system requirements Ensure that your infrastructure meets the hardware and system requirements to run Codacy. Installing Codacy Install Codacy on the cluster using our Helm chart that includes all the necessary components and dependencies. Configuring Codacy Configure integrations with Git providers and set up monitoring. The next sections include detailed instructions on how to complete each step of the installation process. Make sure that you complete each step before advancing to the next one. 1. Setting up the system requirements \u00b6 Before you start, you must prepare and and provision the database server and Kubernetes or MicroK8s cluster that will host Codacy. Carefully review and set up the system requirements to run Codacy by following the instructions on the page below: System requirements Optionally, you can follow one of the guides below to quickly create a new Kubernetes or MicroK8s cluster that satisfies the characteristics described in the system requirements: Creating an Amazon EKS cluster Creating an AKS cluster Creating a MicroK8s cluster 2. Installing Codacy \u00b6 Install Codacy on an existing cluster using our Helm chart: Make sure that you have the following tools installed on your machine: kubectl within one minor version difference of your cluster Important If you are using MicroK8s you don't need to install kubectl because you will execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, check how to create an alias for kubectl . Helm client version 2.16.3 Create a cluster namespace called codacy that will group all resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials provided by Codacy together with your license to a cluster Secret. This is necessary because some Codacy Docker images are currently private. Substitute <docker_username> and <docker_password> with the Docker registry username and password and run the following command: kubectl create secret docker-registry docker-credentials \\ --docker-username = <docker_username> \\ --docker-password = <docker_password> \\ --namespace codacy Download the template file values-production.yaml and use a text editor of your choice to edit the value placeholders as described in the comments. You can download the template file by running: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-production.yaml Create an address record on your DNS provider mapping the hostname you used in the previous step to the IP address of your Ingress controller. Important If you are using MicroK8s you must map the hostname to the public IP address of the machine running MicroK8s. Add Codacy's chart repository to your Helm client and install the Codacy chart using the file values-production.yaml created previously. Important If you are using MicroK8s don't forget to use the file values-microk8s.yaml together with the file values-production.yaml as described here . To do this, uncomment the last line before running the command below. helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --values values-production.yaml \\ # --values values-microk8s.yaml By now all the Codacy pods should be starting in the cluster. Run the following command and wait for all the pods to have the status Running to verify this: $ kubectl get pods -n codacy NAME READY STATUS RESTARTS AGE codacy-activities-6d9db9499-stk2k 1 /1 Running 2 8m57s codacy-activitiesdb-0 1 /1 Running 0 8m57s codacy-api-f7897b965-fgn67 1 /1 Running 0 8m57s codacy-api-f7897b965-kkqsx 1 /1 Running 0 8m57s codacy-core-7bcf697968-85tl6 1 /1 Running 0 8m57s codacy-crow-7c957d45f6-b8zp2 1 /1 Running 2 8m57s codacy-crowdb-0 1 /1 Running 0 8m57s codacy-engine-549bcb69d9-cgrqf 1 /1 Running 1 8m57s codacy-engine-549bcb69d9-sh5f4 1 /1 Running 1 8m57s codacy-fluentdoperator-x5vr2 2 /2 Running 0 8m57s codacy-hotspots-api-b7b9db896-68gxx 1 /1 Running 2 8m57s codacy-hotspots-worker-76bb45b4d6-8gz45 1 /1 Running 3 8m57s codacy-hotspotsdb-0 1 /1 Running 0 8m57s codacy-listener-868b784dcf-npdfh 1 /1 Running 0 8m57s codacy-listenerdb-0 1 /1 Running 0 8m57s codacy-minio-7cfdc7b4f4-254gz 1 /1 Running 0 8m57s codacy-nfsserverprovisioner-0 1 /1 Running 0 8m57s codacy-portal-774d9fc596-rwqj5 1 /1 Running 2 8m56s codacy-rabbitmq-ha-0 1 /1 Running 0 8m57s codacy-ragnaros-69459775b5-hmj4d 1 /1 Running 3 8m57s codacy-remote-provider-service-8fb8556b-rr4ws 1 /1 Running 0 8m56s codacy-worker-manager-656dbf8d6d-n4j7c 1 /1 Running 0 8m57s 3. Configuring Codacy \u00b6 After successfully installing Codacy on your cluster, you are now ready to perform the post-install configuration steps: Use a browser to navigate to the Codacy hostname previously configured on the file values-production.yaml . Follow Codacy's onboarding process, which will guide you through the following steps: Creating an administrator account Configuring one or more of the following supported integrations: GitHub Cloud GitHub Enterprise GitLab Cloud GitLab Enterprise Bitbucket Cloud Bitbucket Server Email Creating an initial organization Inviting users to Codacy As a last step we recommend that you set up monitoring on your Codacy instance.","title":"Installing Codacy on Kubernetes"},{"location":"#installing-codacy-on-kubernetes","text":"This documentation guides you on how to install Codacy on Kubernetes or MicroK8s. To install Codacy you must complete these main steps: Setting up the system requirements Ensure that your infrastructure meets the hardware and system requirements to run Codacy. Installing Codacy Install Codacy on the cluster using our Helm chart that includes all the necessary components and dependencies. Configuring Codacy Configure integrations with Git providers and set up monitoring. The next sections include detailed instructions on how to complete each step of the installation process. Make sure that you complete each step before advancing to the next one.","title":"Installing Codacy on Kubernetes"},{"location":"#1-setting-up-the-system-requirements","text":"Before you start, you must prepare and and provision the database server and Kubernetes or MicroK8s cluster that will host Codacy. Carefully review and set up the system requirements to run Codacy by following the instructions on the page below: System requirements Optionally, you can follow one of the guides below to quickly create a new Kubernetes or MicroK8s cluster that satisfies the characteristics described in the system requirements: Creating an Amazon EKS cluster Creating an AKS cluster Creating a MicroK8s cluster","title":"1. Setting up the system requirements"},{"location":"#2-installing-codacy","text":"Install Codacy on an existing cluster using our Helm chart: Make sure that you have the following tools installed on your machine: kubectl within one minor version difference of your cluster Important If you are using MicroK8s you don't need to install kubectl because you will execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, check how to create an alias for kubectl . Helm client version 2.16.3 Create a cluster namespace called codacy that will group all resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials provided by Codacy together with your license to a cluster Secret. This is necessary because some Codacy Docker images are currently private. Substitute <docker_username> and <docker_password> with the Docker registry username and password and run the following command: kubectl create secret docker-registry docker-credentials \\ --docker-username = <docker_username> \\ --docker-password = <docker_password> \\ --namespace codacy Download the template file values-production.yaml and use a text editor of your choice to edit the value placeholders as described in the comments. You can download the template file by running: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-production.yaml Create an address record on your DNS provider mapping the hostname you used in the previous step to the IP address of your Ingress controller. Important If you are using MicroK8s you must map the hostname to the public IP address of the machine running MicroK8s. Add Codacy's chart repository to your Helm client and install the Codacy chart using the file values-production.yaml created previously. Important If you are using MicroK8s don't forget to use the file values-microk8s.yaml together with the file values-production.yaml as described here . To do this, uncomment the last line before running the command below. helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --values values-production.yaml \\ # --values values-microk8s.yaml By now all the Codacy pods should be starting in the cluster. Run the following command and wait for all the pods to have the status Running to verify this: $ kubectl get pods -n codacy NAME READY STATUS RESTARTS AGE codacy-activities-6d9db9499-stk2k 1 /1 Running 2 8m57s codacy-activitiesdb-0 1 /1 Running 0 8m57s codacy-api-f7897b965-fgn67 1 /1 Running 0 8m57s codacy-api-f7897b965-kkqsx 1 /1 Running 0 8m57s codacy-core-7bcf697968-85tl6 1 /1 Running 0 8m57s codacy-crow-7c957d45f6-b8zp2 1 /1 Running 2 8m57s codacy-crowdb-0 1 /1 Running 0 8m57s codacy-engine-549bcb69d9-cgrqf 1 /1 Running 1 8m57s codacy-engine-549bcb69d9-sh5f4 1 /1 Running 1 8m57s codacy-fluentdoperator-x5vr2 2 /2 Running 0 8m57s codacy-hotspots-api-b7b9db896-68gxx 1 /1 Running 2 8m57s codacy-hotspots-worker-76bb45b4d6-8gz45 1 /1 Running 3 8m57s codacy-hotspotsdb-0 1 /1 Running 0 8m57s codacy-listener-868b784dcf-npdfh 1 /1 Running 0 8m57s codacy-listenerdb-0 1 /1 Running 0 8m57s codacy-minio-7cfdc7b4f4-254gz 1 /1 Running 0 8m57s codacy-nfsserverprovisioner-0 1 /1 Running 0 8m57s codacy-portal-774d9fc596-rwqj5 1 /1 Running 2 8m56s codacy-rabbitmq-ha-0 1 /1 Running 0 8m57s codacy-ragnaros-69459775b5-hmj4d 1 /1 Running 3 8m57s codacy-remote-provider-service-8fb8556b-rr4ws 1 /1 Running 0 8m56s codacy-worker-manager-656dbf8d6d-n4j7c 1 /1 Running 0 8m57s","title":"2. Installing Codacy"},{"location":"#3-configuring-codacy","text":"After successfully installing Codacy on your cluster, you are now ready to perform the post-install configuration steps: Use a browser to navigate to the Codacy hostname previously configured on the file values-production.yaml . Follow Codacy's onboarding process, which will guide you through the following steps: Creating an administrator account Configuring one or more of the following supported integrations: GitHub Cloud GitHub Enterprise GitLab Cloud GitLab Enterprise Bitbucket Cloud Bitbucket Server Email Creating an initial organization Inviting users to Codacy As a last step we recommend that you set up monitoring on your Codacy instance.","title":"3. Configuring Codacy"},{"location":"requirements/","text":"System requirements \u00b6 Before installing Codacy you must ensure that you have the following infrastructure correctly provisioned and configured: Kubernetes or MicroK8s cluster PostgreSQL server The next sections describe in detail how to set up these prerequisites. Kubernetes or MicroK8s cluster setup \u00b6 The cluster running Codacy must satisfy the following requirements: The infrastructure hosting the cluster must be provisioned with the hardware requirements described below The orchestration platform managing the cluster must be one of: Kubernetes version 1.14.* or 1.15.* MicroK8s version 1.15 Tiller, the server part of Helm version 2.16 , must be installed in the cluster The NGINX Ingress controller must be installed and correctly set up in the cluster Cluster hardware requirements \u00b6 The high-level architecture described in the next section is important in understanding how Codacy uses and allocates hardware resources. Below we also provide guidance on resource provisioning for typical scenarios . For a custom hardware resource recommendation, please contact us at support@codacy.com . Codacy architecture \u00b6 You can look at Codacy separately as two parts: The \"Platform\" contains the UI and other components important to treat and show results The \"Analysis\" is the swarm of workers that run between one and four linters simultaneously, depending on factors such as the number of files or the programming languages used in your projects Since all components are running on a cluster, you can increase the number of replicas in every deployment to give you more resilience and throughput, at a cost of increased resource usage. The following is a simplified overview of how to calculate resource allocation for the \"Platform\" and the \"Analysis\": Component vCPU Memory Platform (1 replica per component) 4 8 GB Analysis (1 Analysis Worker + up to 4 linters) 5 (per Analysis Worker) 10 GB (per Analysis Worker) Standard cluster provisioning \u00b6 The resources recommended on the following table are based on our experience and are also the defaults in the values-production.yaml file, which you might need to adapt taking into account your use case. As described in the section above, Codacy's architecture allows scaling the \"Analysis\" part of the platform, meaning that the resources needed for Codacy depend mainly on the rate of commits done by your team that Codacy will be analyzing. Important For MicroK8s clusters we added an extra 1.5 vCPU and 1.5 GB memory to the \"Platform\" to account for the MicroK8s platform itself running on the same machine. Installation type Replicas per component Max. concurrent analysis Platform resources Analysis resources ~ Total resources Kubernetes Small Installation 1 2 4 vCPUs 8 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM Kubernetes Medium Installation (default) 2 4 8 vCPUs 16 GB RAM 20 vCPUs 40 GB RAM 32 vCPUs 64 GB RAM Kubernetes Big Installation 2+ 10+ 8+ vCPUs 16+ GB RAM 50+ vCPUs 100+ GB RAM 60+ vCPUs 110+ GB RAM MicroK8s Minimum 1 2 5.5 vCPUs 9.5 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM MicroK8s Recommended (default) 1+ 2 11+ vCPUs 20+ GB RAM 10 vCPUs 20 GB RAM 21+ vCPUs 40+ GB RAM The storage requirements recommended on the following table depend mainly on the number of repositories that Codacy will be analyzing and should be used as a guideline to determine your installation requirements. Component Bundled in the chart? Minimum recommended NFS Yes 200 GB RabbitMQ Yes 8 GB Minio Yes 20 GB PostgreSQL No (external DB recommended) 500 GB+ PostgreSQL server setup \u00b6 Codacy requires a database server to persist data that must satisfy the following requirements: The infrastructure hosting the database server must be provisioned with the hardware requirements described below The DBMS server must be PostgreSQL version 9.6 The PostgreSQL server must be configured to accept connections from the cluster The Codacy databases and a dedicated user must be created using the instructions below Important Google, the developer of Kubernetes, doesn't recommend running database servers on your cluster . As such, consider using a managed solution like Amazon RDS or Google Cloud SQL, or running the PostgreSQL server on a dedicated virtual machine. PostgreSQL hardware requirements \u00b6 The following are the minimum specifications recommended for provisioning the PostgreSQL server: vCPUs Memory Storage Max. concurrent connections 4 8 GB 500 GB+ 300 Preparing PostgreSQL for Codacy \u00b6 Before installing Codacy you must create a set of databases that will be used by Codacy to persist data. We also recommend that you create a dedicated user for Codacy, with access permissions only to the databases that are specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command line client: psql -U postgres -h <PostgreSQL server hostname> Create the dedicated user that Codacy will use to connect to PostgreSQL. Make sure that you change the username and password to suit your security needs: CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; Take note of the username and password you define, as you will require them later to configure the connection from Codacy to the PostgreSQL server. Make sure that you can connect to the PostgreSQL database using the newly created user. For example, using the psql command line client: psql -U codacy -d postgres -h <PostgreSQL server hostname> Create the databases required by Codacy: CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; CREATE DATABASE crow WITH OWNER = codacy ;","title":"System requirements"},{"location":"requirements/#system-requirements","text":"Before installing Codacy you must ensure that you have the following infrastructure correctly provisioned and configured: Kubernetes or MicroK8s cluster PostgreSQL server The next sections describe in detail how to set up these prerequisites.","title":"System requirements"},{"location":"requirements/#kubernetes-or-microk8s-cluster-setup","text":"The cluster running Codacy must satisfy the following requirements: The infrastructure hosting the cluster must be provisioned with the hardware requirements described below The orchestration platform managing the cluster must be one of: Kubernetes version 1.14.* or 1.15.* MicroK8s version 1.15 Tiller, the server part of Helm version 2.16 , must be installed in the cluster The NGINX Ingress controller must be installed and correctly set up in the cluster","title":"Kubernetes or MicroK8s cluster setup"},{"location":"requirements/#cluster-hardware-requirements","text":"The high-level architecture described in the next section is important in understanding how Codacy uses and allocates hardware resources. Below we also provide guidance on resource provisioning for typical scenarios . For a custom hardware resource recommendation, please contact us at support@codacy.com .","title":"Cluster hardware requirements"},{"location":"requirements/#codacy-architecture","text":"You can look at Codacy separately as two parts: The \"Platform\" contains the UI and other components important to treat and show results The \"Analysis\" is the swarm of workers that run between one and four linters simultaneously, depending on factors such as the number of files or the programming languages used in your projects Since all components are running on a cluster, you can increase the number of replicas in every deployment to give you more resilience and throughput, at a cost of increased resource usage. The following is a simplified overview of how to calculate resource allocation for the \"Platform\" and the \"Analysis\": Component vCPU Memory Platform (1 replica per component) 4 8 GB Analysis (1 Analysis Worker + up to 4 linters) 5 (per Analysis Worker) 10 GB (per Analysis Worker)","title":"Codacy architecture"},{"location":"requirements/#standard-cluster-provisioning","text":"The resources recommended on the following table are based on our experience and are also the defaults in the values-production.yaml file, which you might need to adapt taking into account your use case. As described in the section above, Codacy's architecture allows scaling the \"Analysis\" part of the platform, meaning that the resources needed for Codacy depend mainly on the rate of commits done by your team that Codacy will be analyzing. Important For MicroK8s clusters we added an extra 1.5 vCPU and 1.5 GB memory to the \"Platform\" to account for the MicroK8s platform itself running on the same machine. Installation type Replicas per component Max. concurrent analysis Platform resources Analysis resources ~ Total resources Kubernetes Small Installation 1 2 4 vCPUs 8 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM Kubernetes Medium Installation (default) 2 4 8 vCPUs 16 GB RAM 20 vCPUs 40 GB RAM 32 vCPUs 64 GB RAM Kubernetes Big Installation 2+ 10+ 8+ vCPUs 16+ GB RAM 50+ vCPUs 100+ GB RAM 60+ vCPUs 110+ GB RAM MicroK8s Minimum 1 2 5.5 vCPUs 9.5 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM MicroK8s Recommended (default) 1+ 2 11+ vCPUs 20+ GB RAM 10 vCPUs 20 GB RAM 21+ vCPUs 40+ GB RAM The storage requirements recommended on the following table depend mainly on the number of repositories that Codacy will be analyzing and should be used as a guideline to determine your installation requirements. Component Bundled in the chart? Minimum recommended NFS Yes 200 GB RabbitMQ Yes 8 GB Minio Yes 20 GB PostgreSQL No (external DB recommended) 500 GB+","title":"Standard cluster provisioning"},{"location":"requirements/#postgresql-server-setup","text":"Codacy requires a database server to persist data that must satisfy the following requirements: The infrastructure hosting the database server must be provisioned with the hardware requirements described below The DBMS server must be PostgreSQL version 9.6 The PostgreSQL server must be configured to accept connections from the cluster The Codacy databases and a dedicated user must be created using the instructions below Important Google, the developer of Kubernetes, doesn't recommend running database servers on your cluster . As such, consider using a managed solution like Amazon RDS or Google Cloud SQL, or running the PostgreSQL server on a dedicated virtual machine.","title":"PostgreSQL server setup"},{"location":"requirements/#postgresql-hardware-requirements","text":"The following are the minimum specifications recommended for provisioning the PostgreSQL server: vCPUs Memory Storage Max. concurrent connections 4 8 GB 500 GB+ 300","title":"PostgreSQL hardware requirements"},{"location":"requirements/#preparing-postgresql-for-codacy","text":"Before installing Codacy you must create a set of databases that will be used by Codacy to persist data. We also recommend that you create a dedicated user for Codacy, with access permissions only to the databases that are specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command line client: psql -U postgres -h <PostgreSQL server hostname> Create the dedicated user that Codacy will use to connect to PostgreSQL. Make sure that you change the username and password to suit your security needs: CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; Take note of the username and password you define, as you will require them later to configure the connection from Codacy to the PostgreSQL server. Make sure that you can connect to the PostgreSQL database using the newly created user. For example, using the psql command line client: psql -U codacy -d postgres -h <PostgreSQL server hostname> Create the databases required by Codacy: CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; CREATE DATABASE crow WITH OWNER = codacy ;","title":"Preparing PostgreSQL for Codacy"},{"location":"configuration/monitoring/","text":"Monitoring \u00b6 Currently two monitoring solutions are supported: Crow - A simple, lightweight, and built-in monitoring solution. Prometheus + Grafana + Loki - A comprehensive third-party monitoring solution, recommended for more advanced usage. Setting up monitoring using Crow \u00b6 Crow displays information about the projects pending analysis and the jobs currently running on Codacy. The Crow tool is installed alongside Codacy after the helm chart is deployed to the cluster. It can be accessed through the /monitoring path of the url pointing to your Codacy installation, e.g. http://codacy.company.org/monitoring . You must set the global.codacy.crow.url value in your values.yaml file so that anchor links to your projects can be properly established inside crow . For example: global : codacy : crow : url : \"http://codacy.example.com/monitoring\" Configuring your credentials \u00b6 We highly recommend that you define a custom password for the Crow installation. You can do this either through the values.yaml file or through a --set parameter during the Helm installation process. This parameter can be configured as follows: Through a --set parameter: helm upgrade (...options used to install codacy...) --set crow.config.passwordAuth.password=<--- crow password ---> Through the values.yaml file: crow : config : passwordAuth : password : <--- crow password ---> If you have not configured your crow credentials as described above the following default credentials will be used: username : codacy password : C0dacy123 Setting up monitoring using Grafana, Prometheus, and Loki \u00b6 Prometheus is an open-source systems monitoring and alerting toolkit. Logs can be collected using Loki , which is a horizontally-scalable, highly-available, multi-tenant log aggregation system Its data can be visualized with Grafana , a widely used open source analytics and monitoring solution. The following guide covers the basic installation of the components in this monitoring stack. This solution is considerably more resource demanding than Crow, and is recommended only for more advanced usage. Furthermore, its installation, configuration, and management requires a deeper knowledge of Kubernetes as each component must be carefully tweaked to match your specific use case, using as starting point the .yaml values files provided by us. Installing CRDs \u00b6 The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. Start by adding the custom resources required for installing this bundle in your cluster. NOTE: if installing on MicroK8s use microk8s.kubectl instead of kubectl kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml Install Loki \u00b6 Obtain the configuration file for Loki, values-loki.yaml , and install it as described below. While the default storage class setting for Loki persistence should suit most use cases, you may need to adjust it to your specific Kubernetes installation. For instance, for MicroK8s use storageClassName: microk8s-hostpath . helm repo add loki https://grafana.github.io/loki/charts helm upgrade --install --atomic loki loki/loki --version 0 .17.0 \\ --recreate-pods --namespace codacy --values values-loki.yaml Install Promtail \u00b6 Promtail is an agent which ships the contents of local logs to a Loki instance. Obtain its configuration file, values-promtail.yaml , and install it with helm upgrade --install --atomic promtail loki/promtail --version 0 .13.0 \\ --recreate-pods --namespace codacy --values promtail-values.yaml Install Prometheus and Grafana \u00b6 Obtain the Prometheus bundle configuration file, values-prometheus-operator.yaml . You must edit the Grafana password for the admin user in this file. Use the following command to install this bundle on your cluster. helm upgrade --install --atomic monitoring stable/prometheus-operator --version 6 .9.3 \\ --recreate-pods --namespace codacy --values values-prometheus-operator.yaml Follow the Kubernetes documentation to access the Grafana service now running on your cluster, using the method that best suits your use case. Enable Service Dashboards \u00b6 Now that you have Prometheus and Grafana installed you can enable serviceMonitors and grafana_dashboards for Codacy components. To do this create the following configuration file: # values-monitoring.yaml codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Apply this configuration by issuing an helm upgrade while passing these additional values. To do so append --values values-monitoring.yaml --recreate-pods to the command used to install codacy provided in this documentation . helm upgrade ( ...options used to install codacy... ) --values values-monitoring-values.yaml --recreate-pods","title":"Monitoring"},{"location":"configuration/monitoring/#monitoring","text":"Currently two monitoring solutions are supported: Crow - A simple, lightweight, and built-in monitoring solution. Prometheus + Grafana + Loki - A comprehensive third-party monitoring solution, recommended for more advanced usage.","title":"Monitoring"},{"location":"configuration/monitoring/#setting-up-monitoring-using-crow","text":"Crow displays information about the projects pending analysis and the jobs currently running on Codacy. The Crow tool is installed alongside Codacy after the helm chart is deployed to the cluster. It can be accessed through the /monitoring path of the url pointing to your Codacy installation, e.g. http://codacy.company.org/monitoring . You must set the global.codacy.crow.url value in your values.yaml file so that anchor links to your projects can be properly established inside crow . For example: global : codacy : crow : url : \"http://codacy.example.com/monitoring\"","title":"Setting up monitoring using Crow"},{"location":"configuration/monitoring/#configuring-your-credentials","text":"We highly recommend that you define a custom password for the Crow installation. You can do this either through the values.yaml file or through a --set parameter during the Helm installation process. This parameter can be configured as follows: Through a --set parameter: helm upgrade (...options used to install codacy...) --set crow.config.passwordAuth.password=<--- crow password ---> Through the values.yaml file: crow : config : passwordAuth : password : <--- crow password ---> If you have not configured your crow credentials as described above the following default credentials will be used: username : codacy password : C0dacy123","title":"Configuring your credentials"},{"location":"configuration/monitoring/#setting-up-monitoring-using-grafana-prometheus-and-loki","text":"Prometheus is an open-source systems monitoring and alerting toolkit. Logs can be collected using Loki , which is a horizontally-scalable, highly-available, multi-tenant log aggregation system Its data can be visualized with Grafana , a widely used open source analytics and monitoring solution. The following guide covers the basic installation of the components in this monitoring stack. This solution is considerably more resource demanding than Crow, and is recommended only for more advanced usage. Furthermore, its installation, configuration, and management requires a deeper knowledge of Kubernetes as each component must be carefully tweaked to match your specific use case, using as starting point the .yaml values files provided by us.","title":"Setting up monitoring using Grafana, Prometheus, and Loki"},{"location":"configuration/monitoring/#installing-crds","text":"The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. Start by adding the custom resources required for installing this bundle in your cluster. NOTE: if installing on MicroK8s use microk8s.kubectl instead of kubectl kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml","title":"Installing CRDs"},{"location":"configuration/monitoring/#install-loki","text":"Obtain the configuration file for Loki, values-loki.yaml , and install it as described below. While the default storage class setting for Loki persistence should suit most use cases, you may need to adjust it to your specific Kubernetes installation. For instance, for MicroK8s use storageClassName: microk8s-hostpath . helm repo add loki https://grafana.github.io/loki/charts helm upgrade --install --atomic loki loki/loki --version 0 .17.0 \\ --recreate-pods --namespace codacy --values values-loki.yaml","title":"Install Loki"},{"location":"configuration/monitoring/#install-promtail","text":"Promtail is an agent which ships the contents of local logs to a Loki instance. Obtain its configuration file, values-promtail.yaml , and install it with helm upgrade --install --atomic promtail loki/promtail --version 0 .13.0 \\ --recreate-pods --namespace codacy --values promtail-values.yaml","title":"Install Promtail"},{"location":"configuration/monitoring/#install-prometheus-and-grafana","text":"Obtain the Prometheus bundle configuration file, values-prometheus-operator.yaml . You must edit the Grafana password for the admin user in this file. Use the following command to install this bundle on your cluster. helm upgrade --install --atomic monitoring stable/prometheus-operator --version 6 .9.3 \\ --recreate-pods --namespace codacy --values values-prometheus-operator.yaml Follow the Kubernetes documentation to access the Grafana service now running on your cluster, using the method that best suits your use case.","title":"Install Prometheus and Grafana"},{"location":"configuration/monitoring/#enable-service-dashboards","text":"Now that you have Prometheus and Grafana installed you can enable serviceMonitors and grafana_dashboards for Codacy components. To do this create the following configuration file: # values-monitoring.yaml codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Apply this configuration by issuing an helm upgrade while passing these additional values. To do so append --values values-monitoring.yaml --recreate-pods to the command used to install codacy provided in this documentation . helm upgrade ( ...options used to install codacy... ) --values values-monitoring-values.yaml --recreate-pods","title":"Enable Service Dashboards"},{"location":"configuration/integrations/bitbucket-server/","text":"Bitbucket Server \u00b6 Follow the instructions below to set up the Codacy integration with Bitbucket Server: Follow the instructions on creating a Bitbucket Server application link . Edit the file values-production.yaml , set global.githubEnterprise.enabled: \"true\" and define the remaining values as described below and with the information obtained when you created the GitHub App: bitbucketEnterprise : enabled : \"true\" hostname : example.host.com # Hostname of your Bitbucket Server instance port : 443 # Port of your Bitbucket Server instance protocol : https # Protocol of your Bitbucket Server instance consumerKey : # Generated when creating the Bitbucket Server application link consumerPublicKey : # Generated when creating the Bitbucket Server application link consumerPrivateKey : # Generated when creating the Bitbucket Server application link Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use Bitbucket Server to authenticate to Codacy.","title":"Bitbucket Server"},{"location":"configuration/integrations/bitbucket-server/#bitbucket-server","text":"Follow the instructions below to set up the Codacy integration with Bitbucket Server: Follow the instructions on creating a Bitbucket Server application link . Edit the file values-production.yaml , set global.githubEnterprise.enabled: \"true\" and define the remaining values as described below and with the information obtained when you created the GitHub App: bitbucketEnterprise : enabled : \"true\" hostname : example.host.com # Hostname of your Bitbucket Server instance port : 443 # Port of your Bitbucket Server instance protocol : https # Protocol of your Bitbucket Server instance consumerKey : # Generated when creating the Bitbucket Server application link consumerPublicKey : # Generated when creating the Bitbucket Server application link consumerPrivateKey : # Generated when creating the Bitbucket Server application link Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use Bitbucket Server to authenticate to Codacy.","title":"Bitbucket Server"},{"location":"configuration/integrations/bitbucket/","text":"Bitbucket \u00b6 Since Bitbucket uses OAuth, the first step is to register a consumer for Codacy on Bitbucket side. Create a consumer \u00b6 OAuth needs a key and secret, together these are known as an OAuth consumer. You can create a consumer on any existing individual or team account. To create a consumer, do the following: From your avatar in the bottom left, click Bitbucket settings. Click \"OAuth\" from the left navigation and click the \"Add consumer\" button. Set the name of the consumer (e.g. \"Codacy\" ). Set the callback URL as \"http:// : /login/Bitbucket?codacy_skip_ga=1\" . Tick the \"This is a private consumer\" checkbox: Add the following permissions: Click Save. The system generates a key and a secret for you. Toggle the consumer name to see the generated Key and Secret value for your consumer. Enable Bitbucket and set the key and the secret \u00b6 Set your configuration values for Bitbucket on the values-production.yaml file. Enable Bitbucket by setting enable to \"true\" . Copy the key from Bitbucket and set it in the key field. Copy the secret from Bitbucket and set it in the secret field. Example: global : bitbucket : enabled : \"true\" key : \"12345232123\" secret : \"54321abcxdawj4bfsdh3\" Apply the configuration by issuing an helm upgrade. Example: helm upgrade ( ...options used to install codacy... ) \\ --values values-production.yaml","title":"Bitbucket"},{"location":"configuration/integrations/bitbucket/#bitbucket","text":"Since Bitbucket uses OAuth, the first step is to register a consumer for Codacy on Bitbucket side.","title":"Bitbucket"},{"location":"configuration/integrations/bitbucket/#create-a-consumer","text":"OAuth needs a key and secret, together these are known as an OAuth consumer. You can create a consumer on any existing individual or team account. To create a consumer, do the following: From your avatar in the bottom left, click Bitbucket settings. Click \"OAuth\" from the left navigation and click the \"Add consumer\" button. Set the name of the consumer (e.g. \"Codacy\" ). Set the callback URL as \"http:// : /login/Bitbucket?codacy_skip_ga=1\" . Tick the \"This is a private consumer\" checkbox: Add the following permissions: Click Save. The system generates a key and a secret for you. Toggle the consumer name to see the generated Key and Secret value for your consumer.","title":"Create a consumer"},{"location":"configuration/integrations/bitbucket/#enable-bitbucket-and-set-the-key-and-the-secret","text":"Set your configuration values for Bitbucket on the values-production.yaml file. Enable Bitbucket by setting enable to \"true\" . Copy the key from Bitbucket and set it in the key field. Copy the secret from Bitbucket and set it in the secret field. Example: global : bitbucket : enabled : \"true\" key : \"12345232123\" secret : \"54321abcxdawj4bfsdh3\" Apply the configuration by issuing an helm upgrade. Example: helm upgrade ( ...options used to install codacy... ) \\ --values values-production.yaml","title":"Enable Bitbucket and set the key and the secret"},{"location":"configuration/integrations/create-bitbucket-server-application-link/","text":"Bitbucket Server Application Link \u00b6 NOTE: Since Bitbucket Server uses OAuth1, you'll need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance: Generating the secrets \u00b6 Create a key pair using the RSA algorithm in the PKCS#8 format by running the following command below: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/configuration/git-providers/generate-bitbucket-server-secrets.sh ) Store the keys in a safe place for usage in the next steps and as a backup. Bitbucket Server Application Link \u00b6 To set up Bitbucket Server you need to create an application link on your Bitbucket Server installation. Application Link Creation \u00b6 Open <bitbucket server base url>/plugins/servlet/applinks/listApplicationLinks , where <bitbucket server base url> is the base url of your own Bitbucket Server instance. Create the link, use your Codacy installation URL for this. Name the link \u00b6 Application Name: Name the application (ex: Codacy) Application Type: The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click Edit to add an incoming connection. Add incoming connection \u00b6 Consumer Key: This value should be copied from the consumerKey generated previously. Consumer Name: You can choose any name (ex: Codacy). Public Key: This value should be copied from the consumerPublicKey generated previously. The rest of the fields should be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Bitbucket Server Application Link"},{"location":"configuration/integrations/create-bitbucket-server-application-link/#bitbucket-server-application-link","text":"NOTE: Since Bitbucket Server uses OAuth1, you'll need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance:","title":"Bitbucket Server Application Link"},{"location":"configuration/integrations/create-bitbucket-server-application-link/#generating-the-secrets","text":"Create a key pair using the RSA algorithm in the PKCS#8 format by running the following command below: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/configuration/git-providers/generate-bitbucket-server-secrets.sh ) Store the keys in a safe place for usage in the next steps and as a backup.","title":"Generating the secrets"},{"location":"configuration/integrations/create-bitbucket-server-application-link/#bitbucket-server-application-link_1","text":"To set up Bitbucket Server you need to create an application link on your Bitbucket Server installation.","title":"Bitbucket Server Application Link"},{"location":"configuration/integrations/create-bitbucket-server-application-link/#application-link-creation","text":"Open <bitbucket server base url>/plugins/servlet/applinks/listApplicationLinks , where <bitbucket server base url> is the base url of your own Bitbucket Server instance. Create the link, use your Codacy installation URL for this.","title":"Application Link Creation"},{"location":"configuration/integrations/create-bitbucket-server-application-link/#name-the-link","text":"Application Name: Name the application (ex: Codacy) Application Type: The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click Edit to add an incoming connection.","title":"Name the link"},{"location":"configuration/integrations/create-bitbucket-server-application-link/#add-incoming-connection","text":"Consumer Key: This value should be copied from the consumerKey generated previously. Consumer Name: You can choose any name (ex: Codacy). Public Key: This value should be copied from the consumerPublicKey generated previously. The rest of the fields should be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Add incoming connection"},{"location":"configuration/integrations/create-github-app/","text":"Creating a GitHub App \u00b6 You must create and correctly set up a GitHub App to allow Codacy to integrate with GitHub. To create the GitHub App: If you're using GitHub Cloud , open https://github.com/settings/apps/new . If you're using GitHub Enterprise , open https://<github hostname>/settings/apps/new , where <github hostname> is the hostname of your own GitHub Enterprise instance. Configure the new GitHub App using the values listed on the table below, replacing <codacy hostname> with the hostname of your Codacy instance. Field Value GitHub App name Codacy Homepage URL https://<codacy hostname> User authorization callback URL https://<codacy hostname> Webhook URL https://<codacy hostname>/2.0/events/gh/organization Repository permissions Administration Read & write Checks Read & write Issues Read & write Metadata Read only Pull requests Read & write Webhooks Read & write Commit statuses Read & write Organization permissions Members Read only Webhooks Read & write User permissions Email addresses Read only Git SSH keys Read & write Where can this GitHub App be installed? Any account Scroll to the bottom of the page, click Generate a private key , and save the .pem file containing the private key. Take note of the following information, as you'll need it to configure Codacy: GitHub App name App ID Client ID Client secret Private key (contents of the .pem file generated in the previous step)","title":"Creating a GitHub App"},{"location":"configuration/integrations/create-github-app/#creating-a-github-app","text":"You must create and correctly set up a GitHub App to allow Codacy to integrate with GitHub. To create the GitHub App: If you're using GitHub Cloud , open https://github.com/settings/apps/new . If you're using GitHub Enterprise , open https://<github hostname>/settings/apps/new , where <github hostname> is the hostname of your own GitHub Enterprise instance. Configure the new GitHub App using the values listed on the table below, replacing <codacy hostname> with the hostname of your Codacy instance. Field Value GitHub App name Codacy Homepage URL https://<codacy hostname> User authorization callback URL https://<codacy hostname> Webhook URL https://<codacy hostname>/2.0/events/gh/organization Repository permissions Administration Read & write Checks Read & write Issues Read & write Metadata Read only Pull requests Read & write Webhooks Read & write Commit statuses Read & write Organization permissions Members Read only Webhooks Read & write User permissions Email addresses Read only Git SSH keys Read & write Where can this GitHub App be installed? Any account Scroll to the bottom of the page, click Generate a private key , and save the .pem file containing the private key. Take note of the following information, as you'll need it to configure Codacy: GitHub App name App ID Client ID Client secret Private key (contents of the .pem file generated in the previous step)","title":"Creating a GitHub App"},{"location":"configuration/integrations/email/","text":"Emails \u00b6 Follow the instructions below to set up Codacy to send emails using your SMTP server: Edit the file values-production.yaml , set global.email.enabled: \"true\" and define the remaining values with the credentials for your SMTP server: email : enabled : \"true\" replyTo : \"notifications@mycompany.com\" smtp : hostname : \"smtp.mycompany.com\" protocol : \"smtps\" # smtps | smtp username : \"my-smtp-username\" password : \"a-s!r0ng-pVsSwORG\" # port: 25 Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to receive notifications Emails during usage of the product.","title":"Emails"},{"location":"configuration/integrations/email/#emails","text":"Follow the instructions below to set up Codacy to send emails using your SMTP server: Edit the file values-production.yaml , set global.email.enabled: \"true\" and define the remaining values with the credentials for your SMTP server: email : enabled : \"true\" replyTo : \"notifications@mycompany.com\" smtp : hostname : \"smtp.mycompany.com\" protocol : \"smtps\" # smtps | smtp username : \"my-smtp-username\" password : \"a-s!r0ng-pVsSwORG\" # port: 25 Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to receive notifications Emails during usage of the product.","title":"Emails"},{"location":"configuration/integrations/github-cloud/","text":"GitHub Cloud \u00b6 Follow the instructions below to set up the Codacy integration with GitHub Cloud: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml , set global.github.enabled: \"true\" and define the remaining values with the information obtained when you created the GitHub App: github : enabled : \"true\" clientId : Iv1.0000000000000000 # Client ID clientSecret : a000000000000000 # Client secret app : name : Codacy # GitHub App name id : 00000 # App ID privateKey : \"-----BEGIN RSA PRIVATE KEY-----...\" # Contents of the .pem file with newlines removed -----END RSA PRIVATE KEY----- Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitHub Cloud to authenticate to Codacy.","title":"GitHub Cloud"},{"location":"configuration/integrations/github-cloud/#github-cloud","text":"Follow the instructions below to set up the Codacy integration with GitHub Cloud: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml , set global.github.enabled: \"true\" and define the remaining values with the information obtained when you created the GitHub App: github : enabled : \"true\" clientId : Iv1.0000000000000000 # Client ID clientSecret : a000000000000000 # Client secret app : name : Codacy # GitHub App name id : 00000 # App ID privateKey : \"-----BEGIN RSA PRIVATE KEY-----...\" # Contents of the .pem file with newlines removed -----END RSA PRIVATE KEY----- Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitHub Cloud to authenticate to Codacy.","title":"GitHub Cloud"},{"location":"configuration/integrations/github-enterprise/","text":"GitHub Enterprise \u00b6 Follow the instructions below to set up the Codacy integration with GitHub Enterprise: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml , set global.githubEnterprise.enabled: \"true\" and define the remaining values as described below and with the information obtained when you created the GitHub App: githubEnterprise : enabled : \"true\" hostname : example.host.com # Hostname of your GitHub Enterprise instance port : 443 # Port of your GitHub Enterprise instance protocol : https # Protocol of your GitHub Enterprise instance disableSSL : false # Disable certificate validation isPrivateMode : true # Status of private mode on your GitHub Enterprise instance clientId : Iv1.0000000000000000 # Client ID clientSecret : a000000000000000 # Client secret app : name : Codacy # GitHub App name id : 00000 # App ID privateKey : \"-----BEGIN RSA PRIVATE KEY-----...\" # Contents of the .pem file with newlines removed Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitHub Enterprise to authenticate to Codacy.","title":"GitHub Enterprise"},{"location":"configuration/integrations/github-enterprise/#github-enterprise","text":"Follow the instructions below to set up the Codacy integration with GitHub Enterprise: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml , set global.githubEnterprise.enabled: \"true\" and define the remaining values as described below and with the information obtained when you created the GitHub App: githubEnterprise : enabled : \"true\" hostname : example.host.com # Hostname of your GitHub Enterprise instance port : 443 # Port of your GitHub Enterprise instance protocol : https # Protocol of your GitHub Enterprise instance disableSSL : false # Disable certificate validation isPrivateMode : true # Status of private mode on your GitHub Enterprise instance clientId : Iv1.0000000000000000 # Client ID clientSecret : a000000000000000 # Client secret app : name : Codacy # GitHub App name id : 00000 # App ID privateKey : \"-----BEGIN RSA PRIVATE KEY-----...\" # Contents of the .pem file with newlines removed Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitHub Enterprise to authenticate to Codacy.","title":"GitHub Enterprise"},{"location":"configuration/integrations/gitlab-cloud/","text":"GitLab Cloud \u00b6 GitLab Application \u00b6 Follow the instructions below to set up the Codacy integration with GitLab Cloud: Create a new application pointing to your local Codacy deployment URL with api , read user and read repository scopes. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your hostname as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLab https://codacy.example.com/add/addPermissions/GitLab https://codacy.example.com/add/addProvider/GitLab https://codacy.example.com/add/addService/GitLab Edit the file values-production.yaml , set global.gitlab.enabled: \"true\" and define the remaining values with the information obtained when you created the GitLab Application: gitlab : enabled : \"true\" clientId : a000000000000000 # Client ID clientSecret : a000000000000000 # Client secret Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitLab Cloud to authenticate to Codacy.","title":"GitLab Cloud"},{"location":"configuration/integrations/gitlab-cloud/#gitlab-cloud","text":"","title":"GitLab Cloud"},{"location":"configuration/integrations/gitlab-cloud/#gitlab-application","text":"Follow the instructions below to set up the Codacy integration with GitLab Cloud: Create a new application pointing to your local Codacy deployment URL with api , read user and read repository scopes. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your hostname as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLab https://codacy.example.com/add/addPermissions/GitLab https://codacy.example.com/add/addProvider/GitLab https://codacy.example.com/add/addService/GitLab Edit the file values-production.yaml , set global.gitlab.enabled: \"true\" and define the remaining values with the information obtained when you created the GitLab Application: gitlab : enabled : \"true\" clientId : a000000000000000 # Client ID clientSecret : a000000000000000 # Client secret Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitLab Cloud to authenticate to Codacy.","title":"GitLab Application"},{"location":"configuration/integrations/gitlab-enterprise/","text":"GitLab Enterprise \u00b6 GitLab Application \u00b6 Follow the instructions below to set up the Codacy integration with GitLab Cloud: Create a new application in <gitlabEnterprise url>/profile/applications , where <gitlabEnterprise url> is the URL of your own GitLab Enterprise instance, pointing to your local Codacy instance URL with api , read user and read repository scopes. You'll need to add the following \"Redirect URI\", making sure to update both your protocol to use either HTTP or HTTPS and your hostname as well. Keep in mind that this field is case sensitive. https://codacy.example.com/login/GitLab https://codacy.example.com/add/addPermissions/GitLab https://codacy.example.com/add/addProvider/GitLab https://codacy.example.com/add/addService/GitLab Edit the file values-production.yaml , set global.gitlab.enabled: \"true\" and define the remaining values with the information obtained when you created the GitLab Application: gitlab : enabled : \"true\" clientId : a000000000000000 # Client ID clientSecret : a000000000000000 # Client secret Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitLab Enterprise to authenticate to Codacy.","title":"GitLab Enterprise"},{"location":"configuration/integrations/gitlab-enterprise/#gitlab-enterprise","text":"","title":"GitLab Enterprise"},{"location":"configuration/integrations/gitlab-enterprise/#gitlab-application","text":"Follow the instructions below to set up the Codacy integration with GitLab Cloud: Create a new application in <gitlabEnterprise url>/profile/applications , where <gitlabEnterprise url> is the URL of your own GitLab Enterprise instance, pointing to your local Codacy instance URL with api , read user and read repository scopes. You'll need to add the following \"Redirect URI\", making sure to update both your protocol to use either HTTP or HTTPS and your hostname as well. Keep in mind that this field is case sensitive. https://codacy.example.com/login/GitLab https://codacy.example.com/add/addPermissions/GitLab https://codacy.example.com/add/addProvider/GitLab https://codacy.example.com/add/addService/GitLab Edit the file values-production.yaml , set global.gitlab.enabled: \"true\" and define the remaining values with the information obtained when you created the GitLab Application: gitlab : enabled : \"true\" clientId : a000000000000000 # Client ID clientSecret : a000000000000000 # Client secret Apply this configuration by performing a Helm upgrade. To do so append --values values-production.yaml to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-production.yaml After this is done you will be able to use GitLab Enterprise to authenticate to Codacy.","title":"GitLab Application"},{"location":"extra/database/","text":"Database migration guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Database migration guide"},{"location":"extra/database/#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results","title":"Database migration guide"},{"location":"extra/database/#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"extra/database/#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"extra/database/#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"extra/database/#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"extra/database/#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Sample script"},{"location":"extra/k8s-cheatsheet/","text":"Kubernetes cheatsheet \u00b6 How to install a custom Codacy version \u00b6 Install \u00b6 sudo git clone git://github.com/codacy/chart -b <YOUR-BRANCH> helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE> Update \u00b6 ( cd chart ; sudo git fetch --all --prune --tags ; sudo git reset --hard origin/<YOUR-BRANCH> ; ) helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE> Clean the namespace \u00b6 helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Check uninstall was successful \u00b6 ps aux | grep -i kubectl Edit configmap \u00b6 kubectl get configmaps and kubectl edit configmap <configmap-name> Restart deployment of daemonset \u00b6 daemonsets \u00b6 kubectl get daemonsets and kubectl rollout restart daemonset/<daemonset-name> deployment \u00b6 kubectl get deployment and kubectl rollout restart deployment/<deployment-name> and kubectl rollout status deployment/<deployment-name> -w Read logs \u00b6 daemonset with multiple containers \u00b6 kubectl logs daemonset/<daemonset-name> <container-name> -f service \u00b6 kubectl get svc and kubectl logs -l $( kubectl get svc/<service-name> -o = json | jq \".spec.selector\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' | sed -e 'H;${x;s/\\n/,/g;s/^,//;p;};d' ) -f Open shell inside container \u00b6 kubectl exec -it daemonset/<daemonset-name> -c <container-name> sh or kubectl exec -it deployment/<deployment-name> sh MicroK8s \u00b6 Session Manager SSH \u00b6 When using AWS Session Manager, to connect to the instance where you installed microk8s, since the CLI is very limited you will benefit from using these aliases: alias kubectl = 'sudo microk8s.kubectl -n <namespace-name>' alias helm = 'sudo helm'","title":"Kubernetes cheatsheet"},{"location":"extra/k8s-cheatsheet/#kubernetes-cheatsheet","text":"","title":"Kubernetes cheatsheet"},{"location":"extra/k8s-cheatsheet/#how-to-install-a-custom-codacy-version","text":"","title":"How to install a custom Codacy version"},{"location":"extra/k8s-cheatsheet/#install","text":"sudo git clone git://github.com/codacy/chart -b <YOUR-BRANCH> helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE>","title":"Install"},{"location":"extra/k8s-cheatsheet/#update","text":"( cd chart ; sudo git fetch --all --prune --tags ; sudo git reset --hard origin/<YOUR-BRANCH> ; ) helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE>","title":"Update"},{"location":"extra/k8s-cheatsheet/#clean-the-namespace","text":"helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job &","title":"Clean the namespace"},{"location":"extra/k8s-cheatsheet/#check-uninstall-was-successful","text":"ps aux | grep -i kubectl","title":"Check uninstall was successful"},{"location":"extra/k8s-cheatsheet/#edit-configmap","text":"kubectl get configmaps and kubectl edit configmap <configmap-name>","title":"Edit configmap"},{"location":"extra/k8s-cheatsheet/#restart-deployment-of-daemonset","text":"","title":"Restart deployment of daemonset"},{"location":"extra/k8s-cheatsheet/#daemonsets","text":"kubectl get daemonsets and kubectl rollout restart daemonset/<daemonset-name>","title":"daemonsets"},{"location":"extra/k8s-cheatsheet/#deployment","text":"kubectl get deployment and kubectl rollout restart deployment/<deployment-name> and kubectl rollout status deployment/<deployment-name> -w","title":"deployment"},{"location":"extra/k8s-cheatsheet/#read-logs","text":"","title":"Read logs"},{"location":"extra/k8s-cheatsheet/#daemonset-with-multiple-containers","text":"kubectl logs daemonset/<daemonset-name> <container-name> -f","title":"daemonset with multiple containers"},{"location":"extra/k8s-cheatsheet/#service","text":"kubectl get svc and kubectl logs -l $( kubectl get svc/<service-name> -o = json | jq \".spec.selector\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' | sed -e 'H;${x;s/\\n/,/g;s/^,//;p;};d' ) -f","title":"service"},{"location":"extra/k8s-cheatsheet/#open-shell-inside-container","text":"kubectl exec -it daemonset/<daemonset-name> -c <container-name> sh or kubectl exec -it deployment/<deployment-name> sh","title":"Open shell inside container"},{"location":"extra/k8s-cheatsheet/#microk8s","text":"","title":"MicroK8s"},{"location":"extra/k8s-cheatsheet/#session-manager-ssh","text":"When using AWS Session Manager, to connect to the instance where you installed microk8s, since the CLI is very limited you will benefit from using these aliases: alias kubectl = 'sudo microk8s.kubectl -n <namespace-name>' alias helm = 'sudo helm'","title":"Session Manager SSH"},{"location":"extra/logs-collect/","text":"Collecting logs for Support \u00b6 To help troubleshoot issues, obtain the logs from your Codacy instance and send them to Codacy's Support: Download the logs of the last 7 days as an archive file with the name codacy_logs_<timestamp>.zip by running the following command locally, replacing <namespace> with the namespace in which Codacy was installed: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/extra/extract-codacy-logs.sh ) \\ -n <namespace> To reduce the size of the compressed archive file, you should retrieve logs for a smaller number of days by replacing <days> with a number between 1 and 7: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/extra/extract-codacy-logs.sh ) \\ -n <namespace> -d <days> You can also download the script extract-codacy-logs.sh to run it manually. Send the compressed logs to Codacy's support team at support@codacy.com for analysis. If the file is too big, please upload the file to either a cloud storage service such as Google Drive or to a file transfer service such as WeTransfer and send us the link to the file instead.","title":"Collecting logs for Support"},{"location":"extra/logs-collect/#collecting-logs-for-support","text":"To help troubleshoot issues, obtain the logs from your Codacy instance and send them to Codacy's Support: Download the logs of the last 7 days as an archive file with the name codacy_logs_<timestamp>.zip by running the following command locally, replacing <namespace> with the namespace in which Codacy was installed: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/extra/extract-codacy-logs.sh ) \\ -n <namespace> To reduce the size of the compressed archive file, you should retrieve logs for a smaller number of days by replacing <days> with a number between 1 and 7: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/extra/extract-codacy-logs.sh ) \\ -n <namespace> -d <days> You can also download the script extract-codacy-logs.sh to run it manually. Send the compressed logs to Codacy's support team at support@codacy.com for analysis. If the file is too big, please upload the file to either a cloud storage service such as Google Drive or to a file transfer service such as WeTransfer and send us the link to the file instead.","title":"Collecting logs for Support"},{"location":"extra/uninstall/","text":"Uninstalling Codacy \u00b6 To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of the effects of these commands please run each of the bash subcommands and validate their output. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstalling Codacy"},{"location":"extra/uninstall/#uninstalling-codacy","text":"To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of the effects of these commands please run each of the bash subcommands and validate their output. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstalling Codacy"},{"location":"extra/upgrade/","text":"Upgrading Codacy \u00b6 NOTE: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f , like helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with: helm get values codacy > codacy.yaml Decide on all the values you need to set. Perform the upgrade, with all --set arguments extracted in step 2: helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrading Codacy"},{"location":"extra/upgrade/#upgrading-codacy","text":"NOTE: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f , like helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with: helm get values codacy > codacy.yaml Decide on all the values you need to set. Perform the upgrade, with all --set arguments extracted in step 2: helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrading Codacy"},{"location":"infrastructure/aks-quickstart/","text":"Creating an AKS cluster \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"Creating an AKS cluster"},{"location":"infrastructure/aks-quickstart/#creating-an-aks-cluster","text":"","title":"Creating an AKS cluster"},{"location":"infrastructure/aks-quickstart/#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"infrastructure/aks-quickstart/#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"infrastructure/aks-quickstart/#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"infrastructure/aks-quickstart/#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"infrastructure/eks-quickstart/","text":"Creating an Amazon EKS cluster \u00b6 Follow the instructions below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform. 1. Prepare your environment \u00b6 Prepare your environment to set up the Amazon EKS cluster: Make sure that you have the following tools installed on your machine: Git version >= 2.0.0 AWS CLI version 1 Terraform version >= 0.12 Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the Terraform documentation , if your .aws/credentials are more complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Codacy chart repository and change to the directory that includes the provided Terraform configuration files: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ This folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and a DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster 2. Set up the Terraform state storage backend \u00b6 The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these Terraform templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply This creates an Amazon S3 bucket with a unique name to save the infrastructure state. Take note of the value of state_bucket_name in the output of the command. Edit the main/config.tf and setup/config.tf files and follow the instructions included in the comments to set the name of the Amazon S3 bucket created above and enable the use of the backend in those infrastructure stacks. 3. Create a vanilla Amazon EKS cluster \u00b6 Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type and number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file and set the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section. 4. Set up the cluster to run Codacy \u00b6 Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply Connect to the Kubernetes Dashboard: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token Uninstalling the Amazon EKS cluster \u00b6 Warning If you proceed beyond this point you'll permanently delete and break things. Clean up your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly you must first disable these settings by editing the file backend/state_and_lock.tf and following the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Creating an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#creating-an-amazon-eks-cluster","text":"Follow the instructions below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform.","title":"Creating an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#1-prepare-your-environment","text":"Prepare your environment to set up the Amazon EKS cluster: Make sure that you have the following tools installed on your machine: Git version >= 2.0.0 AWS CLI version 1 Terraform version >= 0.12 Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the Terraform documentation , if your .aws/credentials are more complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Codacy chart repository and change to the directory that includes the provided Terraform configuration files: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ This folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and a DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster","title":"1. Prepare your environment"},{"location":"infrastructure/eks-quickstart/#2-set-up-the-terraform-state-storage-backend","text":"The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these Terraform templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply This creates an Amazon S3 bucket with a unique name to save the infrastructure state. Take note of the value of state_bucket_name in the output of the command. Edit the main/config.tf and setup/config.tf files and follow the instructions included in the comments to set the name of the Amazon S3 bucket created above and enable the use of the backend in those infrastructure stacks.","title":"2. Set up the Terraform state storage backend"},{"location":"infrastructure/eks-quickstart/#3-create-a-vanilla-amazon-eks-cluster","text":"Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type and number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file and set the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section.","title":"3. Create a vanilla Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#4-set-up-the-cluster-to-run-codacy","text":"Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply Connect to the Kubernetes Dashboard: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token","title":"4. Set up the cluster to run Codacy"},{"location":"infrastructure/eks-quickstart/#uninstalling-the-amazon-eks-cluster","text":"Warning If you proceed beyond this point you'll permanently delete and break things. Clean up your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly you must first disable these settings by editing the file backend/state_and_lock.tf and following the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Uninstalling the Amazon EKS cluster"},{"location":"infrastructure/microk8s-quickstart/","text":"Creating a MicroK8s cluster \u00b6 MicroK8s is a lightweight, fully conformant, single-package Kubernetes developed by Canonical. The project is publicly available on GitHub . Follow the instructions below to set up a MicroK8s instance from scratch, including all the necessary dependencies and configurations. As part of this process, the Helm client ( helm ) and the Helm server ( tiller ) will be installed on the MicroK8s instance: helm is the command-line client responsible for resolving the configuration of the chart to be installed and issuing the correct install commands onto the Helm server. tiller is the in-cluster server responsible for receiving the install commands issued by the Helm client and managing the lifecycle of the components that have been installed. 1. Prepare your environment \u00b6 Prepare your environment to set up the MicroK8s instance. You will need a machine running Ubuntu Server 18.04 LTS that: Is correctly provisioned with the resources described for MicroK8s in the system requirements Is able to establish a connection to the PostgreSQL instance described in the system requirements The next steps assume that you are starting from a clean install of Ubuntu Server and require that you run commands on a local or remote command line session on the machine. 2. Installing MicroK8s \u00b6 Install MicroK8s on the machine: Make sure that the package nfs-common is installed: sudo apt update && sudo apt install nfs-common -y Install MicroK8s from the 1.15/stable channel: sudo snap install microk8s --classic --channel = 1 .15/stable sudo usermod -a -G microk8s $USER sudo su - $USER Check that MicroK8s is running: $ microk8s.status --wait-ready microk8s is running addons: knative: disabled jaeger: disabled fluentd: disabled gpu: disabled cilium: disabled storage: disabled registry: disabled rbac: disabled ingress: disabled dns: disabled metrics-server: disabled linkerd: disabled prometheus: disabled istio: disabled dashboard: disabled 3. Configuring MicroK8s \u00b6 Now that MicroK8s is running on the machine we can proceed to enabling the necessary plugins and installing the Helm client and server: Enable the following plugins on MicroK8s: sudo mkdir -p /var/snap/microk8s/current/args sudo echo \"--allow-privileged=true\" >> /var/snap/microk8s/current/args/kube-apiserver microk8s.status --wait-ready microk8s.enable dns microk8s.enable storage microk8s.enable ingress microk8s.status --wait-ready microk8s.stop microk8s.start microk8s.status --wait-ready Install version v2.16.3 of the Helm client: sudo snap install helm --classic --channel = 2 .16/stable Install the Helm server: microk8s.kubectl create serviceaccount --namespace kube-system tiller microk8s.kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init --service-account tiller The plugins are now enabled and the MicroK8s instance bootstrapped. However, we must wait for some MicroK8s pods to be ready, as failing to do so can result in the pods entering a CrashLoopBackoff state: microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = kube-dns microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = hostpath-provisioner # If the following command fails, you probably installed the wrong microk8s version microk8s.kubectl wait -n default --for = condition = Ready pod -l name = nginx-ingress-microk8s microk8s.kubectl -n kube-system wait --for = condition = Ready pod -l name = tiller After these commands return successfully we have ensured that DNS, HTTP, and NGINX Ingress are enabled and working properly inside the MicroK8s instance. Notes on installing Codacy \u00b6 You can now follow the generic Codacy installation instructions but please note the following: You must execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, we suggest that you create an alias so that you can run the commands directly as provided on the instructions: alias kubectl = microk8s.kubectl When running the helm upgrade command that installs the Codacy chart you must append the file values-microk8s.yaml that downsizes some component limits, making it easier to fit Codacy in the lightweight MicroK8s solution. You can download the file values-microk8s.yaml by running: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-microk8s.yaml","title":"Creating a MicroK8s cluster"},{"location":"infrastructure/microk8s-quickstart/#creating-a-microk8s-cluster","text":"MicroK8s is a lightweight, fully conformant, single-package Kubernetes developed by Canonical. The project is publicly available on GitHub . Follow the instructions below to set up a MicroK8s instance from scratch, including all the necessary dependencies and configurations. As part of this process, the Helm client ( helm ) and the Helm server ( tiller ) will be installed on the MicroK8s instance: helm is the command-line client responsible for resolving the configuration of the chart to be installed and issuing the correct install commands onto the Helm server. tiller is the in-cluster server responsible for receiving the install commands issued by the Helm client and managing the lifecycle of the components that have been installed.","title":"Creating a MicroK8s cluster"},{"location":"infrastructure/microk8s-quickstart/#1-prepare-your-environment","text":"Prepare your environment to set up the MicroK8s instance. You will need a machine running Ubuntu Server 18.04 LTS that: Is correctly provisioned with the resources described for MicroK8s in the system requirements Is able to establish a connection to the PostgreSQL instance described in the system requirements The next steps assume that you are starting from a clean install of Ubuntu Server and require that you run commands on a local or remote command line session on the machine.","title":"1. Prepare your environment"},{"location":"infrastructure/microk8s-quickstart/#2-installing-microk8s","text":"Install MicroK8s on the machine: Make sure that the package nfs-common is installed: sudo apt update && sudo apt install nfs-common -y Install MicroK8s from the 1.15/stable channel: sudo snap install microk8s --classic --channel = 1 .15/stable sudo usermod -a -G microk8s $USER sudo su - $USER Check that MicroK8s is running: $ microk8s.status --wait-ready microk8s is running addons: knative: disabled jaeger: disabled fluentd: disabled gpu: disabled cilium: disabled storage: disabled registry: disabled rbac: disabled ingress: disabled dns: disabled metrics-server: disabled linkerd: disabled prometheus: disabled istio: disabled dashboard: disabled","title":"2. Installing MicroK8s"},{"location":"infrastructure/microk8s-quickstart/#3-configuring-microk8s","text":"Now that MicroK8s is running on the machine we can proceed to enabling the necessary plugins and installing the Helm client and server: Enable the following plugins on MicroK8s: sudo mkdir -p /var/snap/microk8s/current/args sudo echo \"--allow-privileged=true\" >> /var/snap/microk8s/current/args/kube-apiserver microk8s.status --wait-ready microk8s.enable dns microk8s.enable storage microk8s.enable ingress microk8s.status --wait-ready microk8s.stop microk8s.start microk8s.status --wait-ready Install version v2.16.3 of the Helm client: sudo snap install helm --classic --channel = 2 .16/stable Install the Helm server: microk8s.kubectl create serviceaccount --namespace kube-system tiller microk8s.kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init --service-account tiller The plugins are now enabled and the MicroK8s instance bootstrapped. However, we must wait for some MicroK8s pods to be ready, as failing to do so can result in the pods entering a CrashLoopBackoff state: microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = kube-dns microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = hostpath-provisioner # If the following command fails, you probably installed the wrong microk8s version microk8s.kubectl wait -n default --for = condition = Ready pod -l name = nginx-ingress-microk8s microk8s.kubectl -n kube-system wait --for = condition = Ready pod -l name = tiller After these commands return successfully we have ensured that DNS, HTTP, and NGINX Ingress are enabled and working properly inside the MicroK8s instance.","title":"3. Configuring MicroK8s"},{"location":"infrastructure/microk8s-quickstart/#notes-on-installing-codacy","text":"You can now follow the generic Codacy installation instructions but please note the following: You must execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, we suggest that you create an alias so that you can run the commands directly as provided on the instructions: alias kubectl = microk8s.kubectl When running the helm upgrade command that installs the Codacy chart you must append the file values-microk8s.yaml that downsizes some component limits, making it easier to fit Codacy in the lightweight MicroK8s solution. You can download the file values-microk8s.yaml by running: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-microk8s.yaml","title":"Notes on installing Codacy"}]}