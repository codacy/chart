{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Installing Codacy on Kubernetes \u00b6 This documentation guides you on how to install Codacy on Kubernetes or MicroK8s. To install Codacy you must complete these main steps: Setting up the system requirements Ensure that your infrastructure meets the hardware and system requirements to run Codacy. Installing Codacy Install Codacy on the cluster using our Helm chart that includes all the necessary components and dependencies. Configuring Codacy Configure integrations with Git providers and set up monitoring. The next sections include detailed instructions on how to complete each step of the installation process. Make sure that you complete each step before advancing to the next one. 1. Setting up the system requirements \u00b6 Before you start, you must prepare and and provision the database server and Kubernetes or MicroK8s cluster that will host Codacy. Carefully review and set up the system requirements to run Codacy by following the instructions on the page below: System requirements Optionally, you can follow one of the guides below to quickly create a new Kubernetes or MicroK8s cluster that satisfies the characteristics described in the system requirements: Creating an Amazon EKS cluster Creating an AKS cluster Creating a MicroK8s cluster 2. Installing Codacy \u00b6 Install Codacy on an existing cluster using our Helm chart: Make sure that you have the following tools installed on your machine: kubectl within one minor version difference of your cluster Important If you are using MicroK8s you don't need to install kubectl because you will execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, check how to create an alias for kubectl . Helm client version 2.16.3 Create a cluster namespace called codacy that will group all resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials provided by Codacy together with your license to a cluster Secret. This is necessary because some Codacy Docker images are currently private. Substitute <docker_username> and <docker_password> with the Docker registry username and password and run the following command: kubectl create secret docker-registry docker-credentials \\ --docker-username = <docker_username> \\ --docker-password = <docker_password> \\ --namespace codacy Download the template file values-production.yaml and use a text editor of your choice to edit the value placeholders as described in the comments. You can download the template file by running: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-production.yaml Create an address record on your DNS provider mapping the hostname you used in the previous step to the IP address of your Ingress controller. Important If you are using MicroK8s you must map the hostname to the public IP address of the machine running MicroK8s. Add Codacy's chart repository to your Helm client and install the Codacy chart using the file values-production.yaml created previously. Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . Use wget to download the extra file and uncomment the last line before running the helm upgrade command below: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-microk8s.yaml helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --values values-production.yaml \\ # --values values-microk8s.yaml By now all the Codacy pods should be starting in the cluster. Run the following command and wait for all the pods to have the status Running , which can take several minutes: $ kubectl get pods -n codacy NAME READY STATUS RESTARTS AGE codacy-activities-6d9db9499-stk2k 1 /1 Running 2 8m57s codacy-activitiesdb-0 1 /1 Running 0 8m57s codacy-api-f7897b965-fgn67 1 /1 Running 0 8m57s codacy-api-f7897b965-kkqsx 1 /1 Running 0 8m57s codacy-core-7bcf697968-85tl6 1 /1 Running 0 8m57s codacy-crow-7c957d45f6-b8zp2 1 /1 Running 2 8m57s codacy-crowdb-0 1 /1 Running 0 8m57s codacy-engine-549bcb69d9-cgrqf 1 /1 Running 1 8m57s codacy-engine-549bcb69d9-sh5f4 1 /1 Running 1 8m57s codacy-fluentdoperator-x5vr2 2 /2 Running 0 8m57s codacy-hotspots-api-b7b9db896-68gxx 1 /1 Running 2 8m57s codacy-hotspots-worker-76bb45b4d6-8gz45 1 /1 Running 3 8m57s codacy-hotspotsdb-0 1 /1 Running 0 8m57s codacy-listener-868b784dcf-npdfh 1 /1 Running 0 8m57s codacy-listenerdb-0 1 /1 Running 0 8m57s codacy-minio-7cfdc7b4f4-254gz 1 /1 Running 0 8m57s codacy-nfsserverprovisioner-0 1 /1 Running 0 8m57s codacy-portal-774d9fc596-rwqj5 1 /1 Running 2 8m56s codacy-rabbitmq-ha-0 1 /1 Running 0 8m57s codacy-ragnaros-69459775b5-hmj4d 1 /1 Running 3 8m57s codacy-remote-provider-service-8fb8556b-rr4ws 1 /1 Running 0 8m56s codacy-worker-manager-656dbf8d6d-n4j7c 1 /1 Running 0 8m57s 3. Configuring Codacy \u00b6 After successfully installing Codacy on your cluster, you are now ready to perform the post-install configuration steps: Use a browser to navigate to the Codacy hostname previously configured on the file values-production.yaml . Follow Codacy's onboarding process, which will guide you through the following steps: Creating an administrator account Configuring one or more of the following supported integrations: GitHub Cloud GitHub Enterprise GitLab Cloud GitLab Enterprise Bitbucket Cloud Bitbucket Server Email Creating an initial organization Inviting users to Codacy As a last step we recommend that you set up monitoring on your Codacy instance. If you run into any issues while configuring Codacy, be sure to check our troubleshooting guide for more help.","title":"Installing Codacy on Kubernetes"},{"location":"#installing-codacy-on-kubernetes","text":"This documentation guides you on how to install Codacy on Kubernetes or MicroK8s. To install Codacy you must complete these main steps: Setting up the system requirements Ensure that your infrastructure meets the hardware and system requirements to run Codacy. Installing Codacy Install Codacy on the cluster using our Helm chart that includes all the necessary components and dependencies. Configuring Codacy Configure integrations with Git providers and set up monitoring. The next sections include detailed instructions on how to complete each step of the installation process. Make sure that you complete each step before advancing to the next one.","title":"Installing Codacy on Kubernetes"},{"location":"#1-setting-up-the-system-requirements","text":"Before you start, you must prepare and and provision the database server and Kubernetes or MicroK8s cluster that will host Codacy. Carefully review and set up the system requirements to run Codacy by following the instructions on the page below: System requirements Optionally, you can follow one of the guides below to quickly create a new Kubernetes or MicroK8s cluster that satisfies the characteristics described in the system requirements: Creating an Amazon EKS cluster Creating an AKS cluster Creating a MicroK8s cluster","title":"1. Setting up the system requirements"},{"location":"#2-installing-codacy","text":"Install Codacy on an existing cluster using our Helm chart: Make sure that you have the following tools installed on your machine: kubectl within one minor version difference of your cluster Important If you are using MicroK8s you don't need to install kubectl because you will execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, check how to create an alias for kubectl . Helm client version 2.16.3 Create a cluster namespace called codacy that will group all resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials provided by Codacy together with your license to a cluster Secret. This is necessary because some Codacy Docker images are currently private. Substitute <docker_username> and <docker_password> with the Docker registry username and password and run the following command: kubectl create secret docker-registry docker-credentials \\ --docker-username = <docker_username> \\ --docker-password = <docker_password> \\ --namespace codacy Download the template file values-production.yaml and use a text editor of your choice to edit the value placeholders as described in the comments. You can download the template file by running: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-production.yaml Create an address record on your DNS provider mapping the hostname you used in the previous step to the IP address of your Ingress controller. Important If you are using MicroK8s you must map the hostname to the public IP address of the machine running MicroK8s. Add Codacy's chart repository to your Helm client and install the Codacy chart using the file values-production.yaml created previously. Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . Use wget to download the extra file and uncomment the last line before running the helm upgrade command below: wget https://raw.githubusercontent.com/codacy/chart/master/codacy/values-microk8s.yaml helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --values values-production.yaml \\ # --values values-microk8s.yaml By now all the Codacy pods should be starting in the cluster. Run the following command and wait for all the pods to have the status Running , which can take several minutes: $ kubectl get pods -n codacy NAME READY STATUS RESTARTS AGE codacy-activities-6d9db9499-stk2k 1 /1 Running 2 8m57s codacy-activitiesdb-0 1 /1 Running 0 8m57s codacy-api-f7897b965-fgn67 1 /1 Running 0 8m57s codacy-api-f7897b965-kkqsx 1 /1 Running 0 8m57s codacy-core-7bcf697968-85tl6 1 /1 Running 0 8m57s codacy-crow-7c957d45f6-b8zp2 1 /1 Running 2 8m57s codacy-crowdb-0 1 /1 Running 0 8m57s codacy-engine-549bcb69d9-cgrqf 1 /1 Running 1 8m57s codacy-engine-549bcb69d9-sh5f4 1 /1 Running 1 8m57s codacy-fluentdoperator-x5vr2 2 /2 Running 0 8m57s codacy-hotspots-api-b7b9db896-68gxx 1 /1 Running 2 8m57s codacy-hotspots-worker-76bb45b4d6-8gz45 1 /1 Running 3 8m57s codacy-hotspotsdb-0 1 /1 Running 0 8m57s codacy-listener-868b784dcf-npdfh 1 /1 Running 0 8m57s codacy-listenerdb-0 1 /1 Running 0 8m57s codacy-minio-7cfdc7b4f4-254gz 1 /1 Running 0 8m57s codacy-nfsserverprovisioner-0 1 /1 Running 0 8m57s codacy-portal-774d9fc596-rwqj5 1 /1 Running 2 8m56s codacy-rabbitmq-ha-0 1 /1 Running 0 8m57s codacy-ragnaros-69459775b5-hmj4d 1 /1 Running 3 8m57s codacy-remote-provider-service-8fb8556b-rr4ws 1 /1 Running 0 8m56s codacy-worker-manager-656dbf8d6d-n4j7c 1 /1 Running 0 8m57s","title":"2. Installing Codacy"},{"location":"#3-configuring-codacy","text":"After successfully installing Codacy on your cluster, you are now ready to perform the post-install configuration steps: Use a browser to navigate to the Codacy hostname previously configured on the file values-production.yaml . Follow Codacy's onboarding process, which will guide you through the following steps: Creating an administrator account Configuring one or more of the following supported integrations: GitHub Cloud GitHub Enterprise GitLab Cloud GitLab Enterprise Bitbucket Cloud Bitbucket Server Email Creating an initial organization Inviting users to Codacy As a last step we recommend that you set up monitoring on your Codacy instance. If you run into any issues while configuring Codacy, be sure to check our troubleshooting guide for more help.","title":"3. Configuring Codacy"},{"location":"requirements/","text":"System requirements \u00b6 Before installing Codacy you must ensure that you have the following infrastructure correctly provisioned and configured: Kubernetes or MicroK8s cluster PostgreSQL server The next sections describe in detail how to set up these prerequisites. Kubernetes or MicroK8s cluster setup \u00b6 The cluster running Codacy must satisfy the following requirements: The infrastructure hosting the cluster must be provisioned with the hardware and networking requirements described below The orchestration platform managing the cluster must be one of: Kubernetes version 1.14.* or 1.15.* MicroK8s version 1.15 Tiller, the server part of Helm version 2.16 , must be installed in the cluster The NGINX Ingress controller must be installed and correctly set up in the cluster Cluster networking requirements \u00b6 The cluster must be configured to accept and establish connections on the following ports: Service Protocol/Port Notes Inbound SSH TCP/22 MicroK8s only , to access the infrastructure remotely. Inbound HTTP TCP/80 Allow access to the Codacy website and API endpoints Inbound HTTPS TCP/443 Allow access to the Codacy website and API endpoints Outbound PostgreSQL TCP/5432 Connection to the PostgreSQL DBMS Outbound SMTP TCP/25 Connection to your SMTP server Outbound SMTPS TCP/465 Connection to your SMTP server over TLS/SSL Outbound Docker Hub * Connection to Docker Hub to download the required container images Outbound Git provider * Connection to the ports required by your remote Git provider Cluster hardware requirements \u00b6 The high-level architecture described in the next section is important in understanding how Codacy uses and allocates hardware resources. Below we also provide guidance on resource provisioning for typical scenarios . For a custom hardware resource recommendation, please contact us at support@codacy.com . Codacy architecture \u00b6 You can look at Codacy separately as two parts: The \"Platform\" contains the UI and other components important to treat and show results The \"Analysis\" is the swarm of workers that run between one and four linters simultaneously, depending on factors such as the number of files or the programming languages used in your projects Since all components are running on a cluster, you can increase the number of replicas in every deployment to give you more resilience and throughput, at a cost of increased resource usage. The following is a simplified overview of how to calculate resource allocation for the \"Platform\" and the \"Analysis\": Component vCPU Memory Platform (1 replica per component) 4 8 GB Analysis (1 Analysis Worker + up to 4 linters) 5 (per Analysis Worker) 10 GB (per Analysis Worker) Standard cluster provisioning \u00b6 The resources recommended on the following table are based on our experience and are also the defaults in the values-production.yaml file, which you might need to adapt taking into account your use case. As described in the section above, Codacy's architecture allows scaling the \"Analysis\" part of the platform, meaning that the resources needed for Codacy depend mainly on the rate of commits done by your team that Codacy will be analyzing. Important For MicroK8s clusters we added an extra 1.5 vCPU and 1.5 GB memory to the \"Platform\" to account for the MicroK8s platform itself running on the same machine. Installation type Replicas per component Max. concurrent analysis Platform resources Analysis resources ~ Total resources Kubernetes Small Installation 1 2 4 vCPUs 8 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM Kubernetes Medium Installation (default) 2 4 8 vCPUs 16 GB RAM 20 vCPUs 40 GB RAM 32 vCPUs 64 GB RAM Kubernetes Big Installation 2+ 10+ 8+ vCPUs 16+ GB RAM 50+ vCPUs 100+ GB RAM 60+ vCPUs 110+ GB RAM MicroK8s Minimum 1 2 5.5 vCPUs 9.5 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM MicroK8s Recommended (default) 1+ 2 9.5+ vCPUs 17.5+ GB RAM 10 vCPUs 20 GB RAM 21+ vCPUs 40+ GB RAM The storage requirements recommended on the following table depend mainly on the number of repositories that Codacy will be analyzing and should be used as a guideline to determine your installation requirements. Component Bundled in the chart? Minimum recommended NFS Yes 200 GB RabbitMQ Yes 8 GB Minio Yes 20 GB PostgreSQL No (external DB recommended) 500 GB+ PostgreSQL server setup \u00b6 Codacy requires a database server to persist data that must satisfy the following requirements: The infrastructure hosting the database server must be provisioned with the hardware requirements described below The DBMS server must be PostgreSQL version 9.6 The PostgreSQL server must be configured to accept connections from the cluster The Codacy databases and a dedicated user must be created using the instructions below Important Google, the developer of Kubernetes, doesn't recommend running database servers on your cluster . As such, consider using a managed solution like Amazon RDS or Google Cloud SQL, or running the PostgreSQL server on a dedicated virtual machine. We recommend that you use a managed solution to reduce maintenance and configuration costs of the PostgreSQL server. The main cloud providers all have this service that you can use, for example: Amazon RDS for PostgreSQL or Amazon Aurora PostgreSQL-Compatible Edition Azure Database for PostgreSQL Google Cloud SQL for PostgreSQL Digital Ocean Managed Databases PostgreSQL hardware requirements \u00b6 The following are the minimum specifications recommended for provisioning the PostgreSQL server: vCPUs Memory Storage Max. concurrent connections 4 8 GB 500 GB+ 300 Preparing PostgreSQL for Codacy \u00b6 Before installing Codacy you must create a set of databases that will be used by Codacy to persist data. We also recommend that you create a dedicated user for Codacy, with access permissions only to the databases that are specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command line client: psql -U postgres -h <PostgreSQL server hostname> Create the dedicated user that Codacy will use to connect to PostgreSQL. Make sure that you change the username and password to suit your security needs: CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; Take note of the username and password you define, as you will require them later to configure the connection from Codacy to the PostgreSQL server. Make sure that you can connect to the PostgreSQL database using the newly created user. For example, using the psql command line client: psql -U codacy -d postgres -h <PostgreSQL server hostname> Create the databases required by Codacy: CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; CREATE DATABASE crow WITH OWNER = codacy ;","title":"System requirements"},{"location":"requirements/#system-requirements","text":"Before installing Codacy you must ensure that you have the following infrastructure correctly provisioned and configured: Kubernetes or MicroK8s cluster PostgreSQL server The next sections describe in detail how to set up these prerequisites.","title":"System requirements"},{"location":"requirements/#kubernetes-or-microk8s-cluster-setup","text":"The cluster running Codacy must satisfy the following requirements: The infrastructure hosting the cluster must be provisioned with the hardware and networking requirements described below The orchestration platform managing the cluster must be one of: Kubernetes version 1.14.* or 1.15.* MicroK8s version 1.15 Tiller, the server part of Helm version 2.16 , must be installed in the cluster The NGINX Ingress controller must be installed and correctly set up in the cluster","title":"Kubernetes or MicroK8s cluster setup"},{"location":"requirements/#cluster-networking-requirements","text":"The cluster must be configured to accept and establish connections on the following ports: Service Protocol/Port Notes Inbound SSH TCP/22 MicroK8s only , to access the infrastructure remotely. Inbound HTTP TCP/80 Allow access to the Codacy website and API endpoints Inbound HTTPS TCP/443 Allow access to the Codacy website and API endpoints Outbound PostgreSQL TCP/5432 Connection to the PostgreSQL DBMS Outbound SMTP TCP/25 Connection to your SMTP server Outbound SMTPS TCP/465 Connection to your SMTP server over TLS/SSL Outbound Docker Hub * Connection to Docker Hub to download the required container images Outbound Git provider * Connection to the ports required by your remote Git provider","title":"Cluster networking requirements"},{"location":"requirements/#cluster-hardware-requirements","text":"The high-level architecture described in the next section is important in understanding how Codacy uses and allocates hardware resources. Below we also provide guidance on resource provisioning for typical scenarios . For a custom hardware resource recommendation, please contact us at support@codacy.com .","title":"Cluster hardware requirements"},{"location":"requirements/#codacy-architecture","text":"You can look at Codacy separately as two parts: The \"Platform\" contains the UI and other components important to treat and show results The \"Analysis\" is the swarm of workers that run between one and four linters simultaneously, depending on factors such as the number of files or the programming languages used in your projects Since all components are running on a cluster, you can increase the number of replicas in every deployment to give you more resilience and throughput, at a cost of increased resource usage. The following is a simplified overview of how to calculate resource allocation for the \"Platform\" and the \"Analysis\": Component vCPU Memory Platform (1 replica per component) 4 8 GB Analysis (1 Analysis Worker + up to 4 linters) 5 (per Analysis Worker) 10 GB (per Analysis Worker)","title":"Codacy architecture"},{"location":"requirements/#standard-cluster-provisioning","text":"The resources recommended on the following table are based on our experience and are also the defaults in the values-production.yaml file, which you might need to adapt taking into account your use case. As described in the section above, Codacy's architecture allows scaling the \"Analysis\" part of the platform, meaning that the resources needed for Codacy depend mainly on the rate of commits done by your team that Codacy will be analyzing. Important For MicroK8s clusters we added an extra 1.5 vCPU and 1.5 GB memory to the \"Platform\" to account for the MicroK8s platform itself running on the same machine. Installation type Replicas per component Max. concurrent analysis Platform resources Analysis resources ~ Total resources Kubernetes Small Installation 1 2 4 vCPUs 8 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM Kubernetes Medium Installation (default) 2 4 8 vCPUs 16 GB RAM 20 vCPUs 40 GB RAM 32 vCPUs 64 GB RAM Kubernetes Big Installation 2+ 10+ 8+ vCPUs 16+ GB RAM 50+ vCPUs 100+ GB RAM 60+ vCPUs 110+ GB RAM MicroK8s Minimum 1 2 5.5 vCPUs 9.5 GB RAM 10 vCPUs 20 GB RAM 16 vCPUs 32 GB RAM MicroK8s Recommended (default) 1+ 2 9.5+ vCPUs 17.5+ GB RAM 10 vCPUs 20 GB RAM 21+ vCPUs 40+ GB RAM The storage requirements recommended on the following table depend mainly on the number of repositories that Codacy will be analyzing and should be used as a guideline to determine your installation requirements. Component Bundled in the chart? Minimum recommended NFS Yes 200 GB RabbitMQ Yes 8 GB Minio Yes 20 GB PostgreSQL No (external DB recommended) 500 GB+","title":"Standard cluster provisioning"},{"location":"requirements/#postgresql-server-setup","text":"Codacy requires a database server to persist data that must satisfy the following requirements: The infrastructure hosting the database server must be provisioned with the hardware requirements described below The DBMS server must be PostgreSQL version 9.6 The PostgreSQL server must be configured to accept connections from the cluster The Codacy databases and a dedicated user must be created using the instructions below Important Google, the developer of Kubernetes, doesn't recommend running database servers on your cluster . As such, consider using a managed solution like Amazon RDS or Google Cloud SQL, or running the PostgreSQL server on a dedicated virtual machine. We recommend that you use a managed solution to reduce maintenance and configuration costs of the PostgreSQL server. The main cloud providers all have this service that you can use, for example: Amazon RDS for PostgreSQL or Amazon Aurora PostgreSQL-Compatible Edition Azure Database for PostgreSQL Google Cloud SQL for PostgreSQL Digital Ocean Managed Databases","title":"PostgreSQL server setup"},{"location":"requirements/#postgresql-hardware-requirements","text":"The following are the minimum specifications recommended for provisioning the PostgreSQL server: vCPUs Memory Storage Max. concurrent connections 4 8 GB 500 GB+ 300","title":"PostgreSQL hardware requirements"},{"location":"requirements/#preparing-postgresql-for-codacy","text":"Before installing Codacy you must create a set of databases that will be used by Codacy to persist data. We also recommend that you create a dedicated user for Codacy, with access permissions only to the databases that are specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command line client: psql -U postgres -h <PostgreSQL server hostname> Create the dedicated user that Codacy will use to connect to PostgreSQL. Make sure that you change the username and password to suit your security needs: CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; Take note of the username and password you define, as you will require them later to configure the connection from Codacy to the PostgreSQL server. Make sure that you can connect to the PostgreSQL database using the newly created user. For example, using the psql command line client: psql -U codacy -d postgres -h <PostgreSQL server hostname> Create the databases required by Codacy: CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; CREATE DATABASE crow WITH OWNER = codacy ;","title":"Preparing PostgreSQL for Codacy"},{"location":"configuration/monitoring/","text":"Monitoring \u00b6 Currently, we support two monitoring solutions: Crow : A simple, lightweight, and built-in monitoring solution, that is enabled by default when you install Codacy. Prometheus + Grafana + Loki : A comprehensive third-party monitoring solution, recommended for more advanced usage. The sections below provide details on how to set up each monitoring solution. Setting up monitoring using Crow \u00b6 Crow displays information about the projects that are pending analysis and the jobs currently running on Codacy. Crow is installed alongside Codacy when the Helm chart is deployed to the cluster. By default, you can access Crow as follows: URL: http://<codacy hostname>/monitoring , where <codacy hostname> is the hostname of your Codacy instance Username: codacy Password: C0dacy123 We highly recommend that you define a custom password for Crow, if you haven't already done it when installing Codacy: Edit the value of crow.config.passwordAuth.password in the values-production.yaml file that you used to install Codacy: crow : config : passwordAuth : password : <--- crow password ---> Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml Setting up monitoring using Grafana, Prometheus, and Loki \u00b6 Prometheus is an open-source systems monitoring and alerting toolkit. Logs can be collected using Loki , which is a horizontally-scalable, highly-available, multi-tenant log aggregation system. Its data can be visualized with Grafana , a widely used open source analytics and monitoring solution. This solution is considerably more resource demanding than Crow, and is recommended only for more advanced usage. Furthermore, its installation, configuration, and management require a deeper knowledge of Kubernetes as each component must be carefully tweaked to match your specific use case, using as starting point the .yaml values files provided by us. The instructions below cover the basic installation of the components in this monitoring stack. 1. Installing Prometheus \u00b6 The simplest way to set up Prometheus in your cluster is by using the Prometheus Operator bundle. Add the custom resources required for installing this bundle in your cluster: Important If you are using MicroK8s use microk8s.kubectl instead of kubectl . kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml 2. Installing Loki \u00b6 Obtain the configuration file for Loki, values-loki.yaml , and install it by running the command below. While the default storage class setting for Loki persistence should suit most use cases, you may need to adjust it to your specific Kubernetes installation. For instance, for MicroK8s use storageClassName: microk8s-hostpath . helm repo add loki https://grafana.github.io/loki/charts helm upgrade --install --atomic loki loki/loki --version 0 .17.0 \\ --recreate-pods --namespace codacy --values values-loki.yaml 3. Installing Promtail \u00b6 Promtail is an agent that ships the contents of local logs to a Loki instance. Obtain the configuration file for Promtail, values-promtail.yaml , and install it by running the command below. helm upgrade --install --atomic promtail loki/promtail --version 0 .13.0 \\ --recreate-pods --namespace codacy --values promtail-values.yaml 4. Installing Prometheus and Grafana \u00b6 Obtain the configuration file for the Prometheus Operator bundle , values-prometheus-operator.yaml . Then: Edit the Grafana password for the admin user in the values-prometheus-operator.yaml file. Install the bundle on your cluster by running the command below. helm upgrade --install --atomic monitoring stable/prometheus-operator --version 6 .9.3 \\ --recreate-pods --namespace codacy --values values-prometheus-operator.yaml Follow the Kubernetes documentation to access the Grafana service that is now running on your cluster, using the method that best suits your use case. 5. Enable Service Dashboards \u00b6 Now that you have Prometheus and Grafana installed you can enable serviceMonitors and grafana_dashboards for Codacy components. Create a file named values-monitoring.yaml with the following content: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Apply this configuration by performing a Helm upgrade. To do so append --values values-monitoring.yaml --recreate-pods to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-monitoring.yaml --recreate-pods","title":"Monitoring"},{"location":"configuration/monitoring/#monitoring","text":"Currently, we support two monitoring solutions: Crow : A simple, lightweight, and built-in monitoring solution, that is enabled by default when you install Codacy. Prometheus + Grafana + Loki : A comprehensive third-party monitoring solution, recommended for more advanced usage. The sections below provide details on how to set up each monitoring solution.","title":"Monitoring"},{"location":"configuration/monitoring/#setting-up-monitoring-using-crow","text":"Crow displays information about the projects that are pending analysis and the jobs currently running on Codacy. Crow is installed alongside Codacy when the Helm chart is deployed to the cluster. By default, you can access Crow as follows: URL: http://<codacy hostname>/monitoring , where <codacy hostname> is the hostname of your Codacy instance Username: codacy Password: C0dacy123 We highly recommend that you define a custom password for Crow, if you haven't already done it when installing Codacy: Edit the value of crow.config.passwordAuth.password in the values-production.yaml file that you used to install Codacy: crow : config : passwordAuth : password : <--- crow password ---> Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml","title":"Setting up monitoring using Crow"},{"location":"configuration/monitoring/#setting-up-monitoring-using-grafana-prometheus-and-loki","text":"Prometheus is an open-source systems monitoring and alerting toolkit. Logs can be collected using Loki , which is a horizontally-scalable, highly-available, multi-tenant log aggregation system. Its data can be visualized with Grafana , a widely used open source analytics and monitoring solution. This solution is considerably more resource demanding than Crow, and is recommended only for more advanced usage. Furthermore, its installation, configuration, and management require a deeper knowledge of Kubernetes as each component must be carefully tweaked to match your specific use case, using as starting point the .yaml values files provided by us. The instructions below cover the basic installation of the components in this monitoring stack.","title":"Setting up monitoring using Grafana, Prometheus, and Loki"},{"location":"configuration/monitoring/#1-installing-prometheus","text":"The simplest way to set up Prometheus in your cluster is by using the Prometheus Operator bundle. Add the custom resources required for installing this bundle in your cluster: Important If you are using MicroK8s use microk8s.kubectl instead of kubectl . kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml","title":"1. Installing Prometheus"},{"location":"configuration/monitoring/#2-installing-loki","text":"Obtain the configuration file for Loki, values-loki.yaml , and install it by running the command below. While the default storage class setting for Loki persistence should suit most use cases, you may need to adjust it to your specific Kubernetes installation. For instance, for MicroK8s use storageClassName: microk8s-hostpath . helm repo add loki https://grafana.github.io/loki/charts helm upgrade --install --atomic loki loki/loki --version 0 .17.0 \\ --recreate-pods --namespace codacy --values values-loki.yaml","title":"2. Installing Loki"},{"location":"configuration/monitoring/#3-installing-promtail","text":"Promtail is an agent that ships the contents of local logs to a Loki instance. Obtain the configuration file for Promtail, values-promtail.yaml , and install it by running the command below. helm upgrade --install --atomic promtail loki/promtail --version 0 .13.0 \\ --recreate-pods --namespace codacy --values promtail-values.yaml","title":"3. Installing Promtail"},{"location":"configuration/monitoring/#4-installing-prometheus-and-grafana","text":"Obtain the configuration file for the Prometheus Operator bundle , values-prometheus-operator.yaml . Then: Edit the Grafana password for the admin user in the values-prometheus-operator.yaml file. Install the bundle on your cluster by running the command below. helm upgrade --install --atomic monitoring stable/prometheus-operator --version 6 .9.3 \\ --recreate-pods --namespace codacy --values values-prometheus-operator.yaml Follow the Kubernetes documentation to access the Grafana service that is now running on your cluster, using the method that best suits your use case.","title":"4. Installing Prometheus and Grafana"},{"location":"configuration/monitoring/#5-enable-service-dashboards","text":"Now that you have Prometheus and Grafana installed you can enable serviceMonitors and grafana_dashboards for Codacy components. Create a file named values-monitoring.yaml with the following content: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Apply this configuration by performing a Helm upgrade. To do so append --values values-monitoring.yaml --recreate-pods to the command used to install Codacy : helm upgrade ( ...options used to install Codacy... ) \\ --values values-monitoring.yaml --recreate-pods","title":"5. Enable Service Dashboards"},{"location":"configuration/integrations/bitbucket-cloud/","text":"Bitbucket Cloud \u00b6 Follow the instructions below to set up the Codacy integration with Bitbucket Cloud. Create an OAuth consumer \u00b6 To integrate Codacy with Bitbucket Cloud, you must register an OAuth consumer for Codacy on Bitbucket. You can create a consumer on any existing individual or team account. To create a consumer, do the following: On Bitbucket, click on your avatar on the bottom left-hand corner and select Bitbucket settings . Select OAuth on the left sidebar and click the button Add consumer . Fill in the fields to create the OAuth consumer: Name: Name of the OAuth consumer. For example, Codacy . Callback URL: Copy the URL below, replacing the HTTP protocol and hostname with the correct values for your Codacy instance. https://codacy.example.com/login/Bitbucket?codacy_skip_ga=1 This is a private consumer: Enable the check box. Add the permissions: Account: Write Team membership: Read Projects: Read Repositories: Admin Pull requests: Write Issues: Write Webhooks: Read and write Click Save, and then click the name of the new OAuth consumer to take note of the generated key and secret. Configure Bitbucket Cloud on Codacy \u00b6 After creating the OAuth consumer on Bitbucket Cloud, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.bitbucket.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the OAuth consumer: global : bitbucket : enabled : \"true\" login : \"true\" # Show login button for Bitbucket Cloud key : \"\" # OAuth consumer key secret : \"\" # OAuth consumer secret Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use Bitbucket Cloud to authenticate to Codacy.","title":"Bitbucket Cloud"},{"location":"configuration/integrations/bitbucket-cloud/#bitbucket-cloud","text":"Follow the instructions below to set up the Codacy integration with Bitbucket Cloud.","title":"Bitbucket Cloud"},{"location":"configuration/integrations/bitbucket-cloud/#create-oauth","text":"To integrate Codacy with Bitbucket Cloud, you must register an OAuth consumer for Codacy on Bitbucket. You can create a consumer on any existing individual or team account. To create a consumer, do the following: On Bitbucket, click on your avatar on the bottom left-hand corner and select Bitbucket settings . Select OAuth on the left sidebar and click the button Add consumer . Fill in the fields to create the OAuth consumer: Name: Name of the OAuth consumer. For example, Codacy . Callback URL: Copy the URL below, replacing the HTTP protocol and hostname with the correct values for your Codacy instance. https://codacy.example.com/login/Bitbucket?codacy_skip_ga=1 This is a private consumer: Enable the check box. Add the permissions: Account: Write Team membership: Read Projects: Read Repositories: Admin Pull requests: Write Issues: Write Webhooks: Read and write Click Save, and then click the name of the new OAuth consumer to take note of the generated key and secret.","title":"Create an OAuth consumer"},{"location":"configuration/integrations/bitbucket-cloud/#configure","text":"After creating the OAuth consumer on Bitbucket Cloud, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.bitbucket.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the OAuth consumer: global : bitbucket : enabled : \"true\" login : \"true\" # Show login button for Bitbucket Cloud key : \"\" # OAuth consumer key secret : \"\" # OAuth consumer secret Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use Bitbucket Cloud to authenticate to Codacy.","title":"Configure Bitbucket Cloud on Codacy"},{"location":"configuration/integrations/bitbucket-server/","text":"Bitbucket Server \u00b6 Follow the instructions below to set up the Codacy integration with Bitbucket Server. Create a Bitbucket Server application link \u00b6 To integrate Codacy with Bitbucket Server, you must create an application link on your Bitbucket Server instance: Since Bitbucket Server uses OAuth1, you need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance. Create a key pair using the RSA algorithm in the PKCS#8 format by executing the command: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/configuration/integrations/generate-bitbucket-server-secrets.sh ) Store the keys in a safe place for usage in the next steps and as a backup. Open <bitbucket server url>/plugins/servlet/applinks/listApplicationLinks , where <bitbucket server url> is the URL of your Bitbucket Server instance. Create a new application link with the URL of your Codacy instance. Fill in the fields: Application Name: Name of the application. For example, Codacy . Application Type: Select Generic Application . The remaining fields should be left blank. After creating the link, click Edit to add an incoming authentication. Fill in the fields of the incomming authentication: Consumer Key: Enter the consumerKey generated previously. Consumer Name: Name of the consumer. For example, Codacy . Public Key: Enter the consumerPublicKey generated previously. The remaining fields should be left blank. Configure Bitbucket Server on Codacy \u00b6 After creating the Bitbucket Server application link, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.bitbucketEnterprise.enabled: \"true\" and define the remaining values as described below and with the information obtained when you created the Bitbucket Server application link: bitbucketEnterprise : enabled : \"true\" login : \"true\" # Show login button for Bitbucket Server hostname : \"bitbucket.example.com\" # Hostname of your Bitbucket Server instance protocol : \"https\" # Protocol of your Bitbucket Server instance port : 7990 # Port of your Bitbucket Server instance consumerKey : \"\" # Generated when creating the Bitbucket Server application link consumerPublicKey : \"\" # Generated when creating the Bitbucket Server application link consumerPrivateKey : \"\" # Generated when creating the Bitbucket Server application link Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use Bitbucket Server to authenticate to Codacy.","title":"Bitbucket Server"},{"location":"configuration/integrations/bitbucket-server/#bitbucket-server","text":"Follow the instructions below to set up the Codacy integration with Bitbucket Server.","title":"Bitbucket Server"},{"location":"configuration/integrations/bitbucket-server/#create-a-bitbucket-server-application-link","text":"To integrate Codacy with Bitbucket Server, you must create an application link on your Bitbucket Server instance: Since Bitbucket Server uses OAuth1, you need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance. Create a key pair using the RSA algorithm in the PKCS#8 format by executing the command: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/configuration/integrations/generate-bitbucket-server-secrets.sh ) Store the keys in a safe place for usage in the next steps and as a backup. Open <bitbucket server url>/plugins/servlet/applinks/listApplicationLinks , where <bitbucket server url> is the URL of your Bitbucket Server instance. Create a new application link with the URL of your Codacy instance. Fill in the fields: Application Name: Name of the application. For example, Codacy . Application Type: Select Generic Application . The remaining fields should be left blank. After creating the link, click Edit to add an incoming authentication. Fill in the fields of the incomming authentication: Consumer Key: Enter the consumerKey generated previously. Consumer Name: Name of the consumer. For example, Codacy . Public Key: Enter the consumerPublicKey generated previously. The remaining fields should be left blank.","title":"Create a Bitbucket Server application link"},{"location":"configuration/integrations/bitbucket-server/#configure-bitbucket-server-on-codacy","text":"After creating the Bitbucket Server application link, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.bitbucketEnterprise.enabled: \"true\" and define the remaining values as described below and with the information obtained when you created the Bitbucket Server application link: bitbucketEnterprise : enabled : \"true\" login : \"true\" # Show login button for Bitbucket Server hostname : \"bitbucket.example.com\" # Hostname of your Bitbucket Server instance protocol : \"https\" # Protocol of your Bitbucket Server instance port : 7990 # Port of your Bitbucket Server instance consumerKey : \"\" # Generated when creating the Bitbucket Server application link consumerPublicKey : \"\" # Generated when creating the Bitbucket Server application link consumerPrivateKey : \"\" # Generated when creating the Bitbucket Server application link Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use Bitbucket Server to authenticate to Codacy.","title":"Configure Bitbucket Server on Codacy"},{"location":"configuration/integrations/email/","text":"Emails \u00b6 Follow the instructions below to set up Codacy to send emails using your SMTP server: Edit the file values-production.yaml that you used to install Codacy . Set global.email.enabled: \"true\" and define the remaining values with the credentials for your SMTP server: email : enabled : \"true\" replyTo : \"notifications@mycompany.com\" # Reply-to field on sent emails smtp : protocol : \"smtp\" # SMTP protocol to use, either smtps or smtp hostname : \"smtp.example.com\" # Hostname of your SMTP server # username: \"\" # Optional username to authenticate on your SMTP server # password: \"\" # Optional password to authenticate on your SMTP server # port: 25 # Optional port of your SMTP server, the default is 25 Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to: Invite new users via email Receive commit and pull request email notifications","title":"Emails"},{"location":"configuration/integrations/email/#emails","text":"Follow the instructions below to set up Codacy to send emails using your SMTP server: Edit the file values-production.yaml that you used to install Codacy . Set global.email.enabled: \"true\" and define the remaining values with the credentials for your SMTP server: email : enabled : \"true\" replyTo : \"notifications@mycompany.com\" # Reply-to field on sent emails smtp : protocol : \"smtp\" # SMTP protocol to use, either smtps or smtp hostname : \"smtp.example.com\" # Hostname of your SMTP server # username: \"\" # Optional username to authenticate on your SMTP server # password: \"\" # Optional password to authenticate on your SMTP server # port: 25 # Optional port of your SMTP server, the default is 25 Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to: Invite new users via email Receive commit and pull request email notifications","title":"Emails"},{"location":"configuration/integrations/github-app-create/","text":"Creating a GitHub App \u00b6 You must create and correctly set up a GitHub App to allow Codacy to integrate with GitHub. To create the GitHub App: If you're using GitHub Cloud , open https://github.com/settings/apps/new . If you're using GitHub Enterprise , open https://github.example.com/settings/apps/new , replacing the HTTP protocol and hostname with the correct values for your GitHub Enterprise instance. Configure the new GitHub App using the values listed on the table below, replacing https://codacy.example.com with the correct base URL of your Codacy instance. Field Value GitHub App name Codacy Homepage URL https://codacy.example.com User authorization callback URL https://codacy.example.com Webhook URL https://codacy.example.com/2.0/events/gh/organization Repository permissions Administration Read & write Checks Read & write Issues Read & write Metadata Read only Pull requests Read & write Webhooks Read & write Commit statuses Read & write Organization permissions Members Read only Webhooks Read & write User permissions Email addresses Read only Git SSH keys Read & write Where can this GitHub App be installed? Any account Scroll to the bottom of the page, click Generate a private key , and save the .pem file containing the private key. Take note of the following information, as you'll need it to configure Codacy: GitHub App name App ID Client ID Client secret Private key (contents of the .pem file generated in the previous step)","title":"Creating a GitHub App"},{"location":"configuration/integrations/github-app-create/#creating-a-github-app","text":"You must create and correctly set up a GitHub App to allow Codacy to integrate with GitHub. To create the GitHub App: If you're using GitHub Cloud , open https://github.com/settings/apps/new . If you're using GitHub Enterprise , open https://github.example.com/settings/apps/new , replacing the HTTP protocol and hostname with the correct values for your GitHub Enterprise instance. Configure the new GitHub App using the values listed on the table below, replacing https://codacy.example.com with the correct base URL of your Codacy instance. Field Value GitHub App name Codacy Homepage URL https://codacy.example.com User authorization callback URL https://codacy.example.com Webhook URL https://codacy.example.com/2.0/events/gh/organization Repository permissions Administration Read & write Checks Read & write Issues Read & write Metadata Read only Pull requests Read & write Webhooks Read & write Commit statuses Read & write Organization permissions Members Read only Webhooks Read & write User permissions Email addresses Read only Git SSH keys Read & write Where can this GitHub App be installed? Any account Scroll to the bottom of the page, click Generate a private key , and save the .pem file containing the private key. Take note of the following information, as you'll need it to configure Codacy: GitHub App name App ID Client ID Client secret Private key (contents of the .pem file generated in the previous step)","title":"Creating a GitHub App"},{"location":"configuration/integrations/github-cloud/","text":"GitHub Cloud \u00b6 Follow the instructions below to set up the Codacy integration with GitHub Cloud: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml that you used to install Codacy . Set global.github.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitHub App: github : enabled : \"true\" login : \"true\" # Show login button for GitHub Cloud clientId : \"\" # Client ID clientSecret : \"\" # Client secret app : name : \"codacy\" # GitHub App name id : \"1234\" # App ID privateKey : \"\" # Contents of the .pem file without newlines Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitHub Cloud to authenticate to Codacy.","title":"GitHub Cloud"},{"location":"configuration/integrations/github-cloud/#github-cloud","text":"Follow the instructions below to set up the Codacy integration with GitHub Cloud: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml that you used to install Codacy . Set global.github.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitHub App: github : enabled : \"true\" login : \"true\" # Show login button for GitHub Cloud clientId : \"\" # Client ID clientSecret : \"\" # Client secret app : name : \"codacy\" # GitHub App name id : \"1234\" # App ID privateKey : \"\" # Contents of the .pem file without newlines Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitHub Cloud to authenticate to Codacy.","title":"GitHub Cloud"},{"location":"configuration/integrations/github-enterprise/","text":"GitHub Enterprise \u00b6 Follow the instructions below to set up the Codacy integration with GitHub Enterprise: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml that you used to install Codacy . Set global.githubEnterprise.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitHub App: githubEnterprise : enabled : \"true\" login : \"true\" # Show login button for GitHub Enterprise hostname : \"github.example.com\" # Hostname of your GitHub Enterprise instance protocol : \"https\" # Protocol of your GitHub Enterprise instance port : 443 # Port of your GitHub Enterprise instance disableSSL : \"false\" # Disable certificate validation isPrivateMode : \"true\" # Status of private mode on your GitHub Enterprise instance clientId : \"\" # GitHub App Client ID clientSecret : \"\" # GitHub App Client secret app : name : \"codacy\" # GitHub App name id : \"1234\" # GitHub App ID privateKey : \"\" # Contents of the .pem file without newlines Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitHub Enterprise to authenticate to Codacy.","title":"GitHub Enterprise"},{"location":"configuration/integrations/github-enterprise/#github-enterprise","text":"Follow the instructions below to set up the Codacy integration with GitHub Enterprise: Follow the instructions on creating a GitHub App . Edit the file values-production.yaml that you used to install Codacy . Set global.githubEnterprise.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitHub App: githubEnterprise : enabled : \"true\" login : \"true\" # Show login button for GitHub Enterprise hostname : \"github.example.com\" # Hostname of your GitHub Enterprise instance protocol : \"https\" # Protocol of your GitHub Enterprise instance port : 443 # Port of your GitHub Enterprise instance disableSSL : \"false\" # Disable certificate validation isPrivateMode : \"true\" # Status of private mode on your GitHub Enterprise instance clientId : \"\" # GitHub App Client ID clientSecret : \"\" # GitHub App Client secret app : name : \"codacy\" # GitHub App name id : \"1234\" # GitHub App ID privateKey : \"\" # Contents of the .pem file without newlines Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitHub Enterprise to authenticate to Codacy.","title":"GitHub Enterprise"},{"location":"configuration/integrations/gitlab-cloud/","text":"GitLab Cloud \u00b6 Follow the instructions below to set up the Codacy integration with GitLab Cloud. Create a GitLab application \u00b6 To integrate Codacy with GitLab Cloud, you must create a GitLab application: Open https://gitlab.com/profile/applications . Fill in the fields to register your Codacy instance on GitLab: Name: Name of the application. For example, Codacy . Redirect URI: Copy the URLs below, replacing the HTTP protocol and hostname with the correct values for your Codacy instance. This field is case sensitive. https://codacy.example.com/login/GitLab https://codacy.example.com/add/addProvider/GitLab https://codacy.example.com/add/addService/GitLab https://codacy.example.com/add/addPermissions/GitLab Scopes: Enable the scopes: api read_user read_repository Click Save application and take note of the generated Application Id and Secret. Configure GitLab Cloud on Codacy \u00b6 After creating the GitLab application, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.gitlab.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitLab application: gitlab : enabled : \"true\" login : \"true\" # Show login button for GitLab Cloud clientId : \"\" # Application ID clientSecret : \"\" # Secret Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitLab Cloud to authenticate to Codacy.","title":"GitLab Cloud"},{"location":"configuration/integrations/gitlab-cloud/#gitlab-cloud","text":"Follow the instructions below to set up the Codacy integration with GitLab Cloud.","title":"GitLab Cloud"},{"location":"configuration/integrations/gitlab-cloud/#create-application","text":"To integrate Codacy with GitLab Cloud, you must create a GitLab application: Open https://gitlab.com/profile/applications . Fill in the fields to register your Codacy instance on GitLab: Name: Name of the application. For example, Codacy . Redirect URI: Copy the URLs below, replacing the HTTP protocol and hostname with the correct values for your Codacy instance. This field is case sensitive. https://codacy.example.com/login/GitLab https://codacy.example.com/add/addProvider/GitLab https://codacy.example.com/add/addService/GitLab https://codacy.example.com/add/addPermissions/GitLab Scopes: Enable the scopes: api read_user read_repository Click Save application and take note of the generated Application Id and Secret.","title":"Create a GitLab application"},{"location":"configuration/integrations/gitlab-cloud/#configure","text":"After creating the GitLab application, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.gitlab.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitLab application: gitlab : enabled : \"true\" login : \"true\" # Show login button for GitLab Cloud clientId : \"\" # Application ID clientSecret : \"\" # Secret Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitLab Cloud to authenticate to Codacy.","title":"Configure GitLab Cloud on Codacy"},{"location":"configuration/integrations/gitlab-enterprise/","text":"GitLab Enterprise \u00b6 Follow the instructions below to set up the Codacy integration with GitLab Enterprise: Create a GitLab application \u00b6 To integrate Codacy with GitLab Enterprise, you must create a GitLab application: Open <gitlab enterprise url>/profile/applications , where <gitlab enterprise url> is the URL of your GitLab Enterprise instance. Fill in the fields to register your Codacy instance on GitLab: Name: Name of the application. For example, Codacy . Redirect URI: Copy the URLs below, replacing the HTTP protocol and hostname with the correct values for your Codacy instance. This field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise https://codacy.example.com/add/addPermissions/GitLabEnterprise Scopes: Enable the scopes: api read_user read_repository openid Click Save application and take note of the generated Application Id and Secret. Configure GitLab Enterprise on Codacy \u00b6 After creating the GitLab application, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.gitlabEnterprise.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitLab application: gitlabEnterprise : enabled : \"true\" login : \"true\" # Show login button for GitLab Enterprise hostname : \"gitlab.example.com\" # Hostname of your GitLab Enterprise instance protocol : \"https\" # Protocol of your GitLab Enterprise instance port : 443 # Port of your GitLab Enterprise instance clientId : \"\" # Application ID clientSecret : \"\" # Secret Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitLab Enterprise to authenticate to Codacy.","title":"GitLab Enterprise"},{"location":"configuration/integrations/gitlab-enterprise/#gitlab-enterprise","text":"Follow the instructions below to set up the Codacy integration with GitLab Enterprise:","title":"GitLab Enterprise"},{"location":"configuration/integrations/gitlab-enterprise/#create-application","text":"To integrate Codacy with GitLab Enterprise, you must create a GitLab application: Open <gitlab enterprise url>/profile/applications , where <gitlab enterprise url> is the URL of your GitLab Enterprise instance. Fill in the fields to register your Codacy instance on GitLab: Name: Name of the application. For example, Codacy . Redirect URI: Copy the URLs below, replacing the HTTP protocol and hostname with the correct values for your Codacy instance. This field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise https://codacy.example.com/add/addPermissions/GitLabEnterprise Scopes: Enable the scopes: api read_user read_repository openid Click Save application and take note of the generated Application Id and Secret.","title":"Create a GitLab application"},{"location":"configuration/integrations/gitlab-enterprise/#configure","text":"After creating the GitLab application, you must configure it on Codacy: Edit the file values-production.yaml that you used to install Codacy . Set global.gitlabEnterprise.enabled: \"true\" and define the remaining values as described below using the information obtained when you created the GitLab application: gitlabEnterprise : enabled : \"true\" login : \"true\" # Show login button for GitLab Enterprise hostname : \"gitlab.example.com\" # Hostname of your GitLab Enterprise instance protocol : \"https\" # Protocol of your GitLab Enterprise instance port : 443 # Port of your GitLab Enterprise instance clientId : \"\" # Application ID clientSecret : \"\" # Secret Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml After this is done you will be able to use GitLab Enterprise to authenticate to Codacy.","title":"Configure GitLab Enterprise on Codacy"},{"location":"infrastructure/aks-quickstart/","text":"Creating an AKS cluster \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"Creating an AKS cluster"},{"location":"infrastructure/aks-quickstart/#creating-an-aks-cluster","text":"","title":"Creating an AKS cluster"},{"location":"infrastructure/aks-quickstart/#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"infrastructure/aks-quickstart/#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"infrastructure/aks-quickstart/#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"infrastructure/aks-quickstart/#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"infrastructure/eks-quickstart/","text":"Creating an Amazon EKS cluster \u00b6 Follow the instructions below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform. 1. Prepare your environment \u00b6 Prepare your environment to set up the Amazon EKS cluster: Make sure that you have the following tools installed on your machine: Git version >= 2.0.0 AWS CLI version 1 Terraform version >= 0.12 Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the Terraform documentation , if your .aws/credentials are more complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Codacy chart repository and change to the directory that includes the provided Terraform configuration files: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ This folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and a DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster 2. Set up the Terraform state storage backend \u00b6 The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these Terraform templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply This creates an Amazon S3 bucket with a unique name to save the infrastructure state. Take note of the value of state_bucket_name in the output of the command. Edit the main/config.tf and setup/config.tf files and follow the instructions included in the comments to set the name of the Amazon S3 bucket created above and enable the use of the backend in those infrastructure stacks. 3. Create a vanilla Amazon EKS cluster \u00b6 Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type and number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file and set the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section. 4. Set up the cluster to run Codacy \u00b6 Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply Connect to the Kubernetes Dashboard: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token Uninstalling the Amazon EKS cluster \u00b6 Warning If you proceed beyond this point you'll permanently delete and break things. Clean up your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly you must first disable these settings by editing the file backend/state_and_lock.tf and following the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Creating an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#creating-an-amazon-eks-cluster","text":"Follow the instructions below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform.","title":"Creating an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#1-prepare-your-environment","text":"Prepare your environment to set up the Amazon EKS cluster: Make sure that you have the following tools installed on your machine: Git version >= 2.0.0 AWS CLI version 1 Terraform version >= 0.12 Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the Terraform documentation , if your .aws/credentials are more complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Codacy chart repository and change to the directory that includes the provided Terraform configuration files: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ This folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and a DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster","title":"1. Prepare your environment"},{"location":"infrastructure/eks-quickstart/#2-set-up-the-terraform-state-storage-backend","text":"The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these Terraform templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply This creates an Amazon S3 bucket with a unique name to save the infrastructure state. Take note of the value of state_bucket_name in the output of the command. Edit the main/config.tf and setup/config.tf files and follow the instructions included in the comments to set the name of the Amazon S3 bucket created above and enable the use of the backend in those infrastructure stacks.","title":"2. Set up the Terraform state storage backend"},{"location":"infrastructure/eks-quickstart/#3-create-a-vanilla-amazon-eks-cluster","text":"Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type and number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file and set the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section.","title":"3. Create a vanilla Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#4-set-up-the-cluster-to-run-codacy","text":"Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply Connect to the Kubernetes Dashboard: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token","title":"4. Set up the cluster to run Codacy"},{"location":"infrastructure/eks-quickstart/#uninstalling-the-amazon-eks-cluster","text":"Warning If you proceed beyond this point you'll permanently delete and break things. Clean up your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly you must first disable these settings by editing the file backend/state_and_lock.tf and following the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Uninstalling the Amazon EKS cluster"},{"location":"infrastructure/microk8s-quickstart/","text":"Creating a MicroK8s cluster \u00b6 MicroK8s is a lightweight, fully conformant, single-package Kubernetes developed by Canonical. The project is publicly available on GitHub . Follow the instructions below to set up a MicroK8s instance from scratch, including all the necessary dependencies and configurations. As part of this process, the Helm client ( helm ) and the Helm server ( tiller ) will be installed on the MicroK8s instance: helm is the command-line client responsible for resolving the configuration of the chart to be installed and issuing the correct install commands onto the Helm server. tiller is the in-cluster server responsible for receiving the install commands issued by the Helm client and managing the lifecycle of the components that have been installed. 1. Prepare your environment \u00b6 Prepare your environment to set up the MicroK8s instance. You will need a machine running Ubuntu Server 18.04 LTS that: Is correctly provisioned with the resources described for MicroK8s in the system requirements Is able to establish a connection to the PostgreSQL instance described in the system requirements The next steps assume that you are starting from a clean install of Ubuntu Server and require that you run commands on a local or remote command line session on the machine. 2. Installing MicroK8s \u00b6 Install MicroK8s on the machine: Make sure that the package nfs-common is installed: sudo apt update && sudo apt install nfs-common -y Install MicroK8s from the 1.15/stable channel: sudo snap install microk8s --classic --channel = 1 .15/stable sudo usermod -a -G microk8s $USER sudo su - $USER Check that MicroK8s is running: microk8s.status --wait-ready 3. Configuring MicroK8s \u00b6 Now that MicroK8s is running on the machine we can proceed to enabling the necessary addons and installing the Helm client and server: Configure MicroK8s to allow privileged containers: sudo mkdir -p /var/snap/microk8s/current/args sudo echo \"--allow-privileged=true\" >> /var/snap/microk8s/current/args/kube-apiserver microk8s.status --wait-ready Enable the following MicroK8s addons: microk8s.enable dns microk8s.status --wait-ready microk8s.enable storage microk8s.status --wait-ready microk8s.enable ingress microk8s.status --wait-ready Important Check the output of the commands to make sure that all the addons are enabled correctly. If by chance any of the addons fails to be enabled, re-execute the microk8s.enable command for that addon. Restart MicroK8s and its services to make sure that all configurations are working: microk8s.stop microk8s.start microk8s.status --wait-ready Install version v2.16.3 of the Helm client: sudo snap install helm --classic --channel = 2 .16/stable Install the Helm server: microk8s.kubectl create serviceaccount --namespace kube-system tiller microk8s.kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init --service-account tiller The addons are now enabled and the MicroK8s instance bootstrapped. However, we must wait for some MicroK8s pods to be ready, as failing to do so can result in the pods entering a CrashLoopBackoff state: microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = kube-dns microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = hostpath-provisioner # If the following command fails, you probably installed the wrong MicroK8s version microk8s.kubectl wait -n default --for = condition = Ready pod -l name = nginx-ingress-microk8s microk8s.kubectl -n kube-system wait --for = condition = Ready pod -l name = tiller Verify that the MicroK8s configuration was successful: microk8s.status --wait-ready The output of the command should be the following: microk8s is running addons: knative: disabled jaeger: disabled fluentd: disabled gpu: disabled cilium: disabled storage: enabled registry: disabled rbac: disabled ingress: enabled dns: enabled metrics-server: disabled linkerd: disabled prometheus: disabled istio: disabled dashboard: disabled After these steps you have ensured that DNS, HTTP, and NGINX Ingress are enabled and working properly inside the MicroK8s instance. Notes on installing Codacy \u00b6 You can now follow the generic Codacy installation instructions but please note the following: You must execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, we suggest that you create an alias so that you can run the commands directly as provided on the instructions: alias kubectl = microk8s.kubectl When running the helm upgrade command that installs the Codacy chart, you will be instructed to also use the file values-microk8s.yaml that downsizes some component limits, making it easier to fit Codacy in the lightweight MicroK8s solution.","title":"Creating a MicroK8s cluster"},{"location":"infrastructure/microk8s-quickstart/#creating-a-microk8s-cluster","text":"MicroK8s is a lightweight, fully conformant, single-package Kubernetes developed by Canonical. The project is publicly available on GitHub . Follow the instructions below to set up a MicroK8s instance from scratch, including all the necessary dependencies and configurations. As part of this process, the Helm client ( helm ) and the Helm server ( tiller ) will be installed on the MicroK8s instance: helm is the command-line client responsible for resolving the configuration of the chart to be installed and issuing the correct install commands onto the Helm server. tiller is the in-cluster server responsible for receiving the install commands issued by the Helm client and managing the lifecycle of the components that have been installed.","title":"Creating a MicroK8s cluster"},{"location":"infrastructure/microk8s-quickstart/#1-prepare-your-environment","text":"Prepare your environment to set up the MicroK8s instance. You will need a machine running Ubuntu Server 18.04 LTS that: Is correctly provisioned with the resources described for MicroK8s in the system requirements Is able to establish a connection to the PostgreSQL instance described in the system requirements The next steps assume that you are starting from a clean install of Ubuntu Server and require that you run commands on a local or remote command line session on the machine.","title":"1. Prepare your environment"},{"location":"infrastructure/microk8s-quickstart/#2-installing-microk8s","text":"Install MicroK8s on the machine: Make sure that the package nfs-common is installed: sudo apt update && sudo apt install nfs-common -y Install MicroK8s from the 1.15/stable channel: sudo snap install microk8s --classic --channel = 1 .15/stable sudo usermod -a -G microk8s $USER sudo su - $USER Check that MicroK8s is running: microk8s.status --wait-ready","title":"2. Installing MicroK8s"},{"location":"infrastructure/microk8s-quickstart/#3-configuring-microk8s","text":"Now that MicroK8s is running on the machine we can proceed to enabling the necessary addons and installing the Helm client and server: Configure MicroK8s to allow privileged containers: sudo mkdir -p /var/snap/microk8s/current/args sudo echo \"--allow-privileged=true\" >> /var/snap/microk8s/current/args/kube-apiserver microk8s.status --wait-ready Enable the following MicroK8s addons: microk8s.enable dns microk8s.status --wait-ready microk8s.enable storage microk8s.status --wait-ready microk8s.enable ingress microk8s.status --wait-ready Important Check the output of the commands to make sure that all the addons are enabled correctly. If by chance any of the addons fails to be enabled, re-execute the microk8s.enable command for that addon. Restart MicroK8s and its services to make sure that all configurations are working: microk8s.stop microk8s.start microk8s.status --wait-ready Install version v2.16.3 of the Helm client: sudo snap install helm --classic --channel = 2 .16/stable Install the Helm server: microk8s.kubectl create serviceaccount --namespace kube-system tiller microk8s.kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init --service-account tiller The addons are now enabled and the MicroK8s instance bootstrapped. However, we must wait for some MicroK8s pods to be ready, as failing to do so can result in the pods entering a CrashLoopBackoff state: microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = kube-dns microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = hostpath-provisioner # If the following command fails, you probably installed the wrong MicroK8s version microk8s.kubectl wait -n default --for = condition = Ready pod -l name = nginx-ingress-microk8s microk8s.kubectl -n kube-system wait --for = condition = Ready pod -l name = tiller Verify that the MicroK8s configuration was successful: microk8s.status --wait-ready The output of the command should be the following: microk8s is running addons: knative: disabled jaeger: disabled fluentd: disabled gpu: disabled cilium: disabled storage: enabled registry: disabled rbac: disabled ingress: enabled dns: enabled metrics-server: disabled linkerd: disabled prometheus: disabled istio: disabled dashboard: disabled After these steps you have ensured that DNS, HTTP, and NGINX Ingress are enabled and working properly inside the MicroK8s instance.","title":"3. Configuring MicroK8s"},{"location":"infrastructure/microk8s-quickstart/#notes-on-installing-codacy","text":"You can now follow the generic Codacy installation instructions but please note the following: You must execute all kubectl commands as microk8s.kubectl commands instead. To simplify this, we suggest that you create an alias so that you can run the commands directly as provided on the instructions: alias kubectl = microk8s.kubectl When running the helm upgrade command that installs the Codacy chart, you will be instructed to also use the file values-microk8s.yaml that downsizes some component limits, making it easier to fit Codacy in the lightweight MicroK8s solution.","title":"Notes on installing Codacy"},{"location":"maintenance/database/","text":"Database migration guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -p $SRC_HOSTPORT -U $DB_USER --clean -Fc $db > /tmp/ $db .dump This will dump the file with the .dump extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -p $DEST_HOSTPORT -U $DB_USER -j 8 -d $db -n public --clean $db .dump With the custom format from pg_dump (by using -Fc ) we can now invoke pg_restore with multiple parallel jobs. This should make the restoration of the databases quicker, depending on which value you provide for the number of parallel jobs to execute. We provide a value of 8 parallel jobs in the example above ( -j 8 ). NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 SRC_HOSTPORT = $2 DEST_HOSTNAME = $3 DEST_HOSTPORT = $4 DB_USER = $5 DB_PASSWORD = $6 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -p $SRC_HOSTPORT -U $DB_USER --clean -Fc $db > /tmp/ $db .dump PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -p $DEST_HOSTPORT -U $DB_USER -d $db -n public --clean $db .dump done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com 25060 postgres\u2013instance1.eu-west-1.rds.amazonaws.com 25060 super_user secret_password","title":"Database migration guide"},{"location":"maintenance/database/#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results","title":"Database migration guide"},{"location":"maintenance/database/#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"maintenance/database/#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"maintenance/database/#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -p $SRC_HOSTPORT -U $DB_USER --clean -Fc $db > /tmp/ $db .dump This will dump the file with the .dump extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"maintenance/database/#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -p $DEST_HOSTPORT -U $DB_USER -j 8 -d $db -n public --clean $db .dump With the custom format from pg_dump (by using -Fc ) we can now invoke pg_restore with multiple parallel jobs. This should make the restoration of the databases quicker, depending on which value you provide for the number of parallel jobs to execute. We provide a value of 8 parallel jobs in the example above ( -j 8 ). NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"maintenance/database/#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 SRC_HOSTPORT = $2 DEST_HOSTNAME = $3 DEST_HOSTPORT = $4 DB_USER = $5 DB_PASSWORD = $6 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -p $SRC_HOSTPORT -U $DB_USER --clean -Fc $db > /tmp/ $db .dump PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -p $DEST_HOSTPORT -U $DB_USER -d $db -n public --clean $db .dump done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com 25060 postgres\u2013instance1.eu-west-1.rds.amazonaws.com 25060 super_user secret_password","title":"Sample script"},{"location":"maintenance/uninstall/","text":"Uninstalling Codacy \u00b6 To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of the effects of these commands please run each of the bash subcommands and validate their output. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstalling Codacy"},{"location":"maintenance/uninstall/#uninstalling-codacy","text":"To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of the effects of these commands please run each of the bash subcommands and validate their output. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstalling Codacy"},{"location":"maintenance/upgrade/","text":"Upgrading Codacy \u00b6 NOTE: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f , like helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with: helm get values codacy > codacy.yaml Decide on all the values you need to set. Perform the upgrade, with all --set arguments extracted in step 2: helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrading Codacy"},{"location":"maintenance/upgrade/#upgrading-codacy","text":"NOTE: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f , like helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with: helm get values codacy > codacy.yaml Decide on all the values you need to set. Perform the upgrade, with all --set arguments extracted in step 2: helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrading Codacy"},{"location":"troubleshoot/k8s-cheatsheet/","text":"Kubernetes cheatsheet \u00b6 How to install a custom Codacy version \u00b6 Install \u00b6 sudo git clone git://github.com/codacy/chart -b <YOUR-BRANCH> helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE> Update \u00b6 ( cd chart ; sudo git fetch --all --prune --tags ; sudo git reset --hard origin/<YOUR-BRANCH> ; ) helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE> Clean the namespace \u00b6 helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Check uninstall was successful \u00b6 ps aux | grep -i kubectl Edit configmap \u00b6 kubectl get configmaps and kubectl edit configmap <configmap-name> Restart deployment of daemonset \u00b6 daemonsets \u00b6 kubectl get daemonsets and kubectl rollout restart daemonset/<daemonset-name> deployment \u00b6 kubectl get deployment and kubectl rollout restart deployment/<deployment-name> and kubectl rollout status deployment/<deployment-name> -w Read logs \u00b6 daemonset with multiple containers \u00b6 kubectl logs daemonset/<daemonset-name> <container-name> -f service \u00b6 kubectl get svc and kubectl logs -l $( kubectl get svc/<service-name> -o = json | jq \".spec.selector\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' | sed -e 'H;${x;s/\\n/,/g;s/^,//;p;};d' ) -f Open shell inside container \u00b6 kubectl exec -it daemonset/<daemonset-name> -c <container-name> sh or kubectl exec -it deployment/<deployment-name> sh MicroK8s \u00b6 Session Manager SSH \u00b6 When using AWS Session Manager, to connect to the instance where you installed microk8s, since the CLI is very limited you will benefit from using these aliases: alias kubectl = 'sudo microk8s.kubectl -n <namespace-name>' alias helm = 'sudo helm'","title":"Kubernetes cheatsheet"},{"location":"troubleshoot/k8s-cheatsheet/#kubernetes-cheatsheet","text":"","title":"Kubernetes cheatsheet"},{"location":"troubleshoot/k8s-cheatsheet/#how-to-install-a-custom-codacy-version","text":"","title":"How to install a custom Codacy version"},{"location":"troubleshoot/k8s-cheatsheet/#install","text":"sudo git clone git://github.com/codacy/chart -b <YOUR-BRANCH> helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE>","title":"Install"},{"location":"troubleshoot/k8s-cheatsheet/#update","text":"( cd chart ; sudo git fetch --all --prune --tags ; sudo git reset --hard origin/<YOUR-BRANCH> ; ) helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE>","title":"Update"},{"location":"troubleshoot/k8s-cheatsheet/#clean-the-namespace","text":"helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job &","title":"Clean the namespace"},{"location":"troubleshoot/k8s-cheatsheet/#check-uninstall-was-successful","text":"ps aux | grep -i kubectl","title":"Check uninstall was successful"},{"location":"troubleshoot/k8s-cheatsheet/#edit-configmap","text":"kubectl get configmaps and kubectl edit configmap <configmap-name>","title":"Edit configmap"},{"location":"troubleshoot/k8s-cheatsheet/#restart-deployment-of-daemonset","text":"","title":"Restart deployment of daemonset"},{"location":"troubleshoot/k8s-cheatsheet/#daemonsets","text":"kubectl get daemonsets and kubectl rollout restart daemonset/<daemonset-name>","title":"daemonsets"},{"location":"troubleshoot/k8s-cheatsheet/#deployment","text":"kubectl get deployment and kubectl rollout restart deployment/<deployment-name> and kubectl rollout status deployment/<deployment-name> -w","title":"deployment"},{"location":"troubleshoot/k8s-cheatsheet/#read-logs","text":"","title":"Read logs"},{"location":"troubleshoot/k8s-cheatsheet/#daemonset-with-multiple-containers","text":"kubectl logs daemonset/<daemonset-name> <container-name> -f","title":"daemonset with multiple containers"},{"location":"troubleshoot/k8s-cheatsheet/#service","text":"kubectl get svc and kubectl logs -l $( kubectl get svc/<service-name> -o = json | jq \".spec.selector\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' | sed -e 'H;${x;s/\\n/,/g;s/^,//;p;};d' ) -f","title":"service"},{"location":"troubleshoot/k8s-cheatsheet/#open-shell-inside-container","text":"kubectl exec -it daemonset/<daemonset-name> -c <container-name> sh or kubectl exec -it deployment/<deployment-name> sh","title":"Open shell inside container"},{"location":"troubleshoot/k8s-cheatsheet/#microk8s","text":"","title":"MicroK8s"},{"location":"troubleshoot/k8s-cheatsheet/#session-manager-ssh","text":"When using AWS Session Manager, to connect to the instance where you installed microk8s, since the CLI is very limited you will benefit from using these aliases: alias kubectl = 'sudo microk8s.kubectl -n <namespace-name>' alias helm = 'sudo helm'","title":"Session Manager SSH"},{"location":"troubleshoot/logs-collect/","text":"Collecting logs for Support \u00b6 To help troubleshoot issues, obtain the logs from your Codacy instance and send them to Codacy's Support: Download the logs of the last 7 days as an archive file with the name codacy_logs_<timestamp>.zip by running the following command locally, replacing <namespace> with the namespace in which Codacy was installed: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/troubleshoot/extract-codacy-logs.sh ) \\ -n <namespace> To reduce the size of the compressed archive file, you should retrieve logs for a smaller number of days by replacing <days> with a number between 1 and 7: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/troubleshoot/extract-codacy-logs.sh ) \\ -n <namespace> -d <days> You can also download the script extract-codacy-logs.sh to run it manually. Send the compressed logs to Codacy's support team at support@codacy.com for analysis. If the file is too big, please upload the file to either a cloud storage service such as Google Drive or to a file transfer service such as WeTransfer and send us the link to the file instead.","title":"Collecting logs for Support"},{"location":"troubleshoot/logs-collect/#collecting-logs-for-support","text":"To help troubleshoot issues, obtain the logs from your Codacy instance and send them to Codacy's Support: Download the logs of the last 7 days as an archive file with the name codacy_logs_<timestamp>.zip by running the following command locally, replacing <namespace> with the namespace in which Codacy was installed: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/troubleshoot/extract-codacy-logs.sh ) \\ -n <namespace> To reduce the size of the compressed archive file, you should retrieve logs for a smaller number of days by replacing <days> with a number between 1 and 7: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/troubleshoot/extract-codacy-logs.sh ) \\ -n <namespace> -d <days> You can also download the script extract-codacy-logs.sh to run it manually. Send the compressed logs to Codacy's support team at support@codacy.com for analysis. If the file is too big, please upload the file to either a cloud storage service such as Google Drive or to a file transfer service such as WeTransfer and send us the link to the file instead.","title":"Collecting logs for Support"},{"location":"troubleshoot/troubleshoot/","text":"Troubleshooting Codacy \u00b6 This page includes information to help you troubleshoot issues that you may come across while installing, configuring, and operating Codacy. If the information provided on this page is not enough to solve your issue, contact support@codacy.com providing: The description of the issue All the information that you were able to obtain while following these troubleshooting instructions The collected logs of your Codacy instance Git provider integrations \u00b6 The following sections help you troubleshoot the integration of Codacy with your Git provider. GitHub Cloud and GitHub Enterprise authentication \u00b6 404 error \u00b6 While trying to authenticate on GitHub you get the following error message: This might mean that there is a mismatch in the Client ID that Codacy is using to authenticate on GitHub. To solve this issue: Make sure that the value of clientId in your values-production.yaml file is the same as the Client ID of the GitHub App that you created If the values were different, update your configuration and re-execute the helm upgrade command as described for GitHub Cloud or GitHub Enterprise If the error persists: Take note of the parameter client_id in the URL of the GitHub error page (for example, Iv1.0000000000000000 ) Check if the value of the parameter matches the value of the Client ID of your GitHub App GitLab Cloud and GitLab Enterprise authentication \u00b6 Invalid redirect URI \u00b6 While trying to authenticate on GitLab you get the following error message: This might mean that the redirect URIs are not correct in the GitLab application that Codacy is using to authenticate on GitLab. To solve this issue: Open the GitLab application that you created on GitLab Cloud or GitLab Enterprise Make sure that all the redirect URIs have the correct protocol for the Codacy instance endpoints, either http:// or https:// Make sure that all the redirect URIs have the full path with the correct case, since the field is case-sensitive If the error persists: Take note of the parameter redirect_uri in the URL of the GitLab error page (for example, https%3A%2F%2Fcodacy.example.com%2Flogin%2FGitLab or https%3A%2F%2Fcodacy.example.com%2Flogin%2FGitLabEnterprise ) Decode the value of the parameter using a tool such as urldecoder.com (for example, https://codacy.example.com/login/GitLab or https://codacy.example.com/login/GitLabEnterprise ) Check if the decoded value matches one of the redirect URIs of your GitLab application Unknown client \u00b6 While trying to authenticate on GitLab you get the following error message: This might mean that there is a mismatch in the Application ID that Codacy is using to authenticate on GitLab. To solve this issue: Make sure that the value of clientId in your values-production.yaml file is the same as the Application ID of the GitLab Cloud or GitLab Enterprise application that you created If the values were different, update your configuration and re-execute the helm upgrade command as described for GitLab Cloud or GitLab Enterprise If the error persists: Take note of the parameter client_id in the URL of the GitLab error page (for example, cca35a2a1f9b9b516ac927d82947bd5149b0e57e922c9e5564ac092ea16a3ccd ) Check if the value of the parameter matches the value of the Application ID of your GitLab application Bitbucket Cloud authentication \u00b6 Invalid client_id \u00b6 While trying to authenticate on Bitbucket Cloud you get the following error message: This might mean that there is a mismatch in the OAuth consumer Client ID that Codacy is using to authenticate on Bitbucket Cloud. To solve this issue: Make sure that the value of key in your values-production.yaml file is the same as the Key of the Bitbucket OAuth consumer that you created If the values were different, update your configuration and re-execute the helm upgrade command as described for Bitbucket Cloud If the error persists: Take note of the parameter client_id in the URL of the Bitbucket Cloud error page (for example, r8QJDkkxj8unYfg4Bd ) Check if the value of the parameter matches the value of the Client ID of your Bitbucket OAuth consumer Codacy configuration \u00b6 The following sections help you troubleshoot the Codacy configuration. Lost or changed database secrets \u00b6 When you open the Codacy UI, an error message states that the secret used to encrypt sensitive data on the database and the one in your configuration file are different. To solve this issue: Obtain the correct key from the Codacy logs by executing the following command, where <namespace> is the cluster namespace where Codacy is installed: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/troubleshoot/extract-codacy-secrets.sh ) \\ -n <namespace> You can also download the script extract-codacy-secrets.sh to run it manually. Copy the value of the key and update your values-production.yaml file with this value. Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml","title":"Troubleshooting Codacy"},{"location":"troubleshoot/troubleshoot/#troubleshooting-codacy","text":"This page includes information to help you troubleshoot issues that you may come across while installing, configuring, and operating Codacy. If the information provided on this page is not enough to solve your issue, contact support@codacy.com providing: The description of the issue All the information that you were able to obtain while following these troubleshooting instructions The collected logs of your Codacy instance","title":"Troubleshooting Codacy"},{"location":"troubleshoot/troubleshoot/#git-provider-integrations","text":"The following sections help you troubleshoot the integration of Codacy with your Git provider.","title":"Git provider integrations"},{"location":"troubleshoot/troubleshoot/#github","text":"","title":"GitHub Cloud and GitHub Enterprise authentication"},{"location":"troubleshoot/troubleshoot/#404-error","text":"While trying to authenticate on GitHub you get the following error message: This might mean that there is a mismatch in the Client ID that Codacy is using to authenticate on GitHub. To solve this issue: Make sure that the value of clientId in your values-production.yaml file is the same as the Client ID of the GitHub App that you created If the values were different, update your configuration and re-execute the helm upgrade command as described for GitHub Cloud or GitHub Enterprise If the error persists: Take note of the parameter client_id in the URL of the GitHub error page (for example, Iv1.0000000000000000 ) Check if the value of the parameter matches the value of the Client ID of your GitHub App","title":"404 error"},{"location":"troubleshoot/troubleshoot/#gitlab","text":"","title":"GitLab Cloud and GitLab Enterprise authentication"},{"location":"troubleshoot/troubleshoot/#invalid-redirect-uri","text":"While trying to authenticate on GitLab you get the following error message: This might mean that the redirect URIs are not correct in the GitLab application that Codacy is using to authenticate on GitLab. To solve this issue: Open the GitLab application that you created on GitLab Cloud or GitLab Enterprise Make sure that all the redirect URIs have the correct protocol for the Codacy instance endpoints, either http:// or https:// Make sure that all the redirect URIs have the full path with the correct case, since the field is case-sensitive If the error persists: Take note of the parameter redirect_uri in the URL of the GitLab error page (for example, https%3A%2F%2Fcodacy.example.com%2Flogin%2FGitLab or https%3A%2F%2Fcodacy.example.com%2Flogin%2FGitLabEnterprise ) Decode the value of the parameter using a tool such as urldecoder.com (for example, https://codacy.example.com/login/GitLab or https://codacy.example.com/login/GitLabEnterprise ) Check if the decoded value matches one of the redirect URIs of your GitLab application","title":"Invalid redirect URI"},{"location":"troubleshoot/troubleshoot/#unknown-client","text":"While trying to authenticate on GitLab you get the following error message: This might mean that there is a mismatch in the Application ID that Codacy is using to authenticate on GitLab. To solve this issue: Make sure that the value of clientId in your values-production.yaml file is the same as the Application ID of the GitLab Cloud or GitLab Enterprise application that you created If the values were different, update your configuration and re-execute the helm upgrade command as described for GitLab Cloud or GitLab Enterprise If the error persists: Take note of the parameter client_id in the URL of the GitLab error page (for example, cca35a2a1f9b9b516ac927d82947bd5149b0e57e922c9e5564ac092ea16a3ccd ) Check if the value of the parameter matches the value of the Application ID of your GitLab application","title":"Unknown client"},{"location":"troubleshoot/troubleshoot/#bitbucket-cloud","text":"","title":"Bitbucket Cloud authentication"},{"location":"troubleshoot/troubleshoot/#invalid-client_id","text":"While trying to authenticate on Bitbucket Cloud you get the following error message: This might mean that there is a mismatch in the OAuth consumer Client ID that Codacy is using to authenticate on Bitbucket Cloud. To solve this issue: Make sure that the value of key in your values-production.yaml file is the same as the Key of the Bitbucket OAuth consumer that you created If the values were different, update your configuration and re-execute the helm upgrade command as described for Bitbucket Cloud If the error persists: Take note of the parameter client_id in the URL of the Bitbucket Cloud error page (for example, r8QJDkkxj8unYfg4Bd ) Check if the value of the parameter matches the value of the Client ID of your Bitbucket OAuth consumer","title":"Invalid client_id"},{"location":"troubleshoot/troubleshoot/#codacy-configuration","text":"The following sections help you troubleshoot the Codacy configuration.","title":"Codacy configuration"},{"location":"troubleshoot/troubleshoot/#db-secrets","text":"When you open the Codacy UI, an error message states that the secret used to encrypt sensitive data on the database and the one in your configuration file are different. To solve this issue: Obtain the correct key from the Codacy logs by executing the following command, where <namespace> is the cluster namespace where Codacy is installed: bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/troubleshoot/extract-codacy-secrets.sh ) \\ -n <namespace> You can also download the script extract-codacy-secrets.sh to run it manually. Copy the value of the key and update your values-production.yaml file with this value. Apply the new configuration by performing a Helm upgrade. To do so execute the command used to install Codacy : Important If you are using MicroK8s you must use the file values-microk8s.yaml together with the file values-production.yaml . To do this, uncomment the last line before running the helm upgrade command below. helm upgrade ( ...options used to install Codacy... ) \\ --recreate-pods --values values-production.yaml \\ # --values values-microk8s.yaml","title":"Lost or changed database secrets"}]}