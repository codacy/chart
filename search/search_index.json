{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"How to install Codacy on Kubernetes \u00b6 Before you start, review the following requirements that must be met when provisioning the infrastructure that will host Codacy: System requirements Preparing to install Codacy \u00b6 To deploy Codacy on Kubernetes make sure that you have the specified versions of the following tools installed: Git version >= 2.0.0 AWS CLI version 1 (for Amazon EKS only) Terraform version >= 0.12 kubectl version compatible with your cluster ( within one minor version difference of your cluster ) By default, the infrastructure setup guides create Kubernetes version 1.14 clusters Helm client version 2.16.1 Please follow the documentation of the tools above to install them on your operating system. Typically, you can use your package manager to install the tools. For example, if you use MacOS with Homebrew , the following command installs all the tools: brew install git awscli terraform kubectl helm@2 Setting up the Kubernetes infrastructure \u00b6 You can follow one of the guides below to quickly set up a Kubernetes cluster on your cloud infrastructure using Terraform configuration files provided by Codacy: Setting up an Amazon EKS cluster Setting up an AKS cluster Installing Codacy \u00b6 Install Codacy in an existing Kubernetes cluster with the provided cloud-native Helm chart. Installing Codacy Post-install configuration \u00b6 After successfully installing Codacy, follow the post-install configuration steps: Enable the Ingress on codacy-api . Start configuring Codacy in the UI. The onboarding process will guide you through the following steps: Create an administrator account Configure one or more of the following supported git providers: Github Cloud Github Enterprise Gitlab Enterprise Bitbucket Enterprise Create an initial organization Invite users Finally, we also recommend that you set up logging and monitoring. Logging Monitoring","title":"How to install Codacy on Kubernetes"},{"location":"#how-to-install-codacy-on-kubernetes","text":"Before you start, review the following requirements that must be met when provisioning the infrastructure that will host Codacy: System requirements","title":"How to install Codacy on Kubernetes"},{"location":"#preparing-to-install-codacy","text":"To deploy Codacy on Kubernetes make sure that you have the specified versions of the following tools installed: Git version >= 2.0.0 AWS CLI version 1 (for Amazon EKS only) Terraform version >= 0.12 kubectl version compatible with your cluster ( within one minor version difference of your cluster ) By default, the infrastructure setup guides create Kubernetes version 1.14 clusters Helm client version 2.16.1 Please follow the documentation of the tools above to install them on your operating system. Typically, you can use your package manager to install the tools. For example, if you use MacOS with Homebrew , the following command installs all the tools: brew install git awscli terraform kubectl helm@2","title":"Preparing to install Codacy"},{"location":"#setting-up-the-kubernetes-infrastructure","text":"You can follow one of the guides below to quickly set up a Kubernetes cluster on your cloud infrastructure using Terraform configuration files provided by Codacy: Setting up an Amazon EKS cluster Setting up an AKS cluster","title":"Setting up the Kubernetes infrastructure"},{"location":"#installing-codacy","text":"Install Codacy in an existing Kubernetes cluster with the provided cloud-native Helm chart. Installing Codacy","title":"Installing Codacy"},{"location":"#post-install-configuration","text":"After successfully installing Codacy, follow the post-install configuration steps: Enable the Ingress on codacy-api . Start configuring Codacy in the UI. The onboarding process will guide you through the following steps: Create an administrator account Configure one or more of the following supported git providers: Github Cloud Github Enterprise Gitlab Enterprise Bitbucket Enterprise Create an initial organization Invite users Finally, we also recommend that you set up logging and monitoring. Logging Monitoring","title":"Post-install configuration"},{"location":"install/","text":"Installing Codacy \u00b6 Follow the steps below to install Codacy on an existing Kubernetes cluster using the provided cloud native Helm chart. Create a Kubernetes namespace called codacy that will group all cluster resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials that you received together with your Codacy license to a secret in the namespace created above. This is necessary because some Codacy Docker images are currently private. kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy Use a text editor of your choice to copy the template below to a new file named values.yaml , changing the values as described in the comments. global : imagePullSecrets : - name : docker-credentials codacy : url : \"http://codacy.example.com\" # This value is important for VCS configuration and badges to work backendUrl : \"http://codacy.example.com\" # This value is important for VCS configuration and badges to work play : cryptoSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` akka : sessionSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` filestore : contentsSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` uuidSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` cacheSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` Add Codacy's chart repository to your helm client and install the Codacy chart using the values in the values.yaml file created in the previous step. helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --recreate-pods \\ --values values.yaml By now all the Codacy pods should be starting in the Kubernetes cluster. Check this with the command below (your output will contain more detail): $ helm status codacy LAST DEPLOYED: Wed Jan 8 15 :52:27 2020 NAMESPACE: codacy STATUS: DEPLOYED RESOURCES: [ ... ] == > v1/Pod ( related ) NAME READY STATUS RESTARTS AGE codacy-activities-78849c8548-ln5sl 1 /1 Running 4 6m11s codacy-activitiesdb-0 1 /1 Running 0 6m3s codacy-api-6f44c8d48-6bw8z 1 /1 Running 0 6m11s codacy-api-6f44c8d48-h6cl4 1 /1 Running 0 6m11s codacy-api-6f44c8d48-vgbl5 1 /1 Running 0 6m11s codacy-core-786c6f79f-7sn7p 1 /1 Running 0 6m11s codacy-core-786c6f79f-pvg9w 1 /1 Running 0 6m11s [ ... ] Download the reference file values-production.yaml and configure the following values for each Codacy database: host (host name of the PostgreSQL server, accessible from the cluster) postgresqlUsername (dedicated Codacy user) postgresqlPassword (password for the dedicated Codacy user) Configure the ingress values on the values-production.yaml file that you have just updated: app: <--- codacy-app.dns.internal ---> (Codacy application DNS) api: <--- codacy-api.dns.internal ---> (Codacy API DNS) external-dns.alpha.kubernetes.io/hostname: '<--- codacy-app.dns.internal --->, <--- codacy-api.dns.internal --->' (Codacy application and API DNS) host: <--- codacy-app.dns.internal ---> (Codacy application DNS) host: <--- codacy-api.dns.internal ---> (Codacy API DNS) Run the command below to update the cluster to use the PostgreSQL server configuration: helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --recreate-pods \\ --values values.yaml \\ --values values-production.yaml","title":"Installing Codacy"},{"location":"install/#installing-codacy","text":"Follow the steps below to install Codacy on an existing Kubernetes cluster using the provided cloud native Helm chart. Create a Kubernetes namespace called codacy that will group all cluster resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials that you received together with your Codacy license to a secret in the namespace created above. This is necessary because some Codacy Docker images are currently private. kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy Use a text editor of your choice to copy the template below to a new file named values.yaml , changing the values as described in the comments. global : imagePullSecrets : - name : docker-credentials codacy : url : \"http://codacy.example.com\" # This value is important for VCS configuration and badges to work backendUrl : \"http://codacy.example.com\" # This value is important for VCS configuration and badges to work play : cryptoSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` akka : sessionSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` filestore : contentsSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` uuidSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` cacheSecret : \"CHANGE ME\" # Generate one with `openssl rand -base64 128 | tr -dc 'a-zA-Z0-9'` Add Codacy's chart repository to your helm client and install the Codacy chart using the values in the values.yaml file created in the previous step. helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --recreate-pods \\ --values values.yaml By now all the Codacy pods should be starting in the Kubernetes cluster. Check this with the command below (your output will contain more detail): $ helm status codacy LAST DEPLOYED: Wed Jan 8 15 :52:27 2020 NAMESPACE: codacy STATUS: DEPLOYED RESOURCES: [ ... ] == > v1/Pod ( related ) NAME READY STATUS RESTARTS AGE codacy-activities-78849c8548-ln5sl 1 /1 Running 4 6m11s codacy-activitiesdb-0 1 /1 Running 0 6m3s codacy-api-6f44c8d48-6bw8z 1 /1 Running 0 6m11s codacy-api-6f44c8d48-h6cl4 1 /1 Running 0 6m11s codacy-api-6f44c8d48-vgbl5 1 /1 Running 0 6m11s codacy-core-786c6f79f-7sn7p 1 /1 Running 0 6m11s codacy-core-786c6f79f-pvg9w 1 /1 Running 0 6m11s [ ... ] Download the reference file values-production.yaml and configure the following values for each Codacy database: host (host name of the PostgreSQL server, accessible from the cluster) postgresqlUsername (dedicated Codacy user) postgresqlPassword (password for the dedicated Codacy user) Configure the ingress values on the values-production.yaml file that you have just updated: app: <--- codacy-app.dns.internal ---> (Codacy application DNS) api: <--- codacy-api.dns.internal ---> (Codacy API DNS) external-dns.alpha.kubernetes.io/hostname: '<--- codacy-app.dns.internal --->, <--- codacy-api.dns.internal --->' (Codacy application and API DNS) host: <--- codacy-app.dns.internal ---> (Codacy application DNS) host: <--- codacy-api.dns.internal ---> (Codacy API DNS) Run the command below to update the cluster to use the PostgreSQL server configuration: helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --recreate-pods \\ --values values.yaml \\ --values values-production.yaml","title":"Installing Codacy"},{"location":"requirements/","text":"System requirements \u00b6 Running Codacy on a Kubernetes cluster requires the following: A Kubernetes 1.14.* or 1.15.* cluster provisioned with the required resources The NGINX Ingress Controller for Kubernetes A PostgreSQL server accessible from the Kubernetes cluster Cluster resource requirements \u00b6 To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 40 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production-ready\" version of Codacy. Analysis \u00b6 Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.genericMax + workers.dedicatedMax Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis. Example \u00b6 Maximum of 6 concurrent analysis worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 40 GB + 36 GB = 76 GB PostgreSQL server setup \u00b6 Codacy requires a working PostgreSQL server to run. The recommended specifications are: 4 CPU 8 GB RAM Minimum 500 GB+ hard drive, depending on the number of repositories you have. For a custom recommendation, please contact us at support@codacy.com. You must manually create the databases required by Codacy on the PostgreSQL server. We recommend that you also create a dedicated user for Codacy, with access permissions only to the databases specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command-line client: psql -U postgres -h <PostgreSQL server hostname> Execute the SQL script below to create the dedicated user and the databases required by Codacy. Make sure that you change the user name and password to suit your security needs. CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; Make sure that you can connect to the PostgreSQL database using the newly created user. For example: psql -U codacy -h <PostgreSQL server hostname>","title":"System requirements"},{"location":"requirements/#system-requirements","text":"Running Codacy on a Kubernetes cluster requires the following: A Kubernetes 1.14.* or 1.15.* cluster provisioned with the required resources The NGINX Ingress Controller for Kubernetes A PostgreSQL server accessible from the Kubernetes cluster","title":"System requirements"},{"location":"requirements/#cluster-resource-requirements","text":"To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 40 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production-ready\" version of Codacy.","title":"Cluster resource requirements"},{"location":"requirements/#analysis","text":"Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.genericMax + workers.dedicatedMax Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis.","title":"Analysis"},{"location":"requirements/#example","text":"Maximum of 6 concurrent analysis worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 40 GB + 36 GB = 76 GB","title":"Example"},{"location":"requirements/#postgresql-server-setup","text":"Codacy requires a working PostgreSQL server to run. The recommended specifications are: 4 CPU 8 GB RAM Minimum 500 GB+ hard drive, depending on the number of repositories you have. For a custom recommendation, please contact us at support@codacy.com. You must manually create the databases required by Codacy on the PostgreSQL server. We recommend that you also create a dedicated user for Codacy, with access permissions only to the databases specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command-line client: psql -U postgres -h <PostgreSQL server hostname> Execute the SQL script below to create the dedicated user and the databases required by Codacy. Make sure that you change the user name and password to suit your security needs. CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; Make sure that you can connect to the PostgreSQL database using the newly created user. For example: psql -U codacy -h <PostgreSQL server hostname>","title":"PostgreSQL server setup"},{"location":"configuration/logging/","text":"Logging \u00b6 This page contains two different sections with different targets: Aggregate logs for day to day opperation Collect logs for support Aggregation of logs \u00b6 There are a lot of different solutions for you to have logs in your cluster. Overall all of them follow the same model, you have an agent on each node, an aggregator to persist the logs and a UI to analyse the logs. One of the simplest stacks ou there is to use: FluentBit as the agent ElasticSearch + Kibana for Aggregation and UI Quick Installation \u00b6 helm repo add elastic https://helm.elastic.co helm install --name elasticsearch elastic/elasticsearch --namespace logging helm install --name fluent-bit stable/fluent-bit --namespace logging \\ --set backend.type = elasticsearch Collect logs for support \u00b6 To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Logging"},{"location":"configuration/logging/#logging","text":"This page contains two different sections with different targets: Aggregate logs for day to day opperation Collect logs for support","title":"Logging"},{"location":"configuration/logging/#aggregation-of-logs","text":"There are a lot of different solutions for you to have logs in your cluster. Overall all of them follow the same model, you have an agent on each node, an aggregator to persist the logs and a UI to analyse the logs. One of the simplest stacks ou there is to use: FluentBit as the agent ElasticSearch + Kibana for Aggregation and UI","title":"Aggregation of logs"},{"location":"configuration/logging/#quick-installation","text":"helm repo add elastic https://helm.elastic.co helm install --name elasticsearch elastic/elasticsearch --namespace logging helm install --name fluent-bit stable/fluent-bit --namespace logging \\ --set backend.type = elasticsearch","title":"Quick Installation"},{"location":"configuration/logging/#collect-logs-for-support","text":"To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Collect logs for support"},{"location":"configuration/monitoring/","text":"Monitoring \u00b6 In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana. Setting up monitoring using Grafana & Prometheus \u00b6 The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Setting up monitoring using Crow [deprecated] \u00b6 Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed alongside Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Monitoring"},{"location":"configuration/monitoring/#monitoring","text":"In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana.","title":"Monitoring"},{"location":"configuration/monitoring/#setting-up-monitoring-using-grafana-prometheus","text":"The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true","title":"Setting up monitoring using Grafana &amp; Prometheus"},{"location":"configuration/monitoring/#setting-up-monitoring-using-crow-deprecated","text":"Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed alongside Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Setting up monitoring using Crow [deprecated]"},{"location":"configuration/git-providers/bitbucket-enterprise/","text":"Bitbucket Server \u00b6 Set your configuration values for your Bitbucket instance on the values.yaml file. NOTE: Since Bitbucket Server uses OAuth1, you'll need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance: Create a key pair using the RSA algorithm in the PKCS#8 format by running the following commands below, making sure that you don't define a passphrase: openssl genrsa -out mykey openssl pkcs8 -nocrypt -in mykey -out mykeypkcs8 -topk8 openssl rsa -in mykey -pubout -out mykey.pub Copy the private key from the mykeypkcs8 file and set it in the consumerPrivateKey , ignoring the first and last lines. Copy the public key from the mykey.pub file and set it in the consumerPublicKey , ignoring the first and last lines. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of. Bitbucket Server Application Link \u00b6 To set up Bitbucket Server you need to create an application link on your Bitbucket Server installation. You can click on here and go to the application links list. Application Link Creation \u00b6 Create the link, use your Codacy installation URL for this Name the link \u00b6 Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection. Add incoming connection \u00b6 Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Bitbucket Server"},{"location":"configuration/git-providers/bitbucket-enterprise/#bitbucket-server","text":"Set your configuration values for your Bitbucket instance on the values.yaml file. NOTE: Since Bitbucket Server uses OAuth1, you'll need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance: Create a key pair using the RSA algorithm in the PKCS#8 format by running the following commands below, making sure that you don't define a passphrase: openssl genrsa -out mykey openssl pkcs8 -nocrypt -in mykey -out mykeypkcs8 -topk8 openssl rsa -in mykey -pubout -out mykey.pub Copy the private key from the mykeypkcs8 file and set it in the consumerPrivateKey , ignoring the first and last lines. Copy the public key from the mykey.pub file and set it in the consumerPublicKey , ignoring the first and last lines. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of.","title":"Bitbucket Server"},{"location":"configuration/git-providers/bitbucket-enterprise/#bitbucket-server-application-link","text":"To set up Bitbucket Server you need to create an application link on your Bitbucket Server installation. You can click on here and go to the application links list.","title":"Bitbucket Server Application Link"},{"location":"configuration/git-providers/bitbucket-enterprise/#application-link-creation","text":"Create the link, use your Codacy installation URL for this","title":"Application Link Creation"},{"location":"configuration/git-providers/bitbucket-enterprise/#name-the-link","text":"Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection.","title":"Name the link"},{"location":"configuration/git-providers/bitbucket-enterprise/#add-incoming-connection","text":"Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Add incoming connection"},{"location":"configuration/git-providers/github-cloud/","text":"GitHub Cloud \u00b6 To integrate with GitHub we will use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page of Codacy. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Cloud"},{"location":"configuration/git-providers/github-cloud/#github-cloud","text":"To integrate with GitHub we will use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Cloud"},{"location":"configuration/git-providers/github-cloud/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page of Codacy. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/git-providers/github-enterprise/","text":"GitHub Enterprise \u00b6 Set your configuration values for your GitHub instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitHub Application \u00b6 To integrate with GitHub we use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Enterprise"},{"location":"configuration/git-providers/github-enterprise/#github-enterprise","text":"Set your configuration values for your GitHub instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitHub Enterprise"},{"location":"configuration/git-providers/github-enterprise/#github-application","text":"To integrate with GitHub we use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Application"},{"location":"configuration/git-providers/github-enterprise/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/git-providers/gitlab-enterprise/","text":"GitLab Enterprise \u00b6 Set your configuration values for your GitLab instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitLab Application \u00b6 To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Enterprise"},{"location":"configuration/git-providers/gitlab-enterprise/#gitlab-enterprise","text":"Set your configuration values for your GitLab instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitLab Enterprise"},{"location":"configuration/git-providers/gitlab-enterprise/#gitlab-application","text":"To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Application"},{"location":"extra/database/","text":"Database Migration Guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Database Migration Guide"},{"location":"extra/database/#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results","title":"Database Migration Guide"},{"location":"extra/database/#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"extra/database/#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"extra/database/#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"extra/database/#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"extra/database/#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Sample script"},{"location":"extra/uninstall/","text":"Uninstall \u00b6 To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstall"},{"location":"extra/uninstall/#uninstall","text":"To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstall"},{"location":"extra/upgrade/","text":"Upgrade \u00b6 NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values Steps \u00b6 The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrade"},{"location":"extra/upgrade/#upgrade","text":"NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values","title":"Upgrade"},{"location":"extra/upgrade/#steps","text":"The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Steps"},{"location":"infrastructure/aks-quickstart/","text":"Setting up an AKS cluster \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"Setting up an AKS cluster"},{"location":"infrastructure/aks-quickstart/#setting-up-an-aks-cluster","text":"","title":"Setting up an AKS cluster"},{"location":"infrastructure/aks-quickstart/#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"infrastructure/aks-quickstart/#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"infrastructure/aks-quickstart/#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"infrastructure/aks-quickstart/#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"infrastructure/eks-quickstart/","text":"Setting up an Amazon EKS cluster \u00b6 Follow the steps below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform. 1. Prepare your environment \u00b6 Prepare your environment to set up the Amazon EKS cluster: Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Chart repository and change to the directory that includes the Terraform configuration files provided by Codacy: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ The folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and the DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster 2. Set up the Terraform state storage backend \u00b6 The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply An Amazon S3 bucket with a unique name to save the infrastructure state is created. Note the value of state_bucket_name in the output of the command. Edit the config.tf files that exist in the main/ and setup/ directories and follow the instructions to set the name of the Amazon S3 bucket and enable the use of the backend in those infrastructure stacks. 3. Create a vanilla Amazon EKS cluster \u00b6 Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type/number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file by setting the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section. 4. Set up the cluster to run Codacy \u00b6 Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply To connect to the Kubernetes Dashboard, run: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token Uninstalling the Amazon EKS cluster \u00b6 WARNING: If you proceed beyond this point you'll permanently delete and break things. Cleanup your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction, so to destroy it cleanly you must first disable these extra settings. Edit the file backend/state_and_lock.tf and follow the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Setting up an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#setting-up-an-amazon-eks-cluster","text":"Follow the steps below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform.","title":"Setting up an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#1-prepare-your-environment","text":"Prepare your environment to set up the Amazon EKS cluster: Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Chart repository and change to the directory that includes the Terraform configuration files provided by Codacy: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ The folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and the DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster","title":"1. Prepare your environment"},{"location":"infrastructure/eks-quickstart/#2-set-up-the-terraform-state-storage-backend","text":"The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply An Amazon S3 bucket with a unique name to save the infrastructure state is created. Note the value of state_bucket_name in the output of the command. Edit the config.tf files that exist in the main/ and setup/ directories and follow the instructions to set the name of the Amazon S3 bucket and enable the use of the backend in those infrastructure stacks.","title":"2. Set up the Terraform state storage backend"},{"location":"infrastructure/eks-quickstart/#3-create-a-vanilla-amazon-eks-cluster","text":"Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type/number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file by setting the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section.","title":"3. Create a vanilla Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#4-set-up-the-cluster-to-run-codacy","text":"Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply To connect to the Kubernetes Dashboard, run: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token","title":"4. Set up the cluster to run Codacy"},{"location":"infrastructure/eks-quickstart/#uninstalling-the-amazon-eks-cluster","text":"WARNING: If you proceed beyond this point you'll permanently delete and break things. Cleanup your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction, so to destroy it cleanly you must first disable these extra settings. Edit the file backend/state_and_lock.tf and follow the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Uninstalling the Amazon EKS cluster"}]}