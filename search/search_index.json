{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"How to install Codacy on Kubernetes \u00b6 Before you start, review the following requirements that must be met when provisioning the infrastructure that will host Codacy: System requirements 1. Setting up the Kubernetes infrastructure \u00b6 You can follow one of the guides below to quickly set up a Kubernetes cluster on your cloud infrastructure using Terraform configuration files provided by Codacy: Setting up an Amazon EKS cluster Setting up an AKS cluster Setting up a MicroK8s machine 2. Installing Codacy \u00b6 Install Codacy in an existing Kubernetes cluster with the provided cloud-native Helm chart. Installing Codacy 3. Post-install configuration \u00b6 After successfully installing Codacy, follow the post-install configuration steps: Enable the Ingress on codacy-api . Start configuring Codacy in the UI. The onboarding process will guide you through the following steps: Create an administrator account Configure one or more of the following supported git providers: Github Cloud Github Enterprise Gitlab Enterprise Bitbucket Enterprise Create an initial organization Invite users Finally, we also recommend that you set up logging and monitoring. Logging Monitoring","title":"How to install Codacy on Kubernetes"},{"location":"#how-to-install-codacy-on-kubernetes","text":"Before you start, review the following requirements that must be met when provisioning the infrastructure that will host Codacy: System requirements","title":"How to install Codacy on Kubernetes"},{"location":"#1-setting-up-the-kubernetes-infrastructure","text":"You can follow one of the guides below to quickly set up a Kubernetes cluster on your cloud infrastructure using Terraform configuration files provided by Codacy: Setting up an Amazon EKS cluster Setting up an AKS cluster Setting up a MicroK8s machine","title":"1. Setting up the Kubernetes infrastructure"},{"location":"#2-installing-codacy","text":"Install Codacy in an existing Kubernetes cluster with the provided cloud-native Helm chart. Installing Codacy","title":"2. Installing Codacy"},{"location":"#3-post-install-configuration","text":"After successfully installing Codacy, follow the post-install configuration steps: Enable the Ingress on codacy-api . Start configuring Codacy in the UI. The onboarding process will guide you through the following steps: Create an administrator account Configure one or more of the following supported git providers: Github Cloud Github Enterprise Gitlab Enterprise Bitbucket Enterprise Create an initial organization Invite users Finally, we also recommend that you set up logging and monitoring. Logging Monitoring","title":"3. Post-install configuration"},{"location":"install/","text":"Installing Codacy \u00b6 Follow the steps below to install Codacy on an existing Kubernetes cluster using the provided cloud-native Helm chart. Before you continue, make sure you have the following tools installed in your machine. kubectl version compatible with your cluster ( within one minor version difference of your cluster ) NOTE: If you are in microk8s, any kubectl command must be executed as microk8s.kubectl instead. Helm client version 2.16.3 Install Codacy as follows: Create a Kubernetes namespace called codacy that will group all cluster resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials that you received together with your Codacy license to a secret in the namespace created above. This is necessary because some Codacy Docker images are currently private. kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy Use a text editor of your choice edit the values-production.yaml file, changing the values with placeholders as described in the comments. Create a record in your DNS provider with the hostname you used in the previous step to send traffic to your ingress controller. NOTE: If you are in microk8s, you need to configure it with the public IP address of your machine. Add Codacy's chart repository to your helm client and install the Codacy chart using the values file created in the previous step. NOTE: If you are in microk8s, don't forget to use the values-microk8s.yaml configuration file as stated here . helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --values values-production.yaml # --values values-microk8s.yaml By now all the Codacy pods should be starting in the Kubernetes cluster. Check this with the command below and wait for all the pods to be Running : $ kubectl get pods -n codacy NAME READY STATUS RESTARTS AGE codacy-activities-6d9db9499-stk2k 1 /1 Running 2 8m57s codacy-activitiesdb-0 1 /1 Running 0 8m57s codacy-api-f7897b965-fgn67 1 /1 Running 0 8m57s codacy-api-f7897b965-kkqsx 1 /1 Running 0 8m57s codacy-core-7bcf697968-85tl6 1 /1 Running 0 8m57s codacy-crow-7c957d45f6-b8zp2 1 /1 Running 2 8m57s codacy-crowdb-0 1 /1 Running 0 8m57s codacy-engine-549bcb69d9-cgrqf 1 /1 Running 1 8m57s codacy-engine-549bcb69d9-sh5f4 1 /1 Running 1 8m57s codacy-fluentdoperator-x5vr2 2 /2 Running 0 8m57s codacy-hotspots-api-b7b9db896-68gxx 1 /1 Running 2 8m57s codacy-hotspots-worker-76bb45b4d6-8gz45 1 /1 Running 3 8m57s codacy-hotspotsdb-0 1 /1 Running 0 8m57s codacy-listener-868b784dcf-npdfh 1 /1 Running 0 8m57s codacy-listenerdb-0 1 /1 Running 0 8m57s codacy-minio-7cfdc7b4f4-254gz 1 /1 Running 0 8m57s codacy-nfsserverprovisioner-0 1 /1 Running 0 8m57s codacy-portal-774d9fc596-rwqj5 1 /1 Running 2 8m56s codacy-rabbitmq-ha-0 1 /1 Running 0 8m57s codacy-ragnaros-69459775b5-hmj4d 1 /1 Running 3 8m57s codacy-remote-provider-service-8fb8556b-rr4ws 1 /1 Running 0 8m56s codacy-worker-manager-656dbf8d6d-n4j7c 1 /1 Running 0 8m57s","title":"Installing Codacy"},{"location":"install/#installing-codacy","text":"Follow the steps below to install Codacy on an existing Kubernetes cluster using the provided cloud-native Helm chart. Before you continue, make sure you have the following tools installed in your machine. kubectl version compatible with your cluster ( within one minor version difference of your cluster ) NOTE: If you are in microk8s, any kubectl command must be executed as microk8s.kubectl instead. Helm client version 2.16.3 Install Codacy as follows: Create a Kubernetes namespace called codacy that will group all cluster resources related to Codacy. kubectl create namespace codacy Add the Docker registry credentials that you received together with your Codacy license to a secret in the namespace created above. This is necessary because some Codacy Docker images are currently private. kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy Use a text editor of your choice edit the values-production.yaml file, changing the values with placeholders as described in the comments. Create a record in your DNS provider with the hostname you used in the previous step to send traffic to your ingress controller. NOTE: If you are in microk8s, you need to configure it with the public IP address of your machine. Add Codacy's chart repository to your helm client and install the Codacy chart using the values file created in the previous step. NOTE: If you are in microk8s, don't forget to use the values-microk8s.yaml configuration file as stated here . helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --values values-production.yaml # --values values-microk8s.yaml By now all the Codacy pods should be starting in the Kubernetes cluster. Check this with the command below and wait for all the pods to be Running : $ kubectl get pods -n codacy NAME READY STATUS RESTARTS AGE codacy-activities-6d9db9499-stk2k 1 /1 Running 2 8m57s codacy-activitiesdb-0 1 /1 Running 0 8m57s codacy-api-f7897b965-fgn67 1 /1 Running 0 8m57s codacy-api-f7897b965-kkqsx 1 /1 Running 0 8m57s codacy-core-7bcf697968-85tl6 1 /1 Running 0 8m57s codacy-crow-7c957d45f6-b8zp2 1 /1 Running 2 8m57s codacy-crowdb-0 1 /1 Running 0 8m57s codacy-engine-549bcb69d9-cgrqf 1 /1 Running 1 8m57s codacy-engine-549bcb69d9-sh5f4 1 /1 Running 1 8m57s codacy-fluentdoperator-x5vr2 2 /2 Running 0 8m57s codacy-hotspots-api-b7b9db896-68gxx 1 /1 Running 2 8m57s codacy-hotspots-worker-76bb45b4d6-8gz45 1 /1 Running 3 8m57s codacy-hotspotsdb-0 1 /1 Running 0 8m57s codacy-listener-868b784dcf-npdfh 1 /1 Running 0 8m57s codacy-listenerdb-0 1 /1 Running 0 8m57s codacy-minio-7cfdc7b4f4-254gz 1 /1 Running 0 8m57s codacy-nfsserverprovisioner-0 1 /1 Running 0 8m57s codacy-portal-774d9fc596-rwqj5 1 /1 Running 2 8m56s codacy-rabbitmq-ha-0 1 /1 Running 0 8m57s codacy-ragnaros-69459775b5-hmj4d 1 /1 Running 3 8m57s codacy-remote-provider-service-8fb8556b-rr4ws 1 /1 Running 0 8m56s codacy-worker-manager-656dbf8d6d-n4j7c 1 /1 Running 0 8m57s","title":"Installing Codacy"},{"location":"requirements/","text":"System requirements \u00b6 Running Codacy on a Kubernetes cluster requires the following: A Kubernetes 1.14.* or 1.15.* cluster provisioned with the required resources The NGINX Ingress Controller for Kubernetes A PostgreSQL server accessible from the Kubernetes cluster Cluster resource requirements \u00b6 To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 40 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production-ready\" version of Codacy. Analysis \u00b6 Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.dedicatedMax Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis. Example \u00b6 Maximum of 6 concurrent analysis worker-manager : config : workers : dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 40 GB + 36 GB = 76 GB PostgreSQL server setup \u00b6 Codacy requires a working PostgreSQL server to run. The recommended specifications are: 4 CPU 8 GB RAM Minimum 500 GB+ hard drive, depending on the number of repositories you have. For a custom recommendation, please contact us at support@codacy.com. You must manually create the databases required by Codacy on the PostgreSQL server. We recommend that you also create a dedicated user for Codacy, with access permissions only to the databases specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command-line client: psql -U postgres -h <PostgreSQL server hostname> Execute the SQL script below to create the dedicated user and the databases required by Codacy. Make sure that you change the user name and password to suit your security needs. CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; CREATE DATABASE crow WITH OWNER = codacy ; Make sure that you can connect to the PostgreSQL database using the newly created user. For example: psql -U codacy -h <PostgreSQL server hostname>","title":"System requirements"},{"location":"requirements/#system-requirements","text":"Running Codacy on a Kubernetes cluster requires the following: A Kubernetes 1.14.* or 1.15.* cluster provisioned with the required resources The NGINX Ingress Controller for Kubernetes A PostgreSQL server accessible from the Kubernetes cluster","title":"System requirements"},{"location":"requirements/#cluster-resource-requirements","text":"To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 40 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production-ready\" version of Codacy.","title":"Cluster resource requirements"},{"location":"requirements/#analysis","text":"Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.dedicatedMax Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis.","title":"Analysis"},{"location":"requirements/#example","text":"Maximum of 6 concurrent analysis worker-manager : config : workers : dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 40 GB + 36 GB = 76 GB","title":"Example"},{"location":"requirements/#postgresql-server-setup","text":"Codacy requires a working PostgreSQL server to run. The recommended specifications are: 4 CPU 8 GB RAM Minimum 500 GB+ hard drive, depending on the number of repositories you have. For a custom recommendation, please contact us at support@codacy.com. You must manually create the databases required by Codacy on the PostgreSQL server. We recommend that you also create a dedicated user for Codacy, with access permissions only to the databases specific to Codacy: Connect to the PostgreSQL server as a database admin user. For example, using the psql command-line client: psql -U postgres -h <PostgreSQL server hostname> Execute the SQL script below to create the dedicated user and the databases required by Codacy. Make sure that you change the user name and password to suit your security needs. CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotspots WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; CREATE DATABASE crow WITH OWNER = codacy ; Make sure that you can connect to the PostgreSQL database using the newly created user. For example: psql -U codacy -h <PostgreSQL server hostname>","title":"PostgreSQL server setup"},{"location":"configuration/logging/","text":"Logging \u00b6 This page contains two different sections with different targets: Aggregate logs for day to day opperation Collect logs for support Aggregation of logs \u00b6 There are a lot of different solutions for you to have logs in your cluster. Overall all of them follow the same model, you have an agent on each node, an aggregator to persist the logs and a UI to analyse the logs. One of the simplest stacks ou there is to use: FluentBit as the agent ElasticSearch + Kibana for Aggregation and UI Quick Installation \u00b6 helm repo add elastic https://helm.elastic.co helm install --name elasticsearch elastic/elasticsearch --namespace logging helm install --name fluent-bit stable/fluent-bit --namespace logging \\ --set backend.type = elasticsearch Collect logs for support \u00b6 To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. To extract the logs and send them to Codacy's support team in case of problems, you can run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/configuration/extract-codacy-logs.sh ) -n <namespace> The logs extraction script is also available here , for manual downloading.","title":"Logging"},{"location":"configuration/logging/#logging","text":"This page contains two different sections with different targets: Aggregate logs for day to day opperation Collect logs for support","title":"Logging"},{"location":"configuration/logging/#aggregation-of-logs","text":"There are a lot of different solutions for you to have logs in your cluster. Overall all of them follow the same model, you have an agent on each node, an aggregator to persist the logs and a UI to analyse the logs. One of the simplest stacks ou there is to use: FluentBit as the agent ElasticSearch + Kibana for Aggregation and UI","title":"Aggregation of logs"},{"location":"configuration/logging/#quick-installation","text":"helm repo add elastic https://helm.elastic.co helm install --name elasticsearch elastic/elasticsearch --namespace logging helm install --name fluent-bit stable/fluent-bit --namespace logging \\ --set backend.type = elasticsearch","title":"Quick Installation"},{"location":"configuration/logging/#collect-logs-for-support","text":"To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. To extract the logs and send them to Codacy's support team in case of problems, you can run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): bash < ( curl -fsSL https://raw.githubusercontent.com/codacy/chart/master/docs/configuration/extract-codacy-logs.sh ) -n <namespace> The logs extraction script is also available here , for manual downloading.","title":"Collect logs for support"},{"location":"configuration/monitoring/","text":"Monitoring \u00b6 In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana. Setting up monitoring using Grafana & Prometheus \u00b6 The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml sleep 5 # wait for crds to be created helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = NodePort \\ --set grafana.adminPassword = \"CHANGE_HERE\" kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Setting up monitoring using Crow \u00b6 Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed alongside Codacy after the helm chart is deployed to the cluster. The crow tool can be accessed through the /monitoring path of the url pointing to your Codacy installation, e.g. http://codacy.company.org/monitoring . You must set the global.codacy.crow.url value in your values.yaml file so that anchor links to your projects can be properly established inside crow . For example: global : codacy : crow : url : \"http://codacy.example.com/monitoring\" Please see the README.md for more information about these values. Configuring your credentials \u00b6 You should provide a password for the crow installation either through the values.yaml file or through a --set parameter during the helm installation process. This parameter can be configured as follows: Through a --set parameter: helm upgrade (...) --set crow.config.passwordAuth.password=<--- crow password ---> Through the values.yaml file: crow : config : passwordAuth : password : <--- crow password ---> Failing to do so your crow will fall back to using the default credentials as described below . Default credentials \u00b6 If you have not configured your crow credentials as described above , you can login with the following default credentials: username : codacy password : C0dacy123","title":"Monitoring"},{"location":"configuration/monitoring/#monitoring","text":"In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana.","title":"Monitoring"},{"location":"configuration/monitoring/#setting-up-monitoring-using-grafana-prometheus","text":"The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/release-0.36/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml sleep 5 # wait for crds to be created helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = NodePort \\ --set grafana.adminPassword = \"CHANGE_HERE\" kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true","title":"Setting up monitoring using Grafana &amp; Prometheus"},{"location":"configuration/monitoring/#setting-up-monitoring-using-crow","text":"Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed alongside Codacy after the helm chart is deployed to the cluster. The crow tool can be accessed through the /monitoring path of the url pointing to your Codacy installation, e.g. http://codacy.company.org/monitoring . You must set the global.codacy.crow.url value in your values.yaml file so that anchor links to your projects can be properly established inside crow . For example: global : codacy : crow : url : \"http://codacy.example.com/monitoring\" Please see the README.md for more information about these values.","title":"Setting up monitoring using Crow"},{"location":"configuration/monitoring/#configuring-your-credentials","text":"You should provide a password for the crow installation either through the values.yaml file or through a --set parameter during the helm installation process. This parameter can be configured as follows: Through a --set parameter: helm upgrade (...) --set crow.config.passwordAuth.password=<--- crow password ---> Through the values.yaml file: crow : config : passwordAuth : password : <--- crow password ---> Failing to do so your crow will fall back to using the default credentials as described below .","title":"Configuring your credentials"},{"location":"configuration/monitoring/#default-credentials","text":"If you have not configured your crow credentials as described above , you can login with the following default credentials: username : codacy password : C0dacy123","title":"Default credentials"},{"location":"configuration/git-providers/bitbucket-enterprise/","text":"Bitbucket Server \u00b6 Set your configuration values for your Bitbucket instance on the values.yaml file. NOTE: Since Bitbucket Server uses OAuth1, you'll need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance: Create a key pair using the RSA algorithm in the PKCS#8 format by running the following commands below, making sure that you don't define a passphrase: openssl genrsa -out mykey openssl pkcs8 -nocrypt -in mykey -out mykeypkcs8 -topk8 openssl rsa -in mykey -pubout -out mykey.pub Copy the private key from the mykeypkcs8 file and set it in the consumerPrivateKey , ignoring the first and last lines. Copy the public key from the mykey.pub file and set it in the consumerPublicKey , ignoring the first and last lines. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of. Bitbucket Server Application Link \u00b6 To set up Bitbucket Server you need to create an application link on your Bitbucket Server installation. You can click on here and go to the application links list. Application Link Creation \u00b6 Create the link, use your Codacy installation URL for this Name the link \u00b6 Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection. Add incoming connection \u00b6 Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Bitbucket Server"},{"location":"configuration/git-providers/bitbucket-enterprise/#bitbucket-server","text":"Set your configuration values for your Bitbucket instance on the values.yaml file. NOTE: Since Bitbucket Server uses OAuth1, you'll need to create a key pair to sign and validate the requests between Codacy and the Bitbucket Server instance: Create a key pair using the RSA algorithm in the PKCS#8 format by running the following commands below, making sure that you don't define a passphrase: openssl genrsa -out mykey openssl pkcs8 -nocrypt -in mykey -out mykeypkcs8 -topk8 openssl rsa -in mykey -pubout -out mykey.pub Copy the private key from the mykeypkcs8 file and set it in the consumerPrivateKey , ignoring the first and last lines. Copy the public key from the mykey.pub file and set it in the consumerPublicKey , ignoring the first and last lines. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of.","title":"Bitbucket Server"},{"location":"configuration/git-providers/bitbucket-enterprise/#bitbucket-server-application-link","text":"To set up Bitbucket Server you need to create an application link on your Bitbucket Server installation. You can click on here and go to the application links list.","title":"Bitbucket Server Application Link"},{"location":"configuration/git-providers/bitbucket-enterprise/#application-link-creation","text":"Create the link, use your Codacy installation URL for this","title":"Application Link Creation"},{"location":"configuration/git-providers/bitbucket-enterprise/#name-the-link","text":"Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection.","title":"Name the link"},{"location":"configuration/git-providers/bitbucket-enterprise/#add-incoming-connection","text":"Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Add incoming connection"},{"location":"configuration/git-providers/github-cloud/","text":"GitHub Cloud \u00b6 To integrate with GitHub we will use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page of Codacy. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Cloud"},{"location":"configuration/git-providers/github-cloud/#github-cloud","text":"To integrate with GitHub we will use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Cloud"},{"location":"configuration/git-providers/github-cloud/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page of Codacy. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/git-providers/github-enterprise/","text":"GitHub Enterprise \u00b6 Set your configuration values for your GitHub instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitHub Application \u00b6 To integrate with GitHub we use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Enterprise"},{"location":"configuration/git-providers/github-enterprise/#github-enterprise","text":"Set your configuration values for your GitHub instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitHub Enterprise"},{"location":"configuration/git-providers/github-enterprise/#github-application","text":"To integrate with GitHub we use a GitHub OAuth Application. To create the application on GitHub, visit Settings / Developer settings / OAuth Apps / New OAuth App and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/IP, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Application"},{"location":"configuration/git-providers/github-enterprise/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/git-providers/gitlab-enterprise/","text":"GitLab Enterprise \u00b6 Set your configuration values for your GitLab instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitLab Application \u00b6 To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Enterprise"},{"location":"configuration/git-providers/gitlab-enterprise/#gitlab-enterprise","text":"Set your configuration values for your GitLab instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitLab Enterprise"},{"location":"configuration/git-providers/gitlab-enterprise/#gitlab-application","text":"To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Application"},{"location":"extra/database/","text":"Database migration guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Database migration guide"},{"location":"extra/database/#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results","title":"Database migration guide"},{"location":"extra/database/#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"extra/database/#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"extra/database/#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"extra/database/#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"extra/database/#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Sample script"},{"location":"extra/k8s-cheatsheet/","text":"Kubernetes cheatsheet \u00b6 How to install a custom Codacy version \u00b6 Install \u00b6 sudo git clone git://github.com/codacy/chart -b <YOUR-BRANCH> helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE> Update \u00b6 ( cd chart ; sudo git fetch --all --prune --tags ; sudo git reset --hard origin/<YOUR-BRANCH> ; ) helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE> Clean the namespace \u00b6 helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Check uninstall was successful \u00b6 ps aux | grep -i kubectl Edit configmap \u00b6 kubectl get configmaps and kubectl edit configmap <configmap-name> Restart deployment of daemonset \u00b6 daemonsets \u00b6 kubectl get daemonsets and kubectl rollout restart daemonset/<daemonset-name> deployment \u00b6 kubectl get deployment and kubectl rollout restart deployment/<deployment-name> and kubectl rollout status deployment/<deployment-name> -w Read logs \u00b6 daemonset with multiple containers \u00b6 kubectl logs daemonset/<daemonset-name> <container-name> -f service \u00b6 kubectl get svc and kubectl logs -l $( kubectl get svc/<service-name> -o = json | jq \".spec.selector\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' | sed -e 'H;${x;s/\\n/,/g;s/^,//;p;};d' ) -f Open shell inside container \u00b6 kubectl exec -it daemonset/<daemonset-name> -c <container-name> sh or kubectl exec -it deployment/<deployment-name> sh MicroK8s \u00b6 Session Manager SSH \u00b6 When using AWS Session Manager, to connect to the instance where you installed microk8s, since the CLI is very limited you will benefit from using these aliases: alias kubectl = 'sudo microk8s.kubectl -n <namespace-name>' alias helm = 'sudo helm'","title":"Kubernetes cheatsheet"},{"location":"extra/k8s-cheatsheet/#kubernetes-cheatsheet","text":"","title":"Kubernetes cheatsheet"},{"location":"extra/k8s-cheatsheet/#how-to-install-a-custom-codacy-version","text":"","title":"How to install a custom Codacy version"},{"location":"extra/k8s-cheatsheet/#install","text":"sudo git clone git://github.com/codacy/chart -b <YOUR-BRANCH> helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE>","title":"Install"},{"location":"extra/k8s-cheatsheet/#update","text":"( cd chart ; sudo git fetch --all --prune --tags ; sudo git reset --hard origin/<YOUR-BRANCH> ; ) helm dep build ./chart/codacy helm upgrade --install codacy ./chart/codacy/ --namespace codacy --atomic --timeout = 300 --values ./<YOUR-VALUES-FILE>","title":"Update"},{"location":"extra/k8s-cheatsheet/#clean-the-namespace","text":"helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job &","title":"Clean the namespace"},{"location":"extra/k8s-cheatsheet/#check-uninstall-was-successful","text":"ps aux | grep -i kubectl","title":"Check uninstall was successful"},{"location":"extra/k8s-cheatsheet/#edit-configmap","text":"kubectl get configmaps and kubectl edit configmap <configmap-name>","title":"Edit configmap"},{"location":"extra/k8s-cheatsheet/#restart-deployment-of-daemonset","text":"","title":"Restart deployment of daemonset"},{"location":"extra/k8s-cheatsheet/#daemonsets","text":"kubectl get daemonsets and kubectl rollout restart daemonset/<daemonset-name>","title":"daemonsets"},{"location":"extra/k8s-cheatsheet/#deployment","text":"kubectl get deployment and kubectl rollout restart deployment/<deployment-name> and kubectl rollout status deployment/<deployment-name> -w","title":"deployment"},{"location":"extra/k8s-cheatsheet/#read-logs","text":"","title":"Read logs"},{"location":"extra/k8s-cheatsheet/#daemonset-with-multiple-containers","text":"kubectl logs daemonset/<daemonset-name> <container-name> -f","title":"daemonset with multiple containers"},{"location":"extra/k8s-cheatsheet/#service","text":"kubectl get svc and kubectl logs -l $( kubectl get svc/<service-name> -o = json | jq \".spec.selector\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' | sed -e 'H;${x;s/\\n/,/g;s/^,//;p;};d' ) -f","title":"service"},{"location":"extra/k8s-cheatsheet/#open-shell-inside-container","text":"kubectl exec -it daemonset/<daemonset-name> -c <container-name> sh or kubectl exec -it deployment/<deployment-name> sh","title":"Open shell inside container"},{"location":"extra/k8s-cheatsheet/#microk8s","text":"","title":"MicroK8s"},{"location":"extra/k8s-cheatsheet/#session-manager-ssh","text":"When using AWS Session Manager, to connect to the instance where you installed microk8s, since the CLI is very limited you will benefit from using these aliases: alias kubectl = 'sudo microk8s.kubectl -n <namespace-name>' alias helm = 'sudo helm'","title":"Session Manager SSH"},{"location":"extra/uninstall/","text":"Uninstalling Codacy \u00b6 To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of the effects of these commands please run each of the bash subcommands and validate their output. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstalling Codacy"},{"location":"extra/uninstall/#uninstalling-codacy","text":"To ensure a clean removal you should uninstall Codacy before destroying the cluster. To do so run: helm del --purge codacy kubectl -n codacy delete --all pod & kubectl -n codacy delete --all pvc & kubectl -n codacy delete --all pv & kubectl -n codacy delete --all job & sleep 5 kubectl -n codacy patch pvc -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pvc -o jsonpath = '{.items[*].metadata.name}' ) kubectl -n codacy patch pv -p '{\"metadata\":{\"finalizers\":null}}' $( kubectl -n codacy get pv -o jsonpath = '{.items[*].metadata.name}' ) sleep 5 kubectl -n codacy delete pod $( kubectl -n codacy get pod -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 kubectl -n codacy get pod & kubectl -n codacy get pvc & kubectl -n codacy get pv & kubectl -n codacy get job & Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of the effects of these commands please run each of the bash subcommands and validate their output. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstalling Codacy"},{"location":"extra/upgrade/","text":"Upgrading Codacy \u00b6 NOTE: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f , like helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with: helm get values codacy > codacy.yaml Decide on all the values you need to set. Perform the upgrade, with all --set arguments extracted in step 2: helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrading Codacy"},{"location":"extra/upgrade/#upgrading-codacy","text":"NOTE: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f , like helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with: helm get values codacy > codacy.yaml Decide on all the values you need to set. Perform the upgrade, with all --set arguments extracted in step 2: helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrading Codacy"},{"location":"infrastructure/aks-quickstart/","text":"Setting up an AKS cluster \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"Setting up an AKS cluster"},{"location":"infrastructure/aks-quickstart/#setting-up-an-aks-cluster","text":"","title":"Setting up an AKS cluster"},{"location":"infrastructure/aks-quickstart/#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"infrastructure/aks-quickstart/#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"infrastructure/aks-quickstart/#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"infrastructure/aks-quickstart/#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"infrastructure/eks-quickstart/","text":"Setting up an Amazon EKS cluster \u00b6 Follow the steps below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform. Before you continue, make sure you have the following tools installed in your machine. Git version >= 2.0.0 AWS CLI version 1 Terraform version >= 0.12 1. Prepare your environment \u00b6 Prepare your environment to set up the Amazon EKS cluster: Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Chart repository and change to the directory that includes the Terraform configuration files provided by Codacy: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ The folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and the DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster 2. Set up the Terraform state storage backend \u00b6 The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply An Amazon S3 bucket with a unique name to save the infrastructure state is created. Note the value of state_bucket_name in the output of the command. Edit the config.tf files that exist in the main/ and setup/ directories and follow the instructions to set the name of the Amazon S3 bucket and enable the use of the backend in those infrastructure stacks. 3. Create a vanilla Amazon EKS cluster \u00b6 Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type/number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file by setting the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section. 4. Set up the cluster to run Codacy \u00b6 Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply To connect to the Kubernetes Dashboard, run: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token Uninstalling the Amazon EKS cluster \u00b6 WARNING: If you proceed beyond this point you'll permanently delete and break things. Cleanup your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction, so to destroy it cleanly you must first disable these extra settings. Edit the file backend/state_and_lock.tf and follow the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Setting up an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#setting-up-an-amazon-eks-cluster","text":"Follow the steps below to set up an Amazon EKS cluster from scratch, including all the necessary underlying infrastructure, using Terraform. Before you continue, make sure you have the following tools installed in your machine. Git version >= 2.0.0 AWS CLI version 1 Terraform version >= 0.12","title":"Setting up an Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#1-prepare-your-environment","text":"Prepare your environment to set up the Amazon EKS cluster: Set up the AWS CLI credentials for your AWS account using the AWS CLI and Terraform documentation as reference. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly: export AWS_SDK_LOAD_CONFIG = 1 Clone the Chart repository and change to the directory that includes the Terraform configuration files provided by Codacy: git clone https://github.com/codacy/chart.git cd chart/docs/infrastructure/EKS/ The folder includes the following infrastructure stacks: backend : Optional S3 bucket for storing the Terraform state and the DynamoDB table for state locking main : Amazon EKS cluster, including the setup of all network and node infrastructure to go from zero to a fully functional cluster setup : Additional setup to be performed before installing Codacy on your vanilla Amazon EKS cluster","title":"1. Prepare your environment"},{"location":"infrastructure/eks-quickstart/#2-set-up-the-terraform-state-storage-backend","text":"The backend stores the current and historical state of your infrastructure. Although using the backend is optional, we recommend that you deploy it, particularly if you are planning to use these templates to make modifications to the cluster in the future: Initialize Terraform and deploy the infrastructure described in the backend/ directory, then follow Terraform's instructions: cd backend/ terraform init && terraform apply An Amazon S3 bucket with a unique name to save the infrastructure state is created. Note the value of state_bucket_name in the output of the command. Edit the config.tf files that exist in the main/ and setup/ directories and follow the instructions to set the name of the Amazon S3 bucket and enable the use of the backend in those infrastructure stacks.","title":"2. Set up the Terraform state storage backend"},{"location":"infrastructure/eks-quickstart/#3-create-a-vanilla-amazon-eks-cluster","text":"Create a cluster that includes all the required network and node setup: Initialize Terraform and deploy the infrastructure described in the main/ directory, then follow Terraform's instructions: cd ../main/ terraform init && terraform apply This process takes around 10 minutes. Consider if you want to tailor the cluster to your needs by customizing the cluster configuration. The cluster configuration (such as the type/number of nodes, network CIDRs, etc.) is exposed as variables in the main/variables.tf file. To customize the defaults of that file we recommend that you use a variable definitions file by setting the variables in a file named terraform.tfvars in the directory main/ . The following is an example terraform.tfvars : some_key = \"a_string_value\" another_key = 3 someting_else = true Subsequently running terraform apply loads the variables in the terraform.tfvars file by default: terraform apply Set up the kubeconfig file that stores the information needed by kubectl to connect to the new cluster by default: aws eks update-kubeconfig --name codacy-cluster Get information about the pods in the cluster to test that the cluster was created and that kubectl can successfully connect to the cluster: kubectl get pods -A You'll notice that nothing is scheduled. That's because we haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more details). We'll do that on the next section.","title":"3. Create a vanilla Amazon EKS cluster"},{"location":"infrastructure/eks-quickstart/#4-set-up-the-cluster-to-run-codacy","text":"Some additional setup is necessary to run Codacy on the newly created cluster, such as allowing workers to join the cluster and installing the following Helm charts: Kubernetes Dashboard nginx-ingress cert-manager Set up the cluster to run Codacy: Initialize Terraform and deploy the infrastructure described in the setup/ directory, then follow Terraform's instructions: cd ../setup/ terraform init && terraform apply To connect to the Kubernetes Dashboard, run: kubectl proxy Open the following URL on a browser and select token : http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy Run the following command to obtain the admin token required to connect to the Kubernetes Dashboard: terraform output admin_token","title":"4. Set up the cluster to run Codacy"},{"location":"infrastructure/eks-quickstart/#uninstalling-the-amazon-eks-cluster","text":"WARNING: If you proceed beyond this point you'll permanently delete and break things. Cleanup your cluster back to a vanilla state by removing the setup required to install Codacy. Run the following command in the setup/ folder: terraform destroy After removing the stacks and setup above, you may now delete the Kubernetes cluster. Run the following command in the main/ directory: terraform destroy This process takes around 10 minutes. Remove the Terraform backend. If you created the Terraform backend with the provided stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction, so to destroy it cleanly you must first disable these extra settings. Edit the file backend/state_and_lock.tf and follow the instructions included in the comments. Afterwards, run the following command in the backend/ directory: terraform apply && terraform destroy Note that you first have to run terraform apply to update the settings, and only then will terraform destroy be able to destroy the backend.","title":"Uninstalling the Amazon EKS cluster"},{"location":"infrastructure/microk8s-quickstart/","text":"Setting up a microk8s instance \u00b6 Microk8s is a single-package fully conformant lightweight Kubernetes that works on 42 Linux versions. The project is publicly available and can be found here . Follow the steps below to set up a microk8s instance from scratch, including all the necessary dependencies and configuration. 1. Concepts \u00b6 Helm and tiller \u00b6 Two executables will get installed onto the cluster as part of this process: helm and tiller . helm is responsible for resolving the configuration of the chart to be installed, while issuing the correct install commands onto the cluster. tiller is responsible for receiving the install commands issued by helm , as well as managing the lifecycle of the components that have been installed. helm is the client facing side, while tiller is the server/cluster facing side. 2. Prepare your environment \u00b6 Prepare your environment to set up the microk8s cluster. For your infrastructure, you will need the following: A machine running Ubuntu 18.04 LTS. You must start a local or remote command line session on this machine. A PostgreSQL instance with all the necessary databases created . The machine above must be able to connect to this PostgreSQL instance. All the following steps assume that you are starting from a blank slate. 3. Installing microk8s \u00b6 Make sure the machine has the nfs-common package installed. sudo apt update && sudo apt install nfs-common -y Install microk8s from the 1.15/stable channel. sudo snap install microk8s --classic --channel = 1 .15/stable && \\ sudo usermod -a -G microk8s $USER && \\ sudo su - $USER Check that microk8s is running. microk8s.status --wait-ready Install the version v2.16.3 of the helm binary HELM_PKG = helm-v2.16.3-linux-amd64.tar.gz wget https://get.helm.sh/ $HELM_PKG tar xvzf $HELM_PKG sudo mv linux-amd64/tiller linux-amd64/helm /usr/local/bin rm -rvf $HELM_PKG linux-amd64/ 4. Configuring microk8s \u00b6 First, we must enable the following plugins on microk8s: sudo echo \"--allow-privileged=true\" >> /var/snap/microk8s/current/args/kube-apiserver && \\ microk8s.enable dns && \\ microk8s.status --wait-ready && \\ microk8s.enable storage && \\ microk8s.status --wait-ready && \\ microk8s.enable ingress && \\ microk8s.status --wait-ready && \\ microk8s.stop && \\ microk8s.start && \\ microk8s.status --wait-ready Install Tiller: microk8s.kubectl create serviceaccount --namespace kube-system tiller && \\ microk8s.kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller && \\ helm init --service-account tiller The plugins are now enabled and the cluster bootstrapped. However, we must still wait for some microk8s internals (dns, http, and ingress) plugins to be ready, as failing to do so can result in pods entering a CrashLoopBackoff state: microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = kube-dns microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = hostpath-provisioner # If the following command fails, you probably installed the wrong microk8s version microk8s.kubectl wait -n default --for = condition = Ready pod -l name = nginx-ingress-microk8s microk8s.kubectl -n kube-system wait --for = condition = Ready pod -l name = tiller After these commands return successfully, we have ensured that dns, http, and nginx ingress are enabled and working properly inside the cluster. 5. Installing Codacy \u00b6 Any kubectl command from our chart installation must be executed as a microk8s.kubectl command. You can also create an alias to simplify the process: alias kubectl = 'microk8s.kubectl' When you get to the installation step you also need to append the values-microk8s.yaml configuration that downsizes some of the limits, making it easier to fit in the lightweight solution that is microk8s.","title":"Setting up a microk8s instance"},{"location":"infrastructure/microk8s-quickstart/#setting-up-a-microk8s-instance","text":"Microk8s is a single-package fully conformant lightweight Kubernetes that works on 42 Linux versions. The project is publicly available and can be found here . Follow the steps below to set up a microk8s instance from scratch, including all the necessary dependencies and configuration.","title":"Setting up a microk8s instance"},{"location":"infrastructure/microk8s-quickstart/#1-concepts","text":"","title":"1. Concepts"},{"location":"infrastructure/microk8s-quickstart/#helm-and-tiller","text":"Two executables will get installed onto the cluster as part of this process: helm and tiller . helm is responsible for resolving the configuration of the chart to be installed, while issuing the correct install commands onto the cluster. tiller is responsible for receiving the install commands issued by helm , as well as managing the lifecycle of the components that have been installed. helm is the client facing side, while tiller is the server/cluster facing side.","title":"Helm and tiller"},{"location":"infrastructure/microk8s-quickstart/#2-prepare-your-environment","text":"Prepare your environment to set up the microk8s cluster. For your infrastructure, you will need the following: A machine running Ubuntu 18.04 LTS. You must start a local or remote command line session on this machine. A PostgreSQL instance with all the necessary databases created . The machine above must be able to connect to this PostgreSQL instance. All the following steps assume that you are starting from a blank slate.","title":"2. Prepare your environment"},{"location":"infrastructure/microk8s-quickstart/#3-installing-microk8s","text":"Make sure the machine has the nfs-common package installed. sudo apt update && sudo apt install nfs-common -y Install microk8s from the 1.15/stable channel. sudo snap install microk8s --classic --channel = 1 .15/stable && \\ sudo usermod -a -G microk8s $USER && \\ sudo su - $USER Check that microk8s is running. microk8s.status --wait-ready Install the version v2.16.3 of the helm binary HELM_PKG = helm-v2.16.3-linux-amd64.tar.gz wget https://get.helm.sh/ $HELM_PKG tar xvzf $HELM_PKG sudo mv linux-amd64/tiller linux-amd64/helm /usr/local/bin rm -rvf $HELM_PKG linux-amd64/","title":"3. Installing microk8s"},{"location":"infrastructure/microk8s-quickstart/#4-configuring-microk8s","text":"First, we must enable the following plugins on microk8s: sudo echo \"--allow-privileged=true\" >> /var/snap/microk8s/current/args/kube-apiserver && \\ microk8s.enable dns && \\ microk8s.status --wait-ready && \\ microk8s.enable storage && \\ microk8s.status --wait-ready && \\ microk8s.enable ingress && \\ microk8s.status --wait-ready && \\ microk8s.stop && \\ microk8s.start && \\ microk8s.status --wait-ready Install Tiller: microk8s.kubectl create serviceaccount --namespace kube-system tiller && \\ microk8s.kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller && \\ helm init --service-account tiller The plugins are now enabled and the cluster bootstrapped. However, we must still wait for some microk8s internals (dns, http, and ingress) plugins to be ready, as failing to do so can result in pods entering a CrashLoopBackoff state: microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = kube-dns microk8s.kubectl wait -n kube-system --for = condition = Ready pod -l k8s-app = hostpath-provisioner # If the following command fails, you probably installed the wrong microk8s version microk8s.kubectl wait -n default --for = condition = Ready pod -l name = nginx-ingress-microk8s microk8s.kubectl -n kube-system wait --for = condition = Ready pod -l name = tiller After these commands return successfully, we have ensured that dns, http, and nginx ingress are enabled and working properly inside the cluster.","title":"4. Configuring microk8s"},{"location":"infrastructure/microk8s-quickstart/#5-installing-codacy","text":"Any kubectl command from our chart installation must be executed as a microk8s.kubectl command. You can also create an alias to simplify the process: alias kubectl = 'microk8s.kubectl' When you get to the installation step you also need to append the values-microk8s.yaml configuration that downsizes some of the limits, making it easier to fit in the lightweight solution that is microk8s.","title":"5. Installing Codacy"}]}