{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Codacy Chart \u00b6 With the Codacy chart it is possible to run all Codacy components and its dependencies with a single command line. edit image Each service in Codacy has its chart published to our charts repository . This chart bundles all the components and their dependencies. For the bundle, we make use of the requirements capability of Helm. Work in progress \u00b6 This chart is still a Work In Progress and is not ready for general usage. Our docker images are currently private and you will not be able to run the chart by yourself. If you are interested in trying out Codacy contact our support at support@codacy.com. Charts \u00b6 Documentation on a per-chart basis is listed here. Some of these repositories are private and accessible to Codacy engineers only. edit image Minio RabbitMQ-HA Postgres kube-fluentd-operator Codacy/ Token Management Codacy/ Website Codacy/ API Codacy/ Ragnaros Codacy/ Activities Codacy/ Repository Listener Codacy/ Portal Codacy/ Worker Manager Codacy/ Engine Codacy/ Core Codacy/ Hotspots API Codacy/ Hotspots Worker Configuration \u00b6 The following table lists the configurable parameters of the Codacy chart and their default values. Global parameters apply to all sub-charts and make it easier to configure resources across different components. Parameter Description Default global.codacy.url Hostname to your Codacy installation nil global.codacy.backendUrl Hostname to your Codacy installation nil global.play.cryptoSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.filestore.contentsSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.filestore.uuidSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.cacheSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.minio.create Create minio internally nil global.rabbitmq.create Create rabbitmq internally nil global.rabbitmq.rabbitmqUsername Username for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqUsername also. nil global.rabbitmq.rabbitmqPassword Password for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqPassword also. nil global.defaultdb.postgresqlUsername Username of the Postgresql server codacy global.defaultdb.postgresqlDatabase Database name of the Postgresql server default global.defaultdb.postgresqlPassword Hostname of the Postgresql server nil global.defaultdb.host Hostname of the Postgresql server nil global.defaultdb.service.port Port of the Postgresql server 5432 global.analysisdb.postgresqlUsername Username of the Postgresql server codacy global.analysisdb.postgresqlDatabase Database name of the Postgresql server analysis global.analysisdb.postgresqlPassword Hostname of the Postgresql server nil global.analysisdb.host Hostname of the Postgresql server nil global.analysisdb.service.port Port of the Postgresql server 5432 global.resultsdb201709.postgresqlUsername Username of the Postgresql server codacy global.resultsdb201709.postgresqlDatabase Database name of the Postgresql server results201709 global.resultsdb201709.postgresqlPassword Hostname of the Postgresql server nil global.resultsdb201709.host Hostname of the Postgresql server nil global.resultsdb201709.service.port Port of the Postgresql server 5432 global.metricsdb.postgresqlUsername Username of the Postgresql server codacy global.metricsdb.postgresqlDatabase Database name of the Postgresql server metrics global.metricsdb.postgresqlPassword Hostname of the Postgresql server nil global.metricsdb.host Hostname of the Postgresql server nil global.metricsdb.service.port Port of the Postgresql server 5432 global.filestoredb.postgresqlUsername Username of the Postgresql server codacy global.filestoredb.postgresqlDatabase Database name of the Postgresql server filestore global.filestoredb.postgresqlPassword Hostname of the Postgresql server nil global.filestoredb.host Hostname of the Postgresql server nil global.filestoredb.service.port Port of the Postgresql server 5432 global.jobsdb.postgresqlUsername Username of the Postgresql server codacy global.jobsdb.postgresqlDatabase Database name of the Postgresql server jobs global.jobsdb.postgresqlPassword Hostname of the Postgresql server nil global.jobsdb.host Hostname of the Postgresql server nil global.jobsdb.service.port Port of the Postgresql server 5432 global.githubEnterprise.enabled Enable githubEnterprise nil global.githubEnterprise.hostname Hostname of githubEnterprise instance nil global.githubEnterprise.protocol Protocol of githubEnterprise instance nil global.githubEnterprise.port Port of githubEnterprise instance nil global.githubEnterprise.isPrivateMode Status of private mode on githubEnterprise instance nil global.githubEnterprise.disableSSL Disable certificate validation on interaction with githubEnterprise instance nil global.gitlabEnterprise.enabled Enable gitlabEnterprise nil global.gitlabEnterprise.hostname Hostname of gitlabEnterprise instance nil global.gitlabEnterprise.protocol Protocol of gitlabEnterprise instance nil global.gitlabEnterprise.port Port of gitlabEnterprise instance nil global.bitbucketEnterprise.enabled Enable bitbucketEnterprise nil global.bitbucketEnterprise.hostname Hostname of bitbucketEnterprise instance nil global.bitbucketEnterprise.protocol Protocol of bitbucketEnterprise instance nil global.bitbucketEnterprise.port Port of bitbucketEnterprise instance nil global.features.cloneSubmodules Enable sharing of configuration files for the analysis tools placed on git submodules false The following parameters are specific to each Codacy component. Parameter Description Default portal.replicaCount Number of replicas 1 portal.image.repository Image repository from dependency portal.image.tag Image tag from dependency portal.service.type Portal service type ClusterIP portal.service.annotations Annotations to be added to the Portal service {} remote-provider-service.replicaCount Number of replicas 1 remote-provider-service.image.repository Image repository from dependency remote-provider-service.image.tag Image tag from dependency remote-provider-service.service.type Remote Provider service type ClusterIP remote-provider-service.service.annotations Annotations to be added to the Remote Provider service {} activities.replicaCount Number of replicas 1 activities.image.repository Image repository from dependency activities.image.tag Image tag from dependency activities.service.type Service type ClusterIP activities.service.annotations Annotations to be added to the service {} activities.activitiesdb.postgresqlUsername Username of the Postgresql server codacy activities.activitiesdb.postgresqlDatabase Database name of the Postgresql server jobs activities.activitiesdb.postgresqlPassword Hostname of the Postgresql server nil activities.activitiesdb.host Hostname of the Postgresql server nil activities.activitiesdb.service.port Port of the Postgresql server 5432 hotspots-api.replicaCount Number of replicas 1 hotspots-api.image.repository Image repository from dependency hotspots-api.image.tag Image tag from dependency hotspots-api.service.type Service type ClusterIP hotspots-api.service.annotations Annotations to be added to the service {} hotspots-api.hotspotsdb.postgresqlUsername Username of the Postgresql server codacy hotspots-api.hotspotsdb.postgresqlDatabase Database name of the Postgresql server hotspots hotspots-api.hotspotsdb.postgresqlPassword Hostname of the Postgresql server nil hotspots-api.hotspotsdb.host Hostname of the Postgresql server nil hotspots-api.hotspotsdb.service.port Port of the Postgresql server 5432 hotspots-worker.replicaCount Number of replicas 1 hotspots-worker.image.repository Image repository from dependency hotspots-worker.image.tag Image tag from dependency hotspots-worker.service.type Service type ClusterIP hotspots-worker.service.annotations Annotations to be added to the service {} listener.replicaCount Number of replicas 1 listener.image.repository Image repository from dependency listener.image.tag Image tag from dependency listener.service.type Service type ClusterIP listener.service.annotations Annotations to be added to the service {} listener.persistence.claim.size Each pod mounts and NFS disk and claims this size. 100Gi listener.nfsserverprovisioner.enabled Creates an NFS server and a storage class to mount volumes in that server. true listener.nfsserverprovisioner.persistence.enabled Creates an NFS provisioner true listener.nfsserverprovisioner.persistence.size Size of the NFS server disk 120Gi listener.listenerdb.postgresqlUsername Username of the Postgresql server codacy listener.listenerdb.postgresqlDatabase Database name of the Postgresql server listener listener.listenerdb.postgresqlPassword Hostname of the Postgresql server PLEASE_CHANGE_ME listener.listenerdb.host Hostname of the Postgresql server codacy-listenerdb.codacy.svc.cluster.local listener.listenerdb.service.port Port of the Postgresql server 5432 core.replicaCount Number of replicas 1 core.image.repository Image repository from dependency core.image.tag Image tag from dependency core.service.type Service type ClusterIP engine.replicaCount Number of replicas 1 engine.image.repository Image repository from dependency engine.image.tag Image tag from dependency engine.service.type Service type ClusterIP engine.service.annotations Annotations to be added to the service {} engine.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.image.repository Image repository from dependency codacy-api.image.tag Image tag from dependency codacy-api.service.type Service type ClusterIP codacy-api.config.license Codacy license for your installation nil codacy-api.service.annotations Annotations to be added to the service {} codacy-api.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.metrics.grafana_dashboards.enabled Create the ConfigMap with the dashboard of this component. Can be imported through grafana sidecar. false worker-manager.config.workers.genericMax TBD 100 worker-manager.config.workers.dedicatedMax TBD 100 crow.replicaCount Number of replicas 1 crow.image.repository Image repository from dependency crow.image.tag Image tag from dependency The following parameters refer to components that are not internal to Codacy, but go as part of this bundle so that you can bootstrap Codacy faster. Parameter Description Default fluentdoperator.enable Enable fluentd operator. It gathers logs from Codacy so that you can send it to our support if needed. false fluentdoperator.expirationDays Number of days to retain logs. More time uses more disk on minio. 14 rabbitmq-ha.rabbitmqUsername Username for the bundled RabbitMQ. rabbitmq rabbitmq-ha.rabbitmqPassword Password for the bundled RabbitMQ. rabbitmq You can also configure values for the PostgreSQL database via the Postgresql README.md For overriding variables see: Customizing the chart","title":"Codacy Chart"},{"location":"#codacy-chart","text":"With the Codacy chart it is possible to run all Codacy components and its dependencies with a single command line. edit image Each service in Codacy has its chart published to our charts repository . This chart bundles all the components and their dependencies. For the bundle, we make use of the requirements capability of Helm.","title":"Codacy Chart"},{"location":"#work-in-progress","text":"This chart is still a Work In Progress and is not ready for general usage. Our docker images are currently private and you will not be able to run the chart by yourself. If you are interested in trying out Codacy contact our support at support@codacy.com.","title":"Work in progress"},{"location":"#charts","text":"Documentation on a per-chart basis is listed here. Some of these repositories are private and accessible to Codacy engineers only. edit image Minio RabbitMQ-HA Postgres kube-fluentd-operator Codacy/ Token Management Codacy/ Website Codacy/ API Codacy/ Ragnaros Codacy/ Activities Codacy/ Repository Listener Codacy/ Portal Codacy/ Worker Manager Codacy/ Engine Codacy/ Core Codacy/ Hotspots API Codacy/ Hotspots Worker","title":"Charts"},{"location":"#configuration","text":"The following table lists the configurable parameters of the Codacy chart and their default values. Global parameters apply to all sub-charts and make it easier to configure resources across different components. Parameter Description Default global.codacy.url Hostname to your Codacy installation nil global.codacy.backendUrl Hostname to your Codacy installation nil global.play.cryptoSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.filestore.contentsSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.filestore.uuidSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.cacheSecret Secrets used internally for encryption. Generate one with `cat /dev/urandom tr -dc 'a-zA-Z0-9' fold -w 128 head -n 1` nil global.minio.create Create minio internally nil global.rabbitmq.create Create rabbitmq internally nil global.rabbitmq.rabbitmqUsername Username for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqUsername also. nil global.rabbitmq.rabbitmqPassword Password for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqPassword also. nil global.defaultdb.postgresqlUsername Username of the Postgresql server codacy global.defaultdb.postgresqlDatabase Database name of the Postgresql server default global.defaultdb.postgresqlPassword Hostname of the Postgresql server nil global.defaultdb.host Hostname of the Postgresql server nil global.defaultdb.service.port Port of the Postgresql server 5432 global.analysisdb.postgresqlUsername Username of the Postgresql server codacy global.analysisdb.postgresqlDatabase Database name of the Postgresql server analysis global.analysisdb.postgresqlPassword Hostname of the Postgresql server nil global.analysisdb.host Hostname of the Postgresql server nil global.analysisdb.service.port Port of the Postgresql server 5432 global.resultsdb201709.postgresqlUsername Username of the Postgresql server codacy global.resultsdb201709.postgresqlDatabase Database name of the Postgresql server results201709 global.resultsdb201709.postgresqlPassword Hostname of the Postgresql server nil global.resultsdb201709.host Hostname of the Postgresql server nil global.resultsdb201709.service.port Port of the Postgresql server 5432 global.metricsdb.postgresqlUsername Username of the Postgresql server codacy global.metricsdb.postgresqlDatabase Database name of the Postgresql server metrics global.metricsdb.postgresqlPassword Hostname of the Postgresql server nil global.metricsdb.host Hostname of the Postgresql server nil global.metricsdb.service.port Port of the Postgresql server 5432 global.filestoredb.postgresqlUsername Username of the Postgresql server codacy global.filestoredb.postgresqlDatabase Database name of the Postgresql server filestore global.filestoredb.postgresqlPassword Hostname of the Postgresql server nil global.filestoredb.host Hostname of the Postgresql server nil global.filestoredb.service.port Port of the Postgresql server 5432 global.jobsdb.postgresqlUsername Username of the Postgresql server codacy global.jobsdb.postgresqlDatabase Database name of the Postgresql server jobs global.jobsdb.postgresqlPassword Hostname of the Postgresql server nil global.jobsdb.host Hostname of the Postgresql server nil global.jobsdb.service.port Port of the Postgresql server 5432 global.githubEnterprise.enabled Enable githubEnterprise nil global.githubEnterprise.hostname Hostname of githubEnterprise instance nil global.githubEnterprise.protocol Protocol of githubEnterprise instance nil global.githubEnterprise.port Port of githubEnterprise instance nil global.githubEnterprise.isPrivateMode Status of private mode on githubEnterprise instance nil global.githubEnterprise.disableSSL Disable certificate validation on interaction with githubEnterprise instance nil global.gitlabEnterprise.enabled Enable gitlabEnterprise nil global.gitlabEnterprise.hostname Hostname of gitlabEnterprise instance nil global.gitlabEnterprise.protocol Protocol of gitlabEnterprise instance nil global.gitlabEnterprise.port Port of gitlabEnterprise instance nil global.bitbucketEnterprise.enabled Enable bitbucketEnterprise nil global.bitbucketEnterprise.hostname Hostname of bitbucketEnterprise instance nil global.bitbucketEnterprise.protocol Protocol of bitbucketEnterprise instance nil global.bitbucketEnterprise.port Port of bitbucketEnterprise instance nil global.features.cloneSubmodules Enable sharing of configuration files for the analysis tools placed on git submodules false The following parameters are specific to each Codacy component. Parameter Description Default portal.replicaCount Number of replicas 1 portal.image.repository Image repository from dependency portal.image.tag Image tag from dependency portal.service.type Portal service type ClusterIP portal.service.annotations Annotations to be added to the Portal service {} remote-provider-service.replicaCount Number of replicas 1 remote-provider-service.image.repository Image repository from dependency remote-provider-service.image.tag Image tag from dependency remote-provider-service.service.type Remote Provider service type ClusterIP remote-provider-service.service.annotations Annotations to be added to the Remote Provider service {} activities.replicaCount Number of replicas 1 activities.image.repository Image repository from dependency activities.image.tag Image tag from dependency activities.service.type Service type ClusterIP activities.service.annotations Annotations to be added to the service {} activities.activitiesdb.postgresqlUsername Username of the Postgresql server codacy activities.activitiesdb.postgresqlDatabase Database name of the Postgresql server jobs activities.activitiesdb.postgresqlPassword Hostname of the Postgresql server nil activities.activitiesdb.host Hostname of the Postgresql server nil activities.activitiesdb.service.port Port of the Postgresql server 5432 hotspots-api.replicaCount Number of replicas 1 hotspots-api.image.repository Image repository from dependency hotspots-api.image.tag Image tag from dependency hotspots-api.service.type Service type ClusterIP hotspots-api.service.annotations Annotations to be added to the service {} hotspots-api.hotspotsdb.postgresqlUsername Username of the Postgresql server codacy hotspots-api.hotspotsdb.postgresqlDatabase Database name of the Postgresql server hotspots hotspots-api.hotspotsdb.postgresqlPassword Hostname of the Postgresql server nil hotspots-api.hotspotsdb.host Hostname of the Postgresql server nil hotspots-api.hotspotsdb.service.port Port of the Postgresql server 5432 hotspots-worker.replicaCount Number of replicas 1 hotspots-worker.image.repository Image repository from dependency hotspots-worker.image.tag Image tag from dependency hotspots-worker.service.type Service type ClusterIP hotspots-worker.service.annotations Annotations to be added to the service {} listener.replicaCount Number of replicas 1 listener.image.repository Image repository from dependency listener.image.tag Image tag from dependency listener.service.type Service type ClusterIP listener.service.annotations Annotations to be added to the service {} listener.persistence.claim.size Each pod mounts and NFS disk and claims this size. 100Gi listener.nfsserverprovisioner.enabled Creates an NFS server and a storage class to mount volumes in that server. true listener.nfsserverprovisioner.persistence.enabled Creates an NFS provisioner true listener.nfsserverprovisioner.persistence.size Size of the NFS server disk 120Gi listener.listenerdb.postgresqlUsername Username of the Postgresql server codacy listener.listenerdb.postgresqlDatabase Database name of the Postgresql server listener listener.listenerdb.postgresqlPassword Hostname of the Postgresql server PLEASE_CHANGE_ME listener.listenerdb.host Hostname of the Postgresql server codacy-listenerdb.codacy.svc.cluster.local listener.listenerdb.service.port Port of the Postgresql server 5432 core.replicaCount Number of replicas 1 core.image.repository Image repository from dependency core.image.tag Image tag from dependency core.service.type Service type ClusterIP engine.replicaCount Number of replicas 1 engine.image.repository Image repository from dependency engine.image.tag Image tag from dependency engine.service.type Service type ClusterIP engine.service.annotations Annotations to be added to the service {} engine.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.image.repository Image repository from dependency codacy-api.image.tag Image tag from dependency codacy-api.service.type Service type ClusterIP codacy-api.config.license Codacy license for your installation nil codacy-api.service.annotations Annotations to be added to the service {} codacy-api.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.metrics.grafana_dashboards.enabled Create the ConfigMap with the dashboard of this component. Can be imported through grafana sidecar. false worker-manager.config.workers.genericMax TBD 100 worker-manager.config.workers.dedicatedMax TBD 100 crow.replicaCount Number of replicas 1 crow.image.repository Image repository from dependency crow.image.tag Image tag from dependency The following parameters refer to components that are not internal to Codacy, but go as part of this bundle so that you can bootstrap Codacy faster. Parameter Description Default fluentdoperator.enable Enable fluentd operator. It gathers logs from Codacy so that you can send it to our support if needed. false fluentdoperator.expirationDays Number of days to retain logs. More time uses more disk on minio. 14 rabbitmq-ha.rabbitmqUsername Username for the bundled RabbitMQ. rabbitmq rabbitmq-ha.rabbitmqPassword Password for the bundled RabbitMQ. rabbitmq You can also configure values for the PostgreSQL database via the Postgresql README.md For overriding variables see: Customizing the chart","title":"Configuration"},{"location":"configuration/","text":"Configuration \u00b6 Use External Databases Logging Monitoring Providers","title":"Configuration"},{"location":"configuration/#configuration","text":"Use External Databases Logging Monitoring Providers","title":"Configuration"},{"location":"configuration/external-dbs/","text":"Use external databases \u00b6 Even thought we deliver postgres as a requirements so that you can start Codacy really fast, it's really not recommended for a production ready installation. Assuming you have a postgres database available. Follow the following steps to have Codacy using it: Create a user to be used by Codacy in your postgres: CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; Create the different databases needed for the different services in postgres: CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotsposts WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; Create a database-values.yaml file that disables the creation of the DBs inside the cluster and configure the correct external hostname , username , password and databaseName . global : defaultdb : create : false postgresqlUsername : codacy postgresqlDatabase : accounts # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 analysisdb : create : false postgresqlUsername : codacy postgresqlDatabase : analysis # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 resultsdb201709 : create : false postgresqlUsername : codacy postgresqlDatabase : results # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 metricsdb : create : false postgresqlUsername : codacy postgresqlDatabase : metrics # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 filestoredb : create : false postgresqlUsername : codacy postgresqlDatabase : filestore # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 jobsdb : create : false postgresqlUsername : codacy postgresqlDatabase : jobs # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 activities : activitiesdb : create : false postgresqlUsername : codacy postgresqlDatabase : activities # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 hotspots-api : hotspotsdb : create : false postgresqlUsername : codacy postgresqlDatabase : hotsposts # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 listener : listenerdb : create : false postgresqlUsername : codacy postgresqlDatabase : listener # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432","title":"Use external databases"},{"location":"configuration/external-dbs/#use-external-databases","text":"Even thought we deliver postgres as a requirements so that you can start Codacy really fast, it's really not recommended for a production ready installation. Assuming you have a postgres database available. Follow the following steps to have Codacy using it: Create a user to be used by Codacy in your postgres: CREATE USER codacy WITH PASSWORD 'codacy' ; ALTER ROLE codacy WITH CREATEDB ; Create the different databases needed for the different services in postgres: CREATE DATABASE accounts WITH OWNER = codacy ; CREATE DATABASE analysis WITH OWNER = codacy ; CREATE DATABASE results WITH OWNER = codacy ; CREATE DATABASE metrics WITH OWNER = codacy ; CREATE DATABASE filestore WITH OWNER = codacy ; CREATE DATABASE jobs WITH OWNER = codacy ; CREATE DATABASE activities WITH OWNER = codacy ; CREATE DATABASE hotsposts WITH OWNER = codacy ; CREATE DATABASE listener WITH OWNER = codacy ; Create a database-values.yaml file that disables the creation of the DBs inside the cluster and configure the correct external hostname , username , password and databaseName . global : defaultdb : create : false postgresqlUsername : codacy postgresqlDatabase : accounts # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 analysisdb : create : false postgresqlUsername : codacy postgresqlDatabase : analysis # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 resultsdb201709 : create : false postgresqlUsername : codacy postgresqlDatabase : results # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 metricsdb : create : false postgresqlUsername : codacy postgresqlDatabase : metrics # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 filestoredb : create : false postgresqlUsername : codacy postgresqlDatabase : filestore # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 jobsdb : create : false postgresqlUsername : codacy postgresqlDatabase : jobs # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 activities : activitiesdb : create : false postgresqlUsername : codacy postgresqlDatabase : activities # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 hotspots-api : hotspotsdb : create : false postgresqlUsername : codacy postgresqlDatabase : hotsposts # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432 listener : listenerdb : create : false postgresqlUsername : codacy postgresqlDatabase : listener # You need to create the DB manually postgresqlPassword : codacy host : codacy-database.internal service : port : 5432","title":"Use external databases"},{"location":"configuration/logging/","text":"Logging \u00b6 To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Logging"},{"location":"configuration/logging/#logging","text":"To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Logging"},{"location":"configuration/monitoring/","text":"Monitoring \u00b6 In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana. Setting up monitoring using Grafana & Prometheus \u00b6 The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Setting up monitoring using Crow [deprecated] \u00b6 Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed along side Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Monitoring"},{"location":"configuration/monitoring/#monitoring","text":"In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana.","title":"Monitoring"},{"location":"configuration/monitoring/#setting-up-monitoring-using-grafana-prometheus","text":"The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true","title":"Setting up monitoring using Grafana &amp; Prometheus"},{"location":"configuration/monitoring/#setting-up-monitoring-using-crow-deprecated","text":"Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed along side Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Setting up monitoring using Crow [deprecated]"},{"location":"configuration/providers/","text":"Providers \u00b6 There are the providers currently supported by Codacy: Github Cloud Github Enterprise Gitlab Enterprise Bitbucket Enterprise","title":"Providers"},{"location":"configuration/providers/#providers","text":"There are the providers currently supported by Codacy: Github Cloud Github Enterprise Gitlab Enterprise Bitbucket Enterprise","title":"Providers"},{"location":"configuration/providers/bitbucket-enterprise/","text":"Bitbucket Stash \u00b6 Set your configuration values for your Bitbucket instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of. Stash Application Link \u00b6 To set up Stash you need to create an application link on your Stash installation. You can click on here and go to the application links list. Application Link Creation \u00b6 Create the link, use your Codacy installation URL for this Name the link \u00b6 Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection. Add incoming connection \u00b6 Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Bitbucket Stash"},{"location":"configuration/providers/bitbucket-enterprise/#bitbucket-stash","text":"Set your configuration values for your Bitbucket instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of.","title":"Bitbucket Stash"},{"location":"configuration/providers/bitbucket-enterprise/#stash-application-link","text":"To set up Stash you need to create an application link on your Stash installation. You can click on here and go to the application links list.","title":"Stash Application Link"},{"location":"configuration/providers/bitbucket-enterprise/#application-link-creation","text":"Create the link, use your Codacy installation URL for this","title":"Application Link Creation"},{"location":"configuration/providers/bitbucket-enterprise/#name-the-link","text":"Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection.","title":"Name the link"},{"location":"configuration/providers/bitbucket-enterprise/#add-incoming-connection","text":"Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Add incoming connection"},{"location":"configuration/providers/github-cloud/","text":"GitHub Cloud \u00b6 GitHub Application \u00b6 To integrate with GitHub we use a GitHub Application. To create the application on GitHub, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IO, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Cloud"},{"location":"configuration/providers/github-cloud/#github-cloud","text":"","title":"GitHub Cloud"},{"location":"configuration/providers/github-cloud/#github-application","text":"To integrate with GitHub we use a GitHub Application. To create the application on GitHub, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IO, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Application"},{"location":"configuration/providers/github-cloud/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/providers/github-enterprise/","text":"GitHub Enterprise \u00b6 Set your configuration values for your GitHub instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitHub Application \u00b6 To integrate with GitHub we use a GitHub Application. To create the application in your GitHub Enterprise, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/ip, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Enterprise"},{"location":"configuration/providers/github-enterprise/#github-enterprise","text":"Set your configuration values for your GitHub instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitHub Enterprise"},{"location":"configuration/providers/github-enterprise/#github-application","text":"To integrate with GitHub we use a GitHub Application. To create the application in your GitHub Enterprise, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/ip, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Application"},{"location":"configuration/providers/github-enterprise/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/providers/gitlab-enterprise/","text":"GitLab Enterprise \u00b6 Set your configuration values for your GitLab instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitLab Application \u00b6 To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Enterprise"},{"location":"configuration/providers/gitlab-enterprise/#gitlab-enterprise","text":"Set your configuration values for your GitLab instance on the values.yaml file. Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitLab Enterprise"},{"location":"configuration/providers/gitlab-enterprise/#gitlab-application","text":"To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Application"},{"location":"installation/","text":"Install \u00b6 Install Codacy on Kubernetes with the cloud native Codacy Helm chart. This guide will cover the required values and common options. Before starting, make sure you are aware of the requirements . TL;DR - Quickly install Codacy for demo without any persistence. \u00b6 Some Codacy images are currently private. For this, you need to create a secret in the same namespace were you will install Codacy. You should receive these credentials together with your license. $ kubectl create namespace codacy $ kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy Create a file named values.yaml , using a text editor of your choice. global : imagePullSecrets : - name : docker-credentials # Access private codacy docker images codacy : url : \"http://codacy.mycompany.com\" # This value is important for VCS configuration and badges to work backendUrl : \"http://codacy.mycompany.com\" # This value is important for VCS configuration and badges to work play : cryptoSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` filestore : contentsSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` uuidSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` cacheSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` $ helm repo add codacy-stable https://charts.codacy.com/stable/ $ helm repo update $ helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --recreate-pods \\ --values values.yaml By now all pods should be starting. $ helm status codacy LAST DEPLOYED: Wed Jan 8 15 :52:27 2020 NAMESPACE: codacy STATUS: DEPLOYED RESOURCES: == > v1/ClusterRole NAME AGE codacy-nfsserverprovisioner 12d == > v1/ClusterRoleBinding NAME AGE codacy-nfsserverprovisioner 12d == > v1/ConfigMap NAME DATA AGE codacy-activities 12 12d codacy-api 69 12d codacy-core 19 12d codacy-crow 17 12d codacy-engine 43 12d codacy-fluentd-config 1 12d codacy-hotspots-api 16 12d codacy-hotspots-worker 18 12d codacy-listener 27 12d codacy-listener-nfs-cleanup 1 12d codacy-minio 1 12d codacy-portal 44 12d codacy-postgres-extended-configuration 1 12d codacy-postgres-init-scripts 1 12d codacy-rabbitmq-ha 2 12d codacy-ragnaros 18 12d codacy-remote-provider-service 12 12d codacy-worker-manager 71 98m == > v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE codacy-activities 1 /2 2 1 12d codacy-api 3 /3 3 3 12d codacy-core 2 /2 2 2 12d codacy-crow 1 /1 1 1 12d codacy-engine 2 /2 2 2 12d codacy-hotspots-api 2 /2 2 2 12d codacy-hotspots-worker 2 /2 2 2 12d codacy-listener 1 /1 1 1 12d codacy-minio 1 /1 1 1 12d codacy-portal 2 /2 2 2 12d codacy-ragnaros 1 /1 1 1 12d codacy-remote-provider-service 2 /2 2 2 12d codacy-worker-manager 2 /2 2 2 12d == > v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE codacy-listener-cache-claim Bound pvc-dfc57bfe-31fd-11ea-8144-4e0c422f671d 140Gi RWO codacy-listener-cache-class 5h56m Filesystem codacy-minio Bound pvc-fa775d78-289c-11ea-8144-4e0c422f671d 20Gi RWO do -block-storage 12d Filesystem == > v1/Pod ( related ) NAME READY STATUS RESTARTS AGE codacy-activities-78849c8548-ln5sl 1 /1 Running 4 6m11s codacy-activitiesdb-0 1 /1 Running 0 6m3s codacy-api-6f44c8d48-6bw8z 1 /1 Running 0 6m11s codacy-api-6f44c8d48-h6cl4 1 /1 Running 0 6m11s codacy-api-6f44c8d48-vgbl5 1 /1 Running 0 6m11s codacy-core-786c6f79f-7sn7p 1 /1 Running 0 6m11s codacy-core-786c6f79f-pvg9w 1 /1 Running 0 6m11s codacy-crow-59dcfd57f-zpdrw 1 /1 Running 3 6m10s codacy-crowdb-0 1 /1 Running 0 6m6s codacy-engine-5ffcddf77b-nrs8n 1 /1 Running 1 6m10s codacy-engine-5ffcddf77b-s229m 1 /1 Running 2 6m10s codacy-fluentdoperator-6qxpb 2 /2 Running 0 5m58s codacy-fluentdoperator-djf8f 2 /2 Running 0 5m59s codacy-fluentdoperator-jx269 2 /2 Running 0 5m57s codacy-fluentdoperator-mlpz5 2 /2 Running 0 5m59s codacy-fluentdoperator-xbmw9 2 /2 Running 0 5m59s codacy-hotspots-api-5978df9455-6q2sv 1 /1 Running 3 6m10s codacy-hotspots-api-5978df9455-nlzwv 1 /1 Running 3 6m10s codacy-hotspots-worker-cbc9dfb7c-dxdqs 1 /1 Running 3 6m10s codacy-hotspots-worker-cbc9dfb7c-jjw4q 1 /1 Running 3 6m10s codacy-hotspotsdb-0 1 /1 Running 0 6m4s codacy-listener-7c6ff5f8cd-hbgh2 1 /1 Running 2 6m10s codacy-listenerdb-0 1 /1 Running 0 6m4s codacy-minio-847c574d94-spbbz 1 /1 Running 0 6m10s codacy-nfsserverprovisioner-0 1 /1 Running 0 6m3s codacy-portal-74bf7887db-fvzvz 1 /1 Running 3 6m9s codacy-portal-74bf7887db-h7tb6 1 /1 Running 4 6m9s codacy-postgres-0 1 /1 Running 0 6m1s codacy-rabbitmq-ha-0 1 /1 Running 0 5m58s codacy-rabbitmq-ha-1 1 /1 Running 0 5m31s codacy-rabbitmq-ha-2 1 /1 Running 0 5m4s codacy-ragnaros-8b899c77f-76bw6 1 /1 Running 2 6m9s codacy-remote-provider-service-7974696f8d-tccvj 1 /1 Running 0 6m9s codacy-remote-provider-service-7974696f8d-wx6zj 1 /1 Running 0 6m9s codacy-worker-manager-77878c655f-7jmjv 1 /1 Running 0 6m8s codacy-worker-manager-77878c655f-8s4bz 1 /1 Running 0 6m9s == > v1/Role NAME AGE codacy-remote-provider-service 12d codacy-worker-manager 12d codacy-worker-manager-worker-permissions 12d == > v1/RoleBinding NAME AGE codacy-remote-provider-service 12d codacy-worker-manager 12d codacy-worker-manager-worker-permissions 12d == > v1/Secret NAME TYPE DATA AGE codacy-activities Opaque 3 12d codacy-activitiesdb Opaque 1 12d codacy-api Opaque 31 12d codacy-core Opaque 6 12d codacy-crow Opaque 5 12d codacy-crowdb Opaque 1 12d codacy-engine Opaque 14 12d codacy-hotspots-api Opaque 4 12d codacy-hotspots-worker Opaque 3 12d codacy-hotspotsdb Opaque 1 12d codacy-listener Opaque 6 12d codacy-listenerdb Opaque 1 12d codacy-minio Opaque 2 12d codacy-portal Opaque 17 12d codacy-postgres Opaque 1 12d codacy-rabbitmq-ha Opaque 6 12d codacy-ragnaros Opaque 11 12d codacy-worker-manager Opaque 17 12d == > v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE codacy-activities ClusterIP 10 .245.117.130 <none> 80 /TCP 12d codacy-activitiesdb ClusterIP 10 .245.86.136 <none> 5432 /TCP 12d codacy-activitiesdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-api LoadBalancer 10 .245.218.1 157 .245.20.169 80 :31652/TCP 12d codacy-core ClusterIP 10 .245.76.203 <none> 80 /TCP 12d codacy-crow ClusterIP 10 .245.149.46 <none> 80 /TCP 12d codacy-crowdb ClusterIP 10 .245.204.138 <none> 5432 /TCP 12d codacy-crowdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-engine ClusterIP 10 .245.196.196 <none> 80 /TCP,9001/TCP 12d codacy-hotspots-api ClusterIP 10 .245.16.156 <none> 80 /TCP 12d codacy-hotspots-worker ClusterIP 10 .245.123.189 <none> 80 /TCP 12d codacy-hotspotsdb ClusterIP 10 .245.208.1 <none> 5432 /TCP 12d codacy-hotspotsdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-listener ClusterIP 10 .245.56.148 <none> 80 /TCP 12d codacy-listenerdb ClusterIP 10 .245.67.58 <none> 5432 /TCP 12d codacy-listenerdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-minio ClusterIP 10 .245.127.197 <none> 9000 /TCP 12d codacy-nfsserverprovisioner ClusterIP 10 .245.159.117 <none> 2049 /TCP,20048/TCP,51413/TCP,51413/UDP 12d codacy-portal ClusterIP 10 .245.104.235 <none> 80 /TCP 12d codacy-postgres ClusterIP 10 .245.20.80 <none> 5432 /TCP 12d codacy-postgres-headless ClusterIP None <none> 5432 /TCP 12d codacy-rabbitmq-ha ClusterIP None <none> 15672 /TCP,5672/TCP,4369/TCP 12d codacy-rabbitmq-ha-discovery ClusterIP None <none> 15672 /TCP,5672/TCP,4369/TCP 12d codacy-ragnaros ClusterIP 10 .245.172.99 <none> 80 /TCP 12d codacy-remote-provider-service ClusterIP 10 .245.247.247 <none> 80 /TCP,5701/TCP 12d codacy-worker-manager ClusterIP 10 .245.211.195 <none> 80 /TCP 12d == > v1/ServiceAccount NAME SECRETS AGE codacy-fluentdoperator 1 12d codacy-minio 1 12d codacy-nfsserverprovisioner 1 12d codacy-rabbitmq-ha 1 12d codacy-remote-provider-service 1 12d codacy-worker-manager 1 12d codacy-worker-manager-worker-permissions 1 12d == > v1/StatefulSet NAME READY AGE codacy-nfsserverprovisioner 1 /1 12d == > v1/StorageClass NAME PROVISIONER AGE codacy-listener-cache-class cluster.local/codacy-nfsserverprovisioner 12d == > v1beta1/ClusterRole NAME AGE codacy-fluentdoperator 12d == > v1beta1/ClusterRoleBinding NAME AGE codacy-fluentdoperator 12d == > v1beta1/CronJob NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE codacy-listener-nfs-cleanup 0 0 * * 6 False 0 4d15h 12d == > v1beta1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE codacy-fluentdoperator 5 5 5 5 5 <none> 12d == > v1beta1/PodDisruptionBudget NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE codacy-activities N/A 1 0 12d codacy-api N/A 1 1 5d22h codacy-core N/A 1 1 12d codacy-engine N/A 1 1 12d codacy-hotspots-api N/A 1 1 12d codacy-hotspots-worker N/A 1 1 12d codacy-listener N/A 1 1 12d codacy-portal N/A 1 1 12d codacy-remote-provider-service N/A 1 1 26h codacy-worker-manager N/A 1 1 5d22h == > v1beta1/Role NAME AGE codacy-rabbitmq-ha 12d == > v1beta1/RoleBinding NAME AGE codacy-rabbitmq-ha 12d == > v1beta1/StatefulSet NAME READY AGE codacy-rabbitmq-ha 3 /3 12d == > v1beta2/StatefulSet NAME READY AGE codacy-activitiesdb 1 /1 12d codacy-crowdb 1 /1 12d codacy-hotspotsdb 1 /1 12d codacy-listenerdb 1 /1 12d codacy-postgres 1 /1 12d Next steps - Making it \"Production Ready\" \u00b6 Use the values-production.yaml file as reference Use external DBs (Ideally a cloud managed postgres) Enable persistence on listener Enable persistence on minio Setup resources and limits Enable the Ingress on codacy-api Setup a git provider Start configuring Codacy in the UI Create an administrator account Create an initial organization Invite users Proceed to more advanced configurations .","title":"Install"},{"location":"installation/#install","text":"Install Codacy on Kubernetes with the cloud native Codacy Helm chart. This guide will cover the required values and common options. Before starting, make sure you are aware of the requirements .","title":"Install"},{"location":"installation/#tldr-quickly-install-codacy-for-demo-without-any-persistence","text":"Some Codacy images are currently private. For this, you need to create a secret in the same namespace were you will install Codacy. You should receive these credentials together with your license. $ kubectl create namespace codacy $ kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy Create a file named values.yaml , using a text editor of your choice. global : imagePullSecrets : - name : docker-credentials # Access private codacy docker images codacy : url : \"http://codacy.mycompany.com\" # This value is important for VCS configuration and badges to work backendUrl : \"http://codacy.mycompany.com\" # This value is important for VCS configuration and badges to work play : cryptoSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` filestore : contentsSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` uuidSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` cacheSecret : \"PLEASE_CHANGE_ME\" # Generate one with `openssl rand -base64 32 | tr -dc 'a-zA-Z0-9'` $ helm repo add codacy-stable https://charts.codacy.com/stable/ $ helm repo update $ helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --recreate-pods \\ --values values.yaml By now all pods should be starting. $ helm status codacy LAST DEPLOYED: Wed Jan 8 15 :52:27 2020 NAMESPACE: codacy STATUS: DEPLOYED RESOURCES: == > v1/ClusterRole NAME AGE codacy-nfsserverprovisioner 12d == > v1/ClusterRoleBinding NAME AGE codacy-nfsserverprovisioner 12d == > v1/ConfigMap NAME DATA AGE codacy-activities 12 12d codacy-api 69 12d codacy-core 19 12d codacy-crow 17 12d codacy-engine 43 12d codacy-fluentd-config 1 12d codacy-hotspots-api 16 12d codacy-hotspots-worker 18 12d codacy-listener 27 12d codacy-listener-nfs-cleanup 1 12d codacy-minio 1 12d codacy-portal 44 12d codacy-postgres-extended-configuration 1 12d codacy-postgres-init-scripts 1 12d codacy-rabbitmq-ha 2 12d codacy-ragnaros 18 12d codacy-remote-provider-service 12 12d codacy-worker-manager 71 98m == > v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE codacy-activities 1 /2 2 1 12d codacy-api 3 /3 3 3 12d codacy-core 2 /2 2 2 12d codacy-crow 1 /1 1 1 12d codacy-engine 2 /2 2 2 12d codacy-hotspots-api 2 /2 2 2 12d codacy-hotspots-worker 2 /2 2 2 12d codacy-listener 1 /1 1 1 12d codacy-minio 1 /1 1 1 12d codacy-portal 2 /2 2 2 12d codacy-ragnaros 1 /1 1 1 12d codacy-remote-provider-service 2 /2 2 2 12d codacy-worker-manager 2 /2 2 2 12d == > v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE codacy-listener-cache-claim Bound pvc-dfc57bfe-31fd-11ea-8144-4e0c422f671d 140Gi RWO codacy-listener-cache-class 5h56m Filesystem codacy-minio Bound pvc-fa775d78-289c-11ea-8144-4e0c422f671d 20Gi RWO do -block-storage 12d Filesystem == > v1/Pod ( related ) NAME READY STATUS RESTARTS AGE codacy-activities-78849c8548-ln5sl 1 /1 Running 4 6m11s codacy-activitiesdb-0 1 /1 Running 0 6m3s codacy-api-6f44c8d48-6bw8z 1 /1 Running 0 6m11s codacy-api-6f44c8d48-h6cl4 1 /1 Running 0 6m11s codacy-api-6f44c8d48-vgbl5 1 /1 Running 0 6m11s codacy-core-786c6f79f-7sn7p 1 /1 Running 0 6m11s codacy-core-786c6f79f-pvg9w 1 /1 Running 0 6m11s codacy-crow-59dcfd57f-zpdrw 1 /1 Running 3 6m10s codacy-crowdb-0 1 /1 Running 0 6m6s codacy-engine-5ffcddf77b-nrs8n 1 /1 Running 1 6m10s codacy-engine-5ffcddf77b-s229m 1 /1 Running 2 6m10s codacy-fluentdoperator-6qxpb 2 /2 Running 0 5m58s codacy-fluentdoperator-djf8f 2 /2 Running 0 5m59s codacy-fluentdoperator-jx269 2 /2 Running 0 5m57s codacy-fluentdoperator-mlpz5 2 /2 Running 0 5m59s codacy-fluentdoperator-xbmw9 2 /2 Running 0 5m59s codacy-hotspots-api-5978df9455-6q2sv 1 /1 Running 3 6m10s codacy-hotspots-api-5978df9455-nlzwv 1 /1 Running 3 6m10s codacy-hotspots-worker-cbc9dfb7c-dxdqs 1 /1 Running 3 6m10s codacy-hotspots-worker-cbc9dfb7c-jjw4q 1 /1 Running 3 6m10s codacy-hotspotsdb-0 1 /1 Running 0 6m4s codacy-listener-7c6ff5f8cd-hbgh2 1 /1 Running 2 6m10s codacy-listenerdb-0 1 /1 Running 0 6m4s codacy-minio-847c574d94-spbbz 1 /1 Running 0 6m10s codacy-nfsserverprovisioner-0 1 /1 Running 0 6m3s codacy-portal-74bf7887db-fvzvz 1 /1 Running 3 6m9s codacy-portal-74bf7887db-h7tb6 1 /1 Running 4 6m9s codacy-postgres-0 1 /1 Running 0 6m1s codacy-rabbitmq-ha-0 1 /1 Running 0 5m58s codacy-rabbitmq-ha-1 1 /1 Running 0 5m31s codacy-rabbitmq-ha-2 1 /1 Running 0 5m4s codacy-ragnaros-8b899c77f-76bw6 1 /1 Running 2 6m9s codacy-remote-provider-service-7974696f8d-tccvj 1 /1 Running 0 6m9s codacy-remote-provider-service-7974696f8d-wx6zj 1 /1 Running 0 6m9s codacy-worker-manager-77878c655f-7jmjv 1 /1 Running 0 6m8s codacy-worker-manager-77878c655f-8s4bz 1 /1 Running 0 6m9s == > v1/Role NAME AGE codacy-remote-provider-service 12d codacy-worker-manager 12d codacy-worker-manager-worker-permissions 12d == > v1/RoleBinding NAME AGE codacy-remote-provider-service 12d codacy-worker-manager 12d codacy-worker-manager-worker-permissions 12d == > v1/Secret NAME TYPE DATA AGE codacy-activities Opaque 3 12d codacy-activitiesdb Opaque 1 12d codacy-api Opaque 31 12d codacy-core Opaque 6 12d codacy-crow Opaque 5 12d codacy-crowdb Opaque 1 12d codacy-engine Opaque 14 12d codacy-hotspots-api Opaque 4 12d codacy-hotspots-worker Opaque 3 12d codacy-hotspotsdb Opaque 1 12d codacy-listener Opaque 6 12d codacy-listenerdb Opaque 1 12d codacy-minio Opaque 2 12d codacy-portal Opaque 17 12d codacy-postgres Opaque 1 12d codacy-rabbitmq-ha Opaque 6 12d codacy-ragnaros Opaque 11 12d codacy-worker-manager Opaque 17 12d == > v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE codacy-activities ClusterIP 10 .245.117.130 <none> 80 /TCP 12d codacy-activitiesdb ClusterIP 10 .245.86.136 <none> 5432 /TCP 12d codacy-activitiesdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-api LoadBalancer 10 .245.218.1 157 .245.20.169 80 :31652/TCP 12d codacy-core ClusterIP 10 .245.76.203 <none> 80 /TCP 12d codacy-crow ClusterIP 10 .245.149.46 <none> 80 /TCP 12d codacy-crowdb ClusterIP 10 .245.204.138 <none> 5432 /TCP 12d codacy-crowdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-engine ClusterIP 10 .245.196.196 <none> 80 /TCP,9001/TCP 12d codacy-hotspots-api ClusterIP 10 .245.16.156 <none> 80 /TCP 12d codacy-hotspots-worker ClusterIP 10 .245.123.189 <none> 80 /TCP 12d codacy-hotspotsdb ClusterIP 10 .245.208.1 <none> 5432 /TCP 12d codacy-hotspotsdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-listener ClusterIP 10 .245.56.148 <none> 80 /TCP 12d codacy-listenerdb ClusterIP 10 .245.67.58 <none> 5432 /TCP 12d codacy-listenerdb-headless ClusterIP None <none> 5432 /TCP 12d codacy-minio ClusterIP 10 .245.127.197 <none> 9000 /TCP 12d codacy-nfsserverprovisioner ClusterIP 10 .245.159.117 <none> 2049 /TCP,20048/TCP,51413/TCP,51413/UDP 12d codacy-portal ClusterIP 10 .245.104.235 <none> 80 /TCP 12d codacy-postgres ClusterIP 10 .245.20.80 <none> 5432 /TCP 12d codacy-postgres-headless ClusterIP None <none> 5432 /TCP 12d codacy-rabbitmq-ha ClusterIP None <none> 15672 /TCP,5672/TCP,4369/TCP 12d codacy-rabbitmq-ha-discovery ClusterIP None <none> 15672 /TCP,5672/TCP,4369/TCP 12d codacy-ragnaros ClusterIP 10 .245.172.99 <none> 80 /TCP 12d codacy-remote-provider-service ClusterIP 10 .245.247.247 <none> 80 /TCP,5701/TCP 12d codacy-worker-manager ClusterIP 10 .245.211.195 <none> 80 /TCP 12d == > v1/ServiceAccount NAME SECRETS AGE codacy-fluentdoperator 1 12d codacy-minio 1 12d codacy-nfsserverprovisioner 1 12d codacy-rabbitmq-ha 1 12d codacy-remote-provider-service 1 12d codacy-worker-manager 1 12d codacy-worker-manager-worker-permissions 1 12d == > v1/StatefulSet NAME READY AGE codacy-nfsserverprovisioner 1 /1 12d == > v1/StorageClass NAME PROVISIONER AGE codacy-listener-cache-class cluster.local/codacy-nfsserverprovisioner 12d == > v1beta1/ClusterRole NAME AGE codacy-fluentdoperator 12d == > v1beta1/ClusterRoleBinding NAME AGE codacy-fluentdoperator 12d == > v1beta1/CronJob NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE codacy-listener-nfs-cleanup 0 0 * * 6 False 0 4d15h 12d == > v1beta1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE codacy-fluentdoperator 5 5 5 5 5 <none> 12d == > v1beta1/PodDisruptionBudget NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE codacy-activities N/A 1 0 12d codacy-api N/A 1 1 5d22h codacy-core N/A 1 1 12d codacy-engine N/A 1 1 12d codacy-hotspots-api N/A 1 1 12d codacy-hotspots-worker N/A 1 1 12d codacy-listener N/A 1 1 12d codacy-portal N/A 1 1 12d codacy-remote-provider-service N/A 1 1 26h codacy-worker-manager N/A 1 1 5d22h == > v1beta1/Role NAME AGE codacy-rabbitmq-ha 12d == > v1beta1/RoleBinding NAME AGE codacy-rabbitmq-ha 12d == > v1beta1/StatefulSet NAME READY AGE codacy-rabbitmq-ha 3 /3 12d == > v1beta2/StatefulSet NAME READY AGE codacy-activitiesdb 1 /1 12d codacy-crowdb 1 /1 12d codacy-hotspotsdb 1 /1 12d codacy-listenerdb 1 /1 12d codacy-postgres 1 /1 12d","title":"TL;DR - Quickly install Codacy for demo without any persistence."},{"location":"installation/#next-steps-making-it-production-ready","text":"Use the values-production.yaml file as reference Use external DBs (Ideally a cloud managed postgres) Enable persistence on listener Enable persistence on minio Setup resources and limits Enable the Ingress on codacy-api Setup a git provider Start configuring Codacy in the UI Create an administrator account Create an initial organization Invite users Proceed to more advanced configurations .","title":"Next steps - Making it \"Production Ready\""},{"location":"installation/uninstall/","text":"Uninstall \u00b6 To ensure a clean removal you should uninstall Codacy before cleaning destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstall"},{"location":"installation/uninstall/#uninstall","text":"To ensure a clean removal you should uninstall Codacy before cleaning destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstall"},{"location":"installation/upgrade/","text":"Upgrade \u00b6 NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values Steps \u00b6 The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrade"},{"location":"installation/upgrade/#upgrade","text":"NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values","title":"Upgrade"},{"location":"installation/upgrade/#steps","text":"The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Steps"},{"location":"migration/database/","text":"Database Migration Guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Database Migration Guide"},{"location":"migration/database/#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results","title":"Database Migration Guide"},{"location":"migration/database/#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"migration/database/#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"migration/database/#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"migration/database/#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar --clean --if-exists --create NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"migration/database/#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $db -F t -f /tmp/ $db .sql.tar PGPASSWORD = $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $db -F t /tmp/ $db .sql.tar --clean --if-exists --create done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Sample script"},{"location":"quickstart/aks_quickstart/","text":"AKS cluster quickstart \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"AKS cluster quickstart"},{"location":"quickstart/aks_quickstart/#aks-cluster-quickstart","text":"","title":"AKS cluster quickstart"},{"location":"quickstart/aks_quickstart/#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"quickstart/aks_quickstart/#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"quickstart/aks_quickstart/#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"quickstart/aks_quickstart/#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"quickstart/eks_quickstart/","text":"EKS cluster setup using terraform \u00b6 This folder includes the terraform templates needed to create an EKS cluster from scratch, including all the necessary underlying infrastructure. It includes the following infrastructure stacks: backend - (optional) the S3 bucket for storing the terraform state and the DynamoDB table for state locking. main - the EKS cluster, including all the network and nodes setup needed to go from zero to a fully functional EKS cluster. setup - additional setup you must perform before installing Codacy on your vanilla EKS cluster. Clone the project and go to that directory: $ git clone git@github.com:codacy/chart.git $ cd chart/docs/quickstart/EKS/ Requirements \u00b6 In order to setup the infrastructure you'll need recent versions of: awscli terraform kubectl helm Please follow the documentation in the above links to setup these tools for your OS. They can usually be easily installed using your package manager. For instance, on macOS, if you are using Homebrew , you can just run: brew install awscli terraform kubernetes-helm kubectl You'll also need to setup the CLI credentials for your AWS account. See how to do it the AWS and Terraform documentation. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly. TL;DR \u00b6 Setting up an EKS cluster for Codacy \u00b6 Assuming AWS is well configured and you fulfill all the prerequisites, the setup without state storage, can be done with: $ export AWS_SDK_LOAD_CONFIG = 1 $ cd main/ $ terraform init && terraform apply $ cd ../setup/ $ terraform init && terraform apply $ aws eks update-kubeconfig --name codacy-cluster Deployment - Long version \u00b6 1. backend - setup terraform state storage \u00b6 The backend stores the current and historical state of your infrastructure. Deploying and using it is optional, but it might make your life easier if you are planning to use these templates to make modifications to the cluster in the future. To deploy it run: terraform init && terraform apply inside the backend/ folder and follow terraform's instructions. An S3 bucket with an unique name to save your state will be created. Note this bucket's name and set it on the config.tf file of the main/ and setup/ stacks where indicated. 2. main - create a vanilla EKS cluster \u00b6 To create a cluster, along with all the necessary network and nodes setup it requires, run: terraform init && terraform apply inside the main/ folder and follow terraform's instructions. This takes a while (~10min). The cluster configuration (e.g., type/number of nodes, network CIDRs,...) are exposed as variables in the variables.tf file. You may tailor the cluster to your needs by editing the defaults that file, by using CLI options, viz. terraform apply -var = \"some_key=some_value\" or by writing them on a file named terraform.tfvars in the same folder as the infrastructure code, which is loaded by default when you run apply. For instance: some_key = \"a_string_value\" another_key = 3 someting_else = true To connect to the cluster get its kubeconfig and set it the as default context with: aws eks update-kubeconfig --name codacy-cluster If you now run kubectl get pods -A you'll see that nothing is scheduled. That's because you haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more info). We'll do that on step 3. 3. setup - configure your EKS cluster to deploy codacy \u00b6 Some additional setup is necessary to run Codacy on the cluster you just created. For one, you need to allow workers to join the cluster. Docker pull secrets need to be added to the codacy namespace, which will also be added here, and the following helm charts will be installed: kubernetes-dashboard , nginx-ingress , and cert-manager . To do it run terraform init && terraform apply You'll be prompted to input Codacy's docker hub repo password. If you'd like to connect to the kubernetes dashboard, get the admin token by running terraform output admin_token and copy the outputed value. To connect to the dashboard run kubectl proxy and then connect to the dashboard url , select token and paste the value you saved above. 4. Installing Codacy \u00b6 To install Codacy please see the installation documentation . Uninstalling \u00b6 WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS 1. Remove the cluster setup required to install Codacy \u00b6 To cleanup your cluster back to a vanilla state you can now run, the following command in the setup/ folder: terraform destroy 2. Remove the cluster \u00b6 After removing all the above stacks and setup, you may now delete the kubernetes cluster by running in the main/ directory: terraform destroy This takes a while (~10min). 3. Removing the terraform backend \u00b6 If you created the terraform backend with the above stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly the easiest path is to disable these extra settings. Go to the backend/ folder and change the state_and_lock.tf file as instructed therein. Afterwards, you can now destroy it by running terraform apply && terraform destroy Note that you first have to apply to change the bucket settings, and only then will destroy work.","title":"EKS cluster setup using terraform"},{"location":"quickstart/eks_quickstart/#eks-cluster-setup-using-terraform","text":"This folder includes the terraform templates needed to create an EKS cluster from scratch, including all the necessary underlying infrastructure. It includes the following infrastructure stacks: backend - (optional) the S3 bucket for storing the terraform state and the DynamoDB table for state locking. main - the EKS cluster, including all the network and nodes setup needed to go from zero to a fully functional EKS cluster. setup - additional setup you must perform before installing Codacy on your vanilla EKS cluster. Clone the project and go to that directory: $ git clone git@github.com:codacy/chart.git $ cd chart/docs/quickstart/EKS/","title":"EKS cluster setup using terraform"},{"location":"quickstart/eks_quickstart/#requirements","text":"In order to setup the infrastructure you'll need recent versions of: awscli terraform kubectl helm Please follow the documentation in the above links to setup these tools for your OS. They can usually be easily installed using your package manager. For instance, on macOS, if you are using Homebrew , you can just run: brew install awscli terraform kubernetes-helm kubectl You'll also need to setup the CLI credentials for your AWS account. See how to do it the AWS and Terraform documentation. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly.","title":"Requirements"},{"location":"quickstart/eks_quickstart/#tldr","text":"","title":"TL;DR"},{"location":"quickstart/eks_quickstart/#setting-up-an-eks-cluster-for-codacy","text":"Assuming AWS is well configured and you fulfill all the prerequisites, the setup without state storage, can be done with: $ export AWS_SDK_LOAD_CONFIG = 1 $ cd main/ $ terraform init && terraform apply $ cd ../setup/ $ terraform init && terraform apply $ aws eks update-kubeconfig --name codacy-cluster","title":"Setting up an EKS cluster for Codacy"},{"location":"quickstart/eks_quickstart/#deployment-long-version","text":"","title":"Deployment - Long version"},{"location":"quickstart/eks_quickstart/#1-backend-setup-terraform-state-storage","text":"The backend stores the current and historical state of your infrastructure. Deploying and using it is optional, but it might make your life easier if you are planning to use these templates to make modifications to the cluster in the future. To deploy it run: terraform init && terraform apply inside the backend/ folder and follow terraform's instructions. An S3 bucket with an unique name to save your state will be created. Note this bucket's name and set it on the config.tf file of the main/ and setup/ stacks where indicated.","title":"1. backend - setup terraform state storage"},{"location":"quickstart/eks_quickstart/#2-main-create-a-vanilla-eks-cluster","text":"To create a cluster, along with all the necessary network and nodes setup it requires, run: terraform init && terraform apply inside the main/ folder and follow terraform's instructions. This takes a while (~10min). The cluster configuration (e.g., type/number of nodes, network CIDRs,...) are exposed as variables in the variables.tf file. You may tailor the cluster to your needs by editing the defaults that file, by using CLI options, viz. terraform apply -var = \"some_key=some_value\" or by writing them on a file named terraform.tfvars in the same folder as the infrastructure code, which is loaded by default when you run apply. For instance: some_key = \"a_string_value\" another_key = 3 someting_else = true To connect to the cluster get its kubeconfig and set it the as default context with: aws eks update-kubeconfig --name codacy-cluster If you now run kubectl get pods -A you'll see that nothing is scheduled. That's because you haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more info). We'll do that on step 3.","title":"2. main - create a vanilla EKS cluster"},{"location":"quickstart/eks_quickstart/#3-setup-configure-your-eks-cluster-to-deploy-codacy","text":"Some additional setup is necessary to run Codacy on the cluster you just created. For one, you need to allow workers to join the cluster. Docker pull secrets need to be added to the codacy namespace, which will also be added here, and the following helm charts will be installed: kubernetes-dashboard , nginx-ingress , and cert-manager . To do it run terraform init && terraform apply You'll be prompted to input Codacy's docker hub repo password. If you'd like to connect to the kubernetes dashboard, get the admin token by running terraform output admin_token and copy the outputed value. To connect to the dashboard run kubectl proxy and then connect to the dashboard url , select token and paste the value you saved above.","title":"3. setup - configure your EKS cluster to deploy codacy"},{"location":"quickstart/eks_quickstart/#4-installing-codacy","text":"To install Codacy please see the installation documentation .","title":"4. Installing Codacy"},{"location":"quickstart/eks_quickstart/#uninstalling","text":"WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS","title":"Uninstalling"},{"location":"quickstart/eks_quickstart/#1-remove-the-cluster-setup-required-to-install-codacy","text":"To cleanup your cluster back to a vanilla state you can now run, the following command in the setup/ folder: terraform destroy","title":"1.  Remove the cluster setup required to install Codacy"},{"location":"quickstart/eks_quickstart/#2-remove-the-cluster","text":"After removing all the above stacks and setup, you may now delete the kubernetes cluster by running in the main/ directory: terraform destroy This takes a while (~10min).","title":"2.  Remove the cluster"},{"location":"quickstart/eks_quickstart/#3-removing-the-terraform-backend","text":"If you created the terraform backend with the above stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly the easiest path is to disable these extra settings. Go to the backend/ folder and change the state_and_lock.tf file as instructed therein. Afterwards, you can now destroy it by running terraform apply && terraform destroy Note that you first have to apply to change the bucket settings, and only then will destroy work.","title":"3.  Removing the terraform backend"},{"location":"release/release/","text":"Preparing the release \u00b6 Checkout the releases branch. Freeze the versions of each component on the requirements.yaml file to this branch. The following sample script will output all the latest versions of the codacy owned components from the STABLE charts museum: #!/bin/bash DEPENDENCIES = $( yq r codacy/requirements.yaml dependencies -j | jq \".[].name\" | sed \"s/\\\"//g\" | grep -v \"minio\\|rabbitmq-ha\\|postgresql\\|log-router\" ) for key in $DEPENDENCIES do version = $( curl -s https://charts.codacy.com/stable/api/charts/ $key | jq . [ 0 ] .version | sed \"s/\\\"//g\" ) printf \"%30s %10s\\n\" $key $version done Which should produce some pretty output like: portal 4.32.3 ragnaros 15.2.2 activities 1.3.0 remote-provider-service 2.44.0 hotspots-api 1.4.1 hotspots-worker 1.4.1 listener 7.11.0 core 3.0.0 engine 5.58.0 codacy-api 4.128.1 worker-manager 8.13.0 crow 4.4.0 Please note you must have jq and yq installed. On MacOS: brew install jq yq Otherwise, you can find instructions here: * https://stedolan.github.io/jq/download/ * https://github.com/mikefarah/yq Run helm dep up . Commit both requirements.yaml and requirements.lock to the branch. The commit message must be \" release : start release [ DD - MM - YYYY ] \". Tag the commit with release -[ DD-MM-YYYY ] . [ buildversion ] and push. This tag will automatically trigger a release candidate build. You chart will be deployed to k8s.release.dev.codacy.org ( codacy-doks-cluster-release cluster in digital ocean). Follow the release workflow of the chart project on circleci. During the release \u00b6 If there are things that need to be fixed: 1. After the components have been fixed, update the versions on the requirements.yaml file. 2. Run helm dep up . 3. Commit both requirements.yaml and requirements.lock to the branch. The commit message shoul contain something such as \" bump : updated [ component ] version \". 4. Repeat steps 5. to 7. from the section above. After the release \u00b6 After the release is complete: 1. There will be a x.y.z tag if the release has been successful. 2. We adopt a long-lived releases branch approach so there is nothing you need to do in particular. 1. Optionally, you can delete the release -[ DD-MM-YYYY ] . [ buildversion ] you have created. Something went wrong \u00b6 If you would like to abandon the current release: 1. Delete the release -[ DD-MM-YYYY ] . [ buildversion ] tag that you have pushed. 2. Run helm rollback codacy-release 0 on the codacy-doks-cluster-release . This will roll back the release to its previous successful deployment.","title":"Preparing the release"},{"location":"release/release/#preparing-the-release","text":"Checkout the releases branch. Freeze the versions of each component on the requirements.yaml file to this branch. The following sample script will output all the latest versions of the codacy owned components from the STABLE charts museum: #!/bin/bash DEPENDENCIES = $( yq r codacy/requirements.yaml dependencies -j | jq \".[].name\" | sed \"s/\\\"//g\" | grep -v \"minio\\|rabbitmq-ha\\|postgresql\\|log-router\" ) for key in $DEPENDENCIES do version = $( curl -s https://charts.codacy.com/stable/api/charts/ $key | jq . [ 0 ] .version | sed \"s/\\\"//g\" ) printf \"%30s %10s\\n\" $key $version done Which should produce some pretty output like: portal 4.32.3 ragnaros 15.2.2 activities 1.3.0 remote-provider-service 2.44.0 hotspots-api 1.4.1 hotspots-worker 1.4.1 listener 7.11.0 core 3.0.0 engine 5.58.0 codacy-api 4.128.1 worker-manager 8.13.0 crow 4.4.0 Please note you must have jq and yq installed. On MacOS: brew install jq yq Otherwise, you can find instructions here: * https://stedolan.github.io/jq/download/ * https://github.com/mikefarah/yq Run helm dep up . Commit both requirements.yaml and requirements.lock to the branch. The commit message must be \" release : start release [ DD - MM - YYYY ] \". Tag the commit with release -[ DD-MM-YYYY ] . [ buildversion ] and push. This tag will automatically trigger a release candidate build. You chart will be deployed to k8s.release.dev.codacy.org ( codacy-doks-cluster-release cluster in digital ocean). Follow the release workflow of the chart project on circleci.","title":"Preparing the release"},{"location":"release/release/#during-the-release","text":"If there are things that need to be fixed: 1. After the components have been fixed, update the versions on the requirements.yaml file. 2. Run helm dep up . 3. Commit both requirements.yaml and requirements.lock to the branch. The commit message shoul contain something such as \" bump : updated [ component ] version \". 4. Repeat steps 5. to 7. from the section above.","title":"During the release"},{"location":"release/release/#after-the-release","text":"After the release is complete: 1. There will be a x.y.z tag if the release has been successful. 2. We adopt a long-lived releases branch approach so there is nothing you need to do in particular. 1. Optionally, you can delete the release -[ DD-MM-YYYY ] . [ buildversion ] you have created.","title":"After the release"},{"location":"release/release/#something-went-wrong","text":"If you would like to abandon the current release: 1. Delete the release -[ DD-MM-YYYY ] . [ buildversion ] tag that you have pushed. 2. Run helm rollback codacy-release 0 on the codacy-doks-cluster-release . This will roll back the release to its previous successful deployment.","title":"Something went wrong"},{"location":"requirements/","text":"Requirements \u00b6 If you want to deploy Codacy on Kubernetes, the following requirements must be met: kubectl 1.13 or higher, compatible with your cluster ( +/- 1 minor release from your cluster ). Helm 2.14> <3.0 A Kubernetes cluster, between version 1.13 and 1.15. 16vCPU and 64GB of RAM is the recommended minimum. Postgres database ( https://support.codacy.com/hc/en-us/articles/360002902573-Installing-postgres-for-Codacy-Enterprise ) Resources \u00b6 To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 39 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production ready\" version of Codacy. Analysis \u00b6 Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.genericMax + workers.dedicatedMax Total resources \u00b6 Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis. Example \u00b6 Maximum of 6 concurrent analysis worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 39 GB + 36 GB = 75 GB","title":"Requirements"},{"location":"requirements/#requirements","text":"If you want to deploy Codacy on Kubernetes, the following requirements must be met: kubectl 1.13 or higher, compatible with your cluster ( +/- 1 minor release from your cluster ). Helm 2.14> <3.0 A Kubernetes cluster, between version 1.13 and 1.15. 16vCPU and 64GB of RAM is the recommended minimum. Postgres database ( https://support.codacy.com/hc/en-us/articles/360002902573-Installing-postgres-for-Codacy-Enterprise )","title":"Requirements"},{"location":"requirements/#resources","text":"To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 39 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production ready\" version of Codacy.","title":"Resources"},{"location":"requirements/#analysis","text":"Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.genericMax + workers.dedicatedMax","title":"Analysis"},{"location":"requirements/#total-resources","text":"Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis.","title":"Total resources"},{"location":"requirements/#example","text":"Maximum of 6 concurrent analysis worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 39 GB + 36 GB = 75 GB","title":"Example"}]}