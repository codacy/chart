{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Codacy Chart \u00b6 With the Codacy chart it is possible to run all Codacy components and its dependencies with a single command line. edit image Each service in Codacy has its chart published to our charts repository . This chart bundles all the components and their dependencies. For the bundle, we make use of the requirements capability of Helm. Work in progress \u00b6 This chart is still a Work In Progress and is not ready for general usage. Our docker images are currently private and you will not be able to run the chart by yourself. If you are interested in trying out Codacy contact our support at support@codacy.com. Charts \u00b6 Documentation on a per-chart basis is listed here. Some of these repositories are private and accessible to Codacy engineers only. edit image Minio RabbitMQ-HA Postgres kube-fluentd-operator Codacy/ Token Management Codacy/ Website Codacy/ API Codacy/ Ragnaros Codacy/ Activities Codacy/ Repository Listener Codacy/ Portal Codacy/ Worker Manager Codacy/ Engine Codacy/ Core Codacy/ Hotspots API Codacy/ Hotspots Worker Configuration \u00b6 The following table lists the configurable parameters of the Codacy chart and their default values. Global parameters apply to all sub-charts and make it easier to configure resources across different components. Parameter Description Default global.codacy.url Hostname to your Codacy installation nil global.codacy.backendUrl Hostname to your Codacy installation nil global.play.cryptoSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.filestore.contentsSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.filestore.uuidSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.cacheSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.minio.create Create minio internally nil global.rabbitmq.create Create rabbitmq internally nil global.rabbitmq.rabbitmqUsername Username for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqUsername also. nil global.rabbitmq.rabbitmqPassword Password for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqPassword also. nil global.defaultdb.postgresqlUsername Username of the Postgresql server codacy global.defaultdb.postgresqlDatabase Database name of the Postgresql server default global.defaultdb.postgresqlPassword Hostname of the Postgresql server nil global.defaultdb.host Hostname of the Postgresql server nil global.defaultdb.service.port Port of the Postgresql server 5432 global.analysisdb.postgresqlUsername Username of the Postgresql server codacy global.analysisdb.postgresqlDatabase Database name of the Postgresql server analysis global.analysisdb.postgresqlPassword Hostname of the Postgresql server nil global.analysisdb.host Hostname of the Postgresql server nil global.analysisdb.service.port Port of the Postgresql server 5432 global.resultsdb.postgresqlUsername Username of the Postgresql server codacy global.resultsdb.postgresqlDatabase Database name of the Postgresql server results global.resultsdb.postgresqlPassword Hostname of the Postgresql server nil global.resultsdb.host Hostname of the Postgresql server nil global.resultsdb.service.port Port of the Postgresql server 5432 global.resultsdb201709.postgresqlUsername Username of the Postgresql server codacy global.resultsdb201709.postgresqlDatabase Database name of the Postgresql server results201709 global.resultsdb201709.postgresqlPassword Hostname of the Postgresql server nil global.resultsdb201709.host Hostname of the Postgresql server nil global.resultsdb201709.service.port Port of the Postgresql server 5432 global.metricsdb.postgresqlUsername Username of the Postgresql server codacy global.metricsdb.postgresqlDatabase Database name of the Postgresql server metrics global.metricsdb.postgresqlPassword Hostname of the Postgresql server nil global.metricsdb.host Hostname of the Postgresql server nil global.metricsdb.service.port Port of the Postgresql server 5432 global.filestoredb.postgresqlUsername Username of the Postgresql server codacy global.filestoredb.postgresqlDatabase Database name of the Postgresql server filestore global.filestoredb.postgresqlPassword Hostname of the Postgresql server nil global.filestoredb.host Hostname of the Postgresql server nil global.filestoredb.service.port Port of the Postgresql server 5432 global.jobsdb.postgresqlUsername Username of the Postgresql server codacy global.jobsdb.postgresqlDatabase Database name of the Postgresql server jobs global.jobsdb.postgresqlPassword Hostname of the Postgresql server nil global.jobsdb.host Hostname of the Postgresql server nil global.jobsdb.service.port Port of the Postgresql server 5432 global.githubEnterprise.hostname Hostname of githubEnterprise instance nil global.githubEnterprise.protocol Protocol of githubEnterprise instance nil global.githubEnterprise.port Port of githubEnterprise instance nil global.githubEnterprise.isPrivateMode Status of private mode on githubEnterprise instance nil global.githubEnterprise.disableSSL Disable certificate validation on interaction with githubEnterprise instance nil global.gitlabEnterprise.hostname Hostname of gitlabEnterprise instance nil global.gitlabEnterprise.protocol Protocol of gitlabEnterprise instance nil global.bitbucketEnterprise.hostname Hostname of bitbucketEnterprise instance nil global.bitbucketEnterprise.protocol Protocol of bitbucketEnterprise instance nil The following parameters are specific to each Codacy component. Parameter Description Default portal.replicaCount Number of replicas 1 portal.image.repository Image repository from dependency portal.image.tag Image tag from dependency portal.service.type Portal service type ClusterIP portal.service.annotations Annotations to be added to the Portal service {} remote-provider-service.replicaCount Number of replicas 1 remote-provider-service.image.repository Image repository from dependency remote-provider-service.image.tag Image tag from dependency remote-provider-service.service.type Remote Provider service type ClusterIP remote-provider-service.service.annotations Annotations to be added to the Remote Provider service {} activities.replicaCount Number of replicas 1 activities.image.repository Image repository from dependency activities.image.tag Image tag from dependency activities.service.type Service type ClusterIP activities.service.annotations Annotations to be added to the service {} activities.activitiesdb.postgresqlUsername Username of the Postgresql server codacy activities.activitiesdb.postgresqlDatabase Database name of the Postgresql server jobs activities.activitiesdb.postgresqlPassword Hostname of the Postgresql server nil activities.activitiesdb.host Hostname of the Postgresql server nil activities.activitiesdb.service.port Port of the Postgresql server 5432 hotspots-api.replicaCount Number of replicas 1 hotspots-api.image.repository Image repository from dependency hotspots-api.image.tag Image tag from dependency hotspots-api.service.type Service type ClusterIP hotspots-api.service.annotations Annotations to be added to the service {} hotspots-api.hotspotsdb.postgresqlUsername Username of the Postgresql server codacy hotspots-api.hotspotsdb.postgresqlDatabase Database name of the Postgresql server hotspots hotspots-api.hotspotsdb.postgresqlPassword Hostname of the Postgresql server nil hotspots-api.hotspotsdb.host Hostname of the Postgresql server nil hotspots-api.hotspotsdb.service.port Port of the Postgresql server 5432 hotspots-worker.replicaCount Number of replicas 1 hotspots-worker.image.repository Image repository from dependency hotspots-worker.image.tag Image tag from dependency hotspots-worker.service.type Service type ClusterIP hotspots-worker.service.annotations Annotations to be added to the service {} listener.replicaCount Number of replicas 1 listener.image.repository Image repository from dependency listener.image.tag Image tag from dependency listener.service.type Service type ClusterIP listener.service.annotations Annotations to be added to the service {} listener.persistence.claim.size Each pod mounts and NFS disk and claims this size. 100Gi listener.nfsserverprovisioner.enabled Creates an NFS server and a storage class to mount volumes in that server. true listener.nfsserverprovisioner.persistence.enabled Creates an NFS provisioner true listener.nfsserverprovisioner.persistence.size Size of the NFS server disk 120Gi listener.listenerdb.postgresqlUsername Username of the Postgresql server codacy listener.listenerdb.postgresqlDatabase Database name of the Postgresql server listener listener.listenerdb.postgresqlPassword Hostname of the Postgresql server PLEASE_CHANGE_ME listener.listenerdb.host Hostname of the Postgresql server codacy-listenerdb.codacy.svc.cluster.local listener.listenerdb.service.port Port of the Postgresql server 5432 core.replicaCount Number of replicas 1 core.image.repository Image repository from dependency core.image.tag Image tag from dependency core.service.type Service type ClusterIP engine.replicaCount Number of replicas 1 engine.image.repository Image repository from dependency engine.image.tag Image tag from dependency engine.service.type Service type ClusterIP engine.service.annotations Annotations to be added to the service {} engine.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.image.repository Image repository from dependency codacy-api.image.tag Image tag from dependency codacy-api.service.type Service type ClusterIP codacy-api.config.license Codacy license for your installation nil codacy-api.service.annotations Annotations to be added to the service {} codacy-api.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.metrics.grafana_dashboards.enabled Create the ConfigMap with the dashboard of this component. Can be imported through grafana sidecar. false worker-manager.config.workers.genericMax TBD 100 worker-manager.config.workers.dedicatedMax TBD 100 crow.replicaCount Number of replicas 1 crow.image.repository Image repository from dependency crow.image.tag Image tag from dependency The following parameters refer to components that are not internal to Codacy, but go as part of this bundle so that you can bootstrap Codacy faster. Parameter Description Default fluentdoperator.enable Enable fluentd operator. It gathers logs from Codacy so that you can send it to our support if needed. false fluentdoperator.expirationDays Number of days to retain logs. More time uses more disk on minio. 14 rabbitmq-ha.rabbitmqUsername Username for the bundled RabbitMQ. rabbitmq rabbitmq-ha.rabbitmqPassword Password for the bundled RabbitMQ. rabbitmq You can also configure values for the PostgreSQL database via the Postgresql README.md For overriding variables see: Customizing the chart","title":"Codacy Chart"},{"location":"#codacy-chart","text":"With the Codacy chart it is possible to run all Codacy components and its dependencies with a single command line. edit image Each service in Codacy has its chart published to our charts repository . This chart bundles all the components and their dependencies. For the bundle, we make use of the requirements capability of Helm.","title":"Codacy Chart"},{"location":"#work-in-progress","text":"This chart is still a Work In Progress and is not ready for general usage. Our docker images are currently private and you will not be able to run the chart by yourself. If you are interested in trying out Codacy contact our support at support@codacy.com.","title":"Work in progress"},{"location":"#charts","text":"Documentation on a per-chart basis is listed here. Some of these repositories are private and accessible to Codacy engineers only. edit image Minio RabbitMQ-HA Postgres kube-fluentd-operator Codacy/ Token Management Codacy/ Website Codacy/ API Codacy/ Ragnaros Codacy/ Activities Codacy/ Repository Listener Codacy/ Portal Codacy/ Worker Manager Codacy/ Engine Codacy/ Core Codacy/ Hotspots API Codacy/ Hotspots Worker","title":"Charts"},{"location":"#configuration","text":"The following table lists the configurable parameters of the Codacy chart and their default values. Global parameters apply to all sub-charts and make it easier to configure resources across different components. Parameter Description Default global.codacy.url Hostname to your Codacy installation nil global.codacy.backendUrl Hostname to your Codacy installation nil global.play.cryptoSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.filestore.contentsSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.filestore.uuidSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.cacheSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 nil global.minio.create Create minio internally nil global.rabbitmq.create Create rabbitmq internally nil global.rabbitmq.rabbitmqUsername Username for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqUsername also. nil global.rabbitmq.rabbitmqPassword Password for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqPassword also. nil global.defaultdb.postgresqlUsername Username of the Postgresql server codacy global.defaultdb.postgresqlDatabase Database name of the Postgresql server default global.defaultdb.postgresqlPassword Hostname of the Postgresql server nil global.defaultdb.host Hostname of the Postgresql server nil global.defaultdb.service.port Port of the Postgresql server 5432 global.analysisdb.postgresqlUsername Username of the Postgresql server codacy global.analysisdb.postgresqlDatabase Database name of the Postgresql server analysis global.analysisdb.postgresqlPassword Hostname of the Postgresql server nil global.analysisdb.host Hostname of the Postgresql server nil global.analysisdb.service.port Port of the Postgresql server 5432 global.resultsdb.postgresqlUsername Username of the Postgresql server codacy global.resultsdb.postgresqlDatabase Database name of the Postgresql server results global.resultsdb.postgresqlPassword Hostname of the Postgresql server nil global.resultsdb.host Hostname of the Postgresql server nil global.resultsdb.service.port Port of the Postgresql server 5432 global.resultsdb201709.postgresqlUsername Username of the Postgresql server codacy global.resultsdb201709.postgresqlDatabase Database name of the Postgresql server results201709 global.resultsdb201709.postgresqlPassword Hostname of the Postgresql server nil global.resultsdb201709.host Hostname of the Postgresql server nil global.resultsdb201709.service.port Port of the Postgresql server 5432 global.metricsdb.postgresqlUsername Username of the Postgresql server codacy global.metricsdb.postgresqlDatabase Database name of the Postgresql server metrics global.metricsdb.postgresqlPassword Hostname of the Postgresql server nil global.metricsdb.host Hostname of the Postgresql server nil global.metricsdb.service.port Port of the Postgresql server 5432 global.filestoredb.postgresqlUsername Username of the Postgresql server codacy global.filestoredb.postgresqlDatabase Database name of the Postgresql server filestore global.filestoredb.postgresqlPassword Hostname of the Postgresql server nil global.filestoredb.host Hostname of the Postgresql server nil global.filestoredb.service.port Port of the Postgresql server 5432 global.jobsdb.postgresqlUsername Username of the Postgresql server codacy global.jobsdb.postgresqlDatabase Database name of the Postgresql server jobs global.jobsdb.postgresqlPassword Hostname of the Postgresql server nil global.jobsdb.host Hostname of the Postgresql server nil global.jobsdb.service.port Port of the Postgresql server 5432 global.githubEnterprise.hostname Hostname of githubEnterprise instance nil global.githubEnterprise.protocol Protocol of githubEnterprise instance nil global.githubEnterprise.port Port of githubEnterprise instance nil global.githubEnterprise.isPrivateMode Status of private mode on githubEnterprise instance nil global.githubEnterprise.disableSSL Disable certificate validation on interaction with githubEnterprise instance nil global.gitlabEnterprise.hostname Hostname of gitlabEnterprise instance nil global.gitlabEnterprise.protocol Protocol of gitlabEnterprise instance nil global.bitbucketEnterprise.hostname Hostname of bitbucketEnterprise instance nil global.bitbucketEnterprise.protocol Protocol of bitbucketEnterprise instance nil The following parameters are specific to each Codacy component. Parameter Description Default portal.replicaCount Number of replicas 1 portal.image.repository Image repository from dependency portal.image.tag Image tag from dependency portal.service.type Portal service type ClusterIP portal.service.annotations Annotations to be added to the Portal service {} remote-provider-service.replicaCount Number of replicas 1 remote-provider-service.image.repository Image repository from dependency remote-provider-service.image.tag Image tag from dependency remote-provider-service.service.type Remote Provider service type ClusterIP remote-provider-service.service.annotations Annotations to be added to the Remote Provider service {} activities.replicaCount Number of replicas 1 activities.image.repository Image repository from dependency activities.image.tag Image tag from dependency activities.service.type Service type ClusterIP activities.service.annotations Annotations to be added to the service {} activities.activitiesdb.postgresqlUsername Username of the Postgresql server codacy activities.activitiesdb.postgresqlDatabase Database name of the Postgresql server jobs activities.activitiesdb.postgresqlPassword Hostname of the Postgresql server nil activities.activitiesdb.host Hostname of the Postgresql server nil activities.activitiesdb.service.port Port of the Postgresql server 5432 hotspots-api.replicaCount Number of replicas 1 hotspots-api.image.repository Image repository from dependency hotspots-api.image.tag Image tag from dependency hotspots-api.service.type Service type ClusterIP hotspots-api.service.annotations Annotations to be added to the service {} hotspots-api.hotspotsdb.postgresqlUsername Username of the Postgresql server codacy hotspots-api.hotspotsdb.postgresqlDatabase Database name of the Postgresql server hotspots hotspots-api.hotspotsdb.postgresqlPassword Hostname of the Postgresql server nil hotspots-api.hotspotsdb.host Hostname of the Postgresql server nil hotspots-api.hotspotsdb.service.port Port of the Postgresql server 5432 hotspots-worker.replicaCount Number of replicas 1 hotspots-worker.image.repository Image repository from dependency hotspots-worker.image.tag Image tag from dependency hotspots-worker.service.type Service type ClusterIP hotspots-worker.service.annotations Annotations to be added to the service {} listener.replicaCount Number of replicas 1 listener.image.repository Image repository from dependency listener.image.tag Image tag from dependency listener.service.type Service type ClusterIP listener.service.annotations Annotations to be added to the service {} listener.persistence.claim.size Each pod mounts and NFS disk and claims this size. 100Gi listener.nfsserverprovisioner.enabled Creates an NFS server and a storage class to mount volumes in that server. true listener.nfsserverprovisioner.persistence.enabled Creates an NFS provisioner true listener.nfsserverprovisioner.persistence.size Size of the NFS server disk 120Gi listener.listenerdb.postgresqlUsername Username of the Postgresql server codacy listener.listenerdb.postgresqlDatabase Database name of the Postgresql server listener listener.listenerdb.postgresqlPassword Hostname of the Postgresql server PLEASE_CHANGE_ME listener.listenerdb.host Hostname of the Postgresql server codacy-listenerdb.codacy.svc.cluster.local listener.listenerdb.service.port Port of the Postgresql server 5432 core.replicaCount Number of replicas 1 core.image.repository Image repository from dependency core.image.tag Image tag from dependency core.service.type Service type ClusterIP engine.replicaCount Number of replicas 1 engine.image.repository Image repository from dependency engine.image.tag Image tag from dependency engine.service.type Service type ClusterIP engine.service.annotations Annotations to be added to the service {} engine.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.image.repository Image repository from dependency codacy-api.image.tag Image tag from dependency codacy-api.service.type Service type ClusterIP codacy-api.config.license Codacy license for your installation nil codacy-api.service.annotations Annotations to be added to the service {} codacy-api.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.metrics.grafana_dashboards.enabled Create the ConfigMap with the dashboard of this component. Can be imported through grafana sidecar. false worker-manager.config.workers.genericMax TBD 100 worker-manager.config.workers.dedicatedMax TBD 100 crow.replicaCount Number of replicas 1 crow.image.repository Image repository from dependency crow.image.tag Image tag from dependency The following parameters refer to components that are not internal to Codacy, but go as part of this bundle so that you can bootstrap Codacy faster. Parameter Description Default fluentdoperator.enable Enable fluentd operator. It gathers logs from Codacy so that you can send it to our support if needed. false fluentdoperator.expirationDays Number of days to retain logs. More time uses more disk on minio. 14 rabbitmq-ha.rabbitmqUsername Username for the bundled RabbitMQ. rabbitmq rabbitmq-ha.rabbitmqPassword Password for the bundled RabbitMQ. rabbitmq You can also configure values for the PostgreSQL database via the Postgresql README.md For overriding variables see: Customizing the chart","title":"Configuration"},{"location":"configuration/","text":"Configuration \u00b6 Logging Monitoring","title":"Configuration"},{"location":"configuration/#configuration","text":"Logging Monitoring","title":"Configuration"},{"location":"configuration/logging/","text":"Logging \u00b6 To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Logging"},{"location":"configuration/logging/#logging","text":"To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which Codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Logging"},{"location":"configuration/monitoring/","text":"Monitoring \u00b6 In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana. Setting up monitoring using Grafana & Prometheus \u00b6 The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Setting up monitoring using Crow [deprecated] \u00b6 Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed along side Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Monitoring"},{"location":"configuration/monitoring/#monitoring","text":"In older versions, Codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana.","title":"Monitoring"},{"location":"configuration/monitoring/#setting-up-monitoring-using-grafana-prometheus","text":"The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true","title":"Setting up monitoring using Grafana &amp; Prometheus"},{"location":"configuration/monitoring/#setting-up-monitoring-using-crow-deprecated","text":"Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed along side Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Setting up monitoring using Crow [deprecated]"},{"location":"configuration/providers/bitbucket-enterprise/","text":"Bitbucket Stash \u00b6 Set the following configurations from your Bitbucket instance on the values.yaml file: global : bitbucketEnterprise : hostname : \"CHANGE_ME\" protocol : \"CHANGE_ME\" Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of. Stash Application Link \u00b6 To set up Stash you need to create an application link on your Stash installation. You can click on here and go to the application links list. Application Link Creation \u00b6 Create the link, use your Codacy installation URL for this Name the link \u00b6 Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection. Add incoming connection \u00b6 Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Bitbucket Stash"},{"location":"configuration/providers/bitbucket-enterprise/#bitbucket-stash","text":"Set the following configurations from your Bitbucket instance on the values.yaml file: global : bitbucketEnterprise : hostname : \"CHANGE_ME\" protocol : \"CHANGE_ME\" Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. Go to admin/integration on Codacy and set the Project Keys on the Bitbucket Server integration, these should be the keys of the projects you would like to retrieve repositories of.","title":"Bitbucket Stash"},{"location":"configuration/providers/bitbucket-enterprise/#stash-application-link","text":"To set up Stash you need to create an application link on your Stash installation. You can click on here and go to the application links list.","title":"Stash Application Link"},{"location":"configuration/providers/bitbucket-enterprise/#application-link-creation","text":"Create the link, use your Codacy installation URL for this","title":"Application Link Creation"},{"location":"configuration/providers/bitbucket-enterprise/#name-the-link","text":"Application Name : You can name the application (ex: Codacy) Application Type : The application type is Generic Application The rest of the configuration should be left blank. After the link is created, click edit to add an incoming connection.","title":"Name the link"},{"location":"configuration/providers/bitbucket-enterprise/#add-incoming-connection","text":"Consumer Key : This value should be copied from the \"Client ID\" field in the Codacy setup page. Consumer Name : You can choose any name (ex: Codacy). Public Key : This value should be copied from the \"Client Secret\" field on the Codacy setup page. The rest of the fields can be left blank. After the application link is created, you will be able to add Bitbucket Server as an integration in the repository settings.","title":"Add incoming connection"},{"location":"configuration/providers/github-cloud/","text":"GitHub Cloud \u00b6 GitHub Application \u00b6 To integrate with GitHub we use a GitHub Application. To create the application on GitHub, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IO, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Cloud"},{"location":"configuration/providers/github-cloud/#github-cloud","text":"","title":"GitHub Cloud"},{"location":"configuration/providers/github-cloud/#github-application","text":"To integrate with GitHub we use a GitHub Application. To create the application on GitHub, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment URL. The URL should contain the endpoint/IO, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Application"},{"location":"configuration/providers/github-cloud/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Cloud as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/providers/github-enterprise/","text":"GitHub Enterprise \u00b6 Set the following configuration from your GitHub instance on the values.yaml file: global : gitHubEnterprise : hostname : \"CHANGE_ME\" protocol : \"CHANGE_ME\" isPrivateMode : \"CHANGE_ME\" port : \"CHANGE_ME\" # This is an optional field disableSSL : \"CHANGE_ME\" # This is an optional field Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitHub Application \u00b6 To integrate with GitHub we use a GitHub Application. To create the application in your GitHub Enterprise, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/ip, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000 Token retrieval \u00b6 After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"GitHub Enterprise"},{"location":"configuration/providers/github-enterprise/#github-enterprise","text":"Set the following configuration from your GitHub instance on the values.yaml file: global : gitHubEnterprise : hostname : \"CHANGE_ME\" protocol : \"CHANGE_ME\" isPrivateMode : \"CHANGE_ME\" port : \"CHANGE_ME\" # This is an optional field disableSSL : \"CHANGE_ME\" # This is an optional field Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitHub Enterprise"},{"location":"configuration/providers/github-enterprise/#github-application","text":"To integrate with GitHub we use a GitHub Application. To create the application in your GitHub Enterprise, visit settings/applications/new and create an application pointing to your local Codacy deployment URL. You can fill all the fields with the suggested text above or use your own text except for the field Authorization callback URL where you must insert your local Codacy deployment url. The URL should contain the endpoint/ip, the protocol (HTTP or HTTPS), and, if applicable, the port where it is running. Correct: http://your.codacy.url.com http://your.codacy.url.com:9000 http://53.43.42.12gi http://53.43.42.12:9000 Incorrect: your.codacy.url.com your.codacy.url.com:9000 53.43.42.12 53.43.42.12:9000","title":"GitHub Application"},{"location":"configuration/providers/github-enterprise/#token-retrieval","text":"After the application is created, you should copy both the Client ID and the Client Secret and paste them in the setup page on your Codacy Self-hosted. After this is done you will be able to use GitHub Enterprise as an authentication method to add repositories and as an integration in the repository settings.","title":"Token retrieval"},{"location":"configuration/providers/gitlab-enterprise/","text":"GitLab Enterprise \u00b6 Set the following configurations from your GitLab instance on the values.yaml file: global : gitLabEnterprise : hostname : \"CHANGE_ME\" protocol : \"CHANGE_ME\" Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied. GitLab Application \u00b6 To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Enterprise"},{"location":"configuration/providers/gitlab-enterprise/#gitlab-enterprise","text":"Set the following configurations from your GitLab instance on the values.yaml file: global : gitLabEnterprise : hostname : \"CHANGE_ME\" protocol : \"CHANGE_ME\" Please note that you must go to http://codacy.example.com/admin/integrations , select the desired provider and Test & Save your configuration for it to be applied.","title":"GitLab Enterprise"},{"location":"configuration/providers/gitlab-enterprise/#gitlab-application","text":"To integrate with GitHub we use a GitLab Application. Create an application pointing to your local Codacy deployment URL. You'll need to add the following 'Redirect URI'. Make sure to update your protocol to use either http or https and your domain name as well. Keep in mind this field is case sensitive. https://codacy.example.com/login/GitLabEnterprise https://codacy.example.com/add/addProvider/GitLabEnterprise https://codacy.example.com/add/addService/GitLabEnterprise Then paste the application ID and secret in Codacy Self-hosted.","title":"GitLab Application"},{"location":"installation/","text":"Install \u00b6 Install Codacy on Kubernetes with the cloud native Codacy Helm chart. This guide will cover the required values and common options. Before starting, make sure you are aware of the requirements . TL;DR \u00b6 Quickly install Codacy for demo without any persistence. export SHARED_PLAY_CRYPTO_SECRET = $( openssl rand -base64 32 | tr -dc 'a-zA-Z0-9' ) echo \"Store this secret: $SHARED_PLAY_CRYPTO_SECRET \" Create a new secret . This will be used by Codacy to encrypt data before storing it in the database. Don't lose it, don't change it. kubectl create namespace codacy kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy export CODACY_URL = \"http://codacy.example.com\" helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --set global.imagePullSecrets [ 0 ] .name = docker-credentials \\ --set global.play.cryptoSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.contentsSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.uuidSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.cacheSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.codacy.url = ${ CODACY_URL } \\ --set global.codacy.backendUrl = ${ CODACY_URL } By now all pods should be starting, but you can only access Codacy from withing your cluster. You should either configure and Ingress or set the service type of the codacy-api to LoadBalancer. Next steps: Enable persistence Enable the Ingress Proceed to more advanced configurations . Selecting configuration options \u00b6 In each section collect the options that will be combined to use with helm install . Secrets \u00b6 Some secrets need to be created (eg. cryptosecret) Also, some Codacy images are currently private. For this, you need to create a secret in the same namespace were you will install Codacy. You should receive these credentials together with your license. kubectl create secret docker-registry docker-credentials --docker-username=$DOCKER_USERNAME --docker-password=$DOCKER_PASSWORD --namespace $NAMESPACE Monitoring the Deployment \u00b6 This will output the list of resources installed once the deployment finishes which may take 5-10 minutes. The status of the deployment can be checked by running helm status codacy which can also be done while the deployment is taking place if you run the command in another terminal.","title":"Install"},{"location":"installation/#install","text":"Install Codacy on Kubernetes with the cloud native Codacy Helm chart. This guide will cover the required values and common options. Before starting, make sure you are aware of the requirements .","title":"Install"},{"location":"installation/#tldr","text":"Quickly install Codacy for demo without any persistence. export SHARED_PLAY_CRYPTO_SECRET = $( openssl rand -base64 32 | tr -dc 'a-zA-Z0-9' ) echo \"Store this secret: $SHARED_PLAY_CRYPTO_SECRET \" Create a new secret . This will be used by Codacy to encrypt data before storing it in the database. Don't lose it, don't change it. kubectl create namespace codacy kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy export CODACY_URL = \"http://codacy.example.com\" helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --set global.imagePullSecrets [ 0 ] .name = docker-credentials \\ --set global.play.cryptoSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.contentsSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.uuidSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.cacheSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.codacy.url = ${ CODACY_URL } \\ --set global.codacy.backendUrl = ${ CODACY_URL } By now all pods should be starting, but you can only access Codacy from withing your cluster. You should either configure and Ingress or set the service type of the codacy-api to LoadBalancer. Next steps: Enable persistence Enable the Ingress Proceed to more advanced configurations .","title":"TL;DR"},{"location":"installation/#selecting-configuration-options","text":"In each section collect the options that will be combined to use with helm install .","title":"Selecting configuration options"},{"location":"installation/#secrets","text":"Some secrets need to be created (eg. cryptosecret) Also, some Codacy images are currently private. For this, you need to create a secret in the same namespace were you will install Codacy. You should receive these credentials together with your license. kubectl create secret docker-registry docker-credentials --docker-username=$DOCKER_USERNAME --docker-password=$DOCKER_PASSWORD --namespace $NAMESPACE","title":"Secrets"},{"location":"installation/#monitoring-the-deployment","text":"This will output the list of resources installed once the deployment finishes which may take 5-10 minutes. The status of the deployment can be checked by running helm status codacy which can also be done while the deployment is taking place if you run the command in another terminal.","title":"Monitoring the Deployment"},{"location":"installation/uninstall/","text":"Uninstall \u00b6 To ensure a clean removal you should uninstall Codacy before cleaning destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstall"},{"location":"installation/uninstall/#uninstall","text":"To ensure a clean removal you should uninstall Codacy before cleaning destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODS to delete:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"Uninstall"},{"location":"installation/upgrade/","text":"Upgrade \u00b6 NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values Steps \u00b6 The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrade"},{"location":"installation/upgrade/#upgrade","text":"NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values","title":"Upgrade"},{"location":"installation/upgrade/#steps","text":"The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Steps"},{"location":"migration/database/","text":"Database Migration Guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results results201709 Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results results201709 ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" PGPASSWORD= $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Database Migration Guide"},{"location":"migration/database/#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results results201709","title":"Database Migration Guide"},{"location":"migration/database/#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"migration/database/#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"migration/database/#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"migration/database/#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar NOTE: If you run into any problems while restoring, make sure that you have the database created in that postgres instance (e.g. before restoring the jobs database the postgres instance should have an empty database called jobs created there) For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"migration/database/#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results results201709 ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" PGPASSWORD= $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Sample script"},{"location":"quickstart/aks_quickstart/","text":"AKS cluster quickstart \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"AKS cluster quickstart"},{"location":"quickstart/aks_quickstart/#aks-cluster-quickstart","text":"","title":"AKS cluster quickstart"},{"location":"quickstart/aks_quickstart/#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"quickstart/aks_quickstart/#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"quickstart/aks_quickstart/#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"quickstart/aks_quickstart/#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"quickstart/eks_quickstart/","text":"EKS cluster setup using terraform \u00b6 This folder includes the terraform templates needed to create an EKS cluster from scratch, including all the necessary underlying infrastructure. It includes the following infrastructure stacks: backend - (optional) the S3 bucket for storing the terraform state and the DynamoDB table for state locking. main - the EKS cluster, including all the network and nodes setup needed to go from zero to a fully functional EKS cluster. setup - additional setup you must perform before installing Codacy on your vanilla EKS cluster. Requirements \u00b6 In order to setup the infrastructure you'll need recent versions of: awscli terraform kubectl helm Please follow the documentation in the above links to setup these tools for your OS. They can usually be easily installed using your package manager. For instance, on macOS, if you are using Homebrew , you can just run: brew install awscli terraform kubernetes-helm kubectl You'll also need to setup the CLI credentials for your AWS account. See how to do it the AWS and Terraform documentation. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly. TL;DR \u00b6 Seting up an EKS cluster for Codacy \u00b6 Assuming AWS is well configured and you fulfill all the prerequisites, the setup without state storage, can be done with: export AWS_SDK_LOAD_CONFIG = 1 cd main/ terraform init && terraform apply cd ../setup/ terraform init && terraform apply aws eks update-kubeconfig --name codacy-cluster Deployment \u00b6 1. backend - setup terraform state storage \u00b6 The backend stores the current and historical state of your infrastructure. Deploying and using it is optional, but it might make your life easier if you are planning to use these templates to make modifications to the cluster in the future. To deploy it run: terraform init && terraform apply inside the backend/ folder and follow terraform's instructions. An S3 bucket with an unique name to save your state will be created. Note this bucket's name and set it on the config.tf file of the main/ and setup/ stacks where indicated. 2. main - create a vanilla EKS cluster \u00b6 To create a cluster, along with all the necessary network and nodes setup it requires, run: terraform init && terraform apply inside the main/ folder and follow terraform's instructions. This takes a while (~10min). The cluster configuration (e.g., type/number of nodes, network CIDRs,...) are exposed as variables in the variables.tf file. You may tailor the cluster to your needs by editing the defaults that file, by using CLI options, viz. terraform apply -var = \"some_key=some_value\" or by writing them on a file named terraform.tfvars in the same folder as the infrastructure code, which is loaded by default when you run apply. For instance: some_key = \"a_string_value\" another_key = 3 someting_else = true To connect to the cluster get its kubeconfig and set it the as default context with: aws eks update-kubeconfig --name codacy-cluster If you now run kubectl get pods -A you'll see that nothing is scheduled. That's because you haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more info). We'll do that on step 3. 3. setup - configure your EKS cluster to deploy codacy \u00b6 Some additional setup is necessary to run Codacy on the cluster you just created. For one, you need to allow workers to join the cluster. Docker pull secrets need to be added to the codacy namespace, which will also be added here, and the following helm charts will be installed: kubernetes-dashboard , nginx-ingress , and cert-manager . To do it run terraform init && terraform apply You'll be prompted to input Codacy's docker hub repo password. If you'd like to connect to the kubernetes dashboard, get the admin token by running terraform output admin_token and copy the outputed value. To connect to the dashboard run kubectl proxy and then connect to the dashboard url , select token and paste the value you saved above. 4. Installing Codacy \u00b6 To install Codacy please see the installation documentation . Uninstalling \u00b6 WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS 1. Remove the cluster setup required to install Codacy \u00b6 To cleanup your cluster back to a vanilla state you can now run, the following command in the setup/ folder: terraform destroy 2. Remove the cluster \u00b6 After removing all the above stacks and setup, you may now delete the kubernetes cluster by running in the main/ directory: terraform destroy This takes a while (~10min). 3. Removing the terraform backend \u00b6 If you created the terraform backend with the above stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly the easiest path is to disable these extra settings. Go to the backend/ folder and change the state_and_lock.tf file as instructed therein. Afterwards, you can now destroy it by running terraform apply && terraform destroy Note that you first have to apply to change the bucket settings, and only then will destroy work.","title":"EKS cluster setup using terraform"},{"location":"quickstart/eks_quickstart/#eks-cluster-setup-using-terraform","text":"This folder includes the terraform templates needed to create an EKS cluster from scratch, including all the necessary underlying infrastructure. It includes the following infrastructure stacks: backend - (optional) the S3 bucket for storing the terraform state and the DynamoDB table for state locking. main - the EKS cluster, including all the network and nodes setup needed to go from zero to a fully functional EKS cluster. setup - additional setup you must perform before installing Codacy on your vanilla EKS cluster.","title":"EKS cluster setup using terraform"},{"location":"quickstart/eks_quickstart/#requirements","text":"In order to setup the infrastructure you'll need recent versions of: awscli terraform kubectl helm Please follow the documentation in the above links to setup these tools for your OS. They can usually be easily installed using your package manager. For instance, on macOS, if you are using Homebrew , you can just run: brew install awscli terraform kubernetes-helm kubectl You'll also need to setup the CLI credentials for your AWS account. See how to do it the AWS and Terraform documentation. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly.","title":"Requirements"},{"location":"quickstart/eks_quickstart/#tldr","text":"","title":"TL;DR"},{"location":"quickstart/eks_quickstart/#seting-up-an-eks-cluster-for-codacy","text":"Assuming AWS is well configured and you fulfill all the prerequisites, the setup without state storage, can be done with: export AWS_SDK_LOAD_CONFIG = 1 cd main/ terraform init && terraform apply cd ../setup/ terraform init && terraform apply aws eks update-kubeconfig --name codacy-cluster","title":"Seting up an EKS cluster for Codacy"},{"location":"quickstart/eks_quickstart/#deployment","text":"","title":"Deployment"},{"location":"quickstart/eks_quickstart/#1-backend-setup-terraform-state-storage","text":"The backend stores the current and historical state of your infrastructure. Deploying and using it is optional, but it might make your life easier if you are planning to use these templates to make modifications to the cluster in the future. To deploy it run: terraform init && terraform apply inside the backend/ folder and follow terraform's instructions. An S3 bucket with an unique name to save your state will be created. Note this bucket's name and set it on the config.tf file of the main/ and setup/ stacks where indicated.","title":"1. backend - setup terraform state storage"},{"location":"quickstart/eks_quickstart/#2-main-create-a-vanilla-eks-cluster","text":"To create a cluster, along with all the necessary network and nodes setup it requires, run: terraform init && terraform apply inside the main/ folder and follow terraform's instructions. This takes a while (~10min). The cluster configuration (e.g., type/number of nodes, network CIDRs,...) are exposed as variables in the variables.tf file. You may tailor the cluster to your needs by editing the defaults that file, by using CLI options, viz. terraform apply -var = \"some_key=some_value\" or by writing them on a file named terraform.tfvars in the same folder as the infrastructure code, which is loaded by default when you run apply. For instance: some_key = \"a_string_value\" another_key = 3 someting_else = true To connect to the cluster get its kubeconfig and set it the as default context with: aws eks update-kubeconfig --name codacy-cluster If you now run kubectl get pods -A you'll see that nothing is scheduled. That's because you haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more info). We'll do that on step 3.","title":"2. main - create a vanilla EKS cluster"},{"location":"quickstart/eks_quickstart/#3-setup-configure-your-eks-cluster-to-deploy-codacy","text":"Some additional setup is necessary to run Codacy on the cluster you just created. For one, you need to allow workers to join the cluster. Docker pull secrets need to be added to the codacy namespace, which will also be added here, and the following helm charts will be installed: kubernetes-dashboard , nginx-ingress , and cert-manager . To do it run terraform init && terraform apply You'll be prompted to input Codacy's docker hub repo password. If you'd like to connect to the kubernetes dashboard, get the admin token by running terraform output admin_token and copy the outputed value. To connect to the dashboard run kubectl proxy and then connect to the dashboard url , select token and paste the value you saved above.","title":"3. setup - configure your EKS cluster to deploy codacy"},{"location":"quickstart/eks_quickstart/#4-installing-codacy","text":"To install Codacy please see the installation documentation .","title":"4. Installing Codacy"},{"location":"quickstart/eks_quickstart/#uninstalling","text":"WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS","title":"Uninstalling"},{"location":"quickstart/eks_quickstart/#1-remove-the-cluster-setup-required-to-install-codacy","text":"To cleanup your cluster back to a vanilla state you can now run, the following command in the setup/ folder: terraform destroy","title":"1.  Remove the cluster setup required to install Codacy"},{"location":"quickstart/eks_quickstart/#2-remove-the-cluster","text":"After removing all the above stacks and setup, you may now delete the kubernetes cluster by running in the main/ directory: terraform destroy This takes a while (~10min).","title":"2.  Remove the cluster"},{"location":"quickstart/eks_quickstart/#3-removing-the-terraform-backend","text":"If you created the terraform backend with the above stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly the easiest path is to disable these extra settings. Go to the backend/ folder and change the state_and_lock.tf file as instructed therein. Afterwards, you can now destroy it by running terraform apply && terraform destroy Note that you first have to apply to change the bucket settings, and only then will destroy work.","title":"3.  Removing the terraform backend"},{"location":"requirements/","text":"Requirements \u00b6 If you want to deploy Codacy on Kubernetes, the following requirements must be met: kubectl 1.13 or higher, compatible with your cluster ( +/- 1 minor release from your cluster ). Helm 2.14> <3.0 A Kubernetes cluster, between version 1.13 and 1.15. 16vCPU and 64GB of RAM is the recommended minimum. Postgres database ( https://support.codacy.com/hc/en-us/articles/360002902573-Installing-postgres-for-Codacy-Enterprise ) Resources \u00b6 To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 39 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production ready\" version of Codacy. Analysis \u00b6 Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.genericMax + workers.dedicatedMax Total resources \u00b6 Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis. Example \u00b6 Maximum of 6 concurrent analysis worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 39 GB + 36 GB = 75 GB","title":"Requirements"},{"location":"requirements/#requirements","text":"If you want to deploy Codacy on Kubernetes, the following requirements must be met: kubectl 1.13 or higher, compatible with your cluster ( +/- 1 minor release from your cluster ). Helm 2.14> <3.0 A Kubernetes cluster, between version 1.13 and 1.15. 16vCPU and 64GB of RAM is the recommended minimum. Postgres database ( https://support.codacy.com/hc/en-us/articles/360002902573-Installing-postgres-for-Codacy-Enterprise )","title":"Requirements"},{"location":"requirements/#resources","text":"To have an enjoyable experience with Codacy, you should have the following calculations in mind when allocating resources for the installation and defining the number of concurrent analysis. Without accounting for analysis ( next section ), you should need at least the following resources: CPU: 7 CPU Memory: 39 GB Check the values-production.yaml file to find a configuration reference that should work for you to run a \"production ready\" version of Codacy.","title":"Resources"},{"location":"requirements/#analysis","text":"Each analysis runs a maximum number of 4 plugins in parallel (not configurable) Note: All the following configurations are nested inside the worker-manager.config configuration object, but for simplicity, we decided to omit the full path. CPU: workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory: workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis: workers.genericMax + workers.dedicatedMax","title":"Analysis"},{"location":"requirements/#total-resources","text":"Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis.","title":"Total resources"},{"location":"requirements/#example","text":"Maximum of 6 concurrent analysis worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB Total: CPU: 7 CPU + 18 CPU = 25 CPU Memory: 39 GB + 36 GB = 75 GB","title":"Example"}]}