{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Codacy Chart \u00b6 With the Codacy chart it is possible to run all Codacy components and its dependencies with a single command line. edit image Each service/component in Codacy has its chart published to https://charts.codacy.com/stable . This chart bundles all the components and their dependencies. For the bundle, we make use of the requirements capability of Helm. Work in progress \u00b6 This chart is still a Work In Progress and is not ready for general usage. Our docker images are currently private and you will not be able to run the chart by yourself. If you are interested in trying out Codacy contact our support at support@codacy.com. Charts \u00b6 Documentation on a per-chart basis is listed here. Some of these repositories are private and accessible to Codacy engineers only. Minio RabbitMQ-HA Postgres kube-fluentd-operator Codacy/ Token Management Codacy/ Website Codacy/ API Codacy/ Ragnaros Codacy/ Activities Codacy/ Repository Listener Codacy/ Portal Codacy/ Worker Manager Codacy/ Engine Codacy/ Core Codacy/ Hotspots API Codacy/ Hotspots Worker Configuration \u00b6 The following table lists the configurable parameters of the Codacy chart and their default values. Global parameters apply to all sub-charts and make it easier to configure resources across different components. Parameter Description Default global.codacy.url Hostname to your Codacy installation None global.codacy.backendUrl Hostname to your Codacy installation None global.play.cryptoSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.filestore.contentsSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.filestore.uuidSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.cacheSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.minio.create Create minio internally None global.rabbitmq.create Create rabbitmq internally None global.rabbitmq.rabbitmqUsername Username for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqUsername also. None global.rabbitmq.rabbitmqPassword Password for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqPassword also. None global.defaultdb.postgresqlUsername Username of the Postgresql server codacy global.defaultdb.postgresqlDatabase Database name of the Postgresql server default global.defaultdb.postgresqlPassword Hostname of the Postgresql server None global.defaultdb.host Hostname of the Postgresql server None global.defaultdb.service.port Port of the Postgresql server 5432 global.analysisdb.postgresqlUsername Username of the Postgresql server codacy global.analysisdb.postgresqlDatabase Database name of the Postgresql server analysis global.analysisdb.postgresqlPassword Hostname of the Postgresql server None global.analysisdb.host Hostname of the Postgresql server None global.analysisdb.service.port Port of the Postgresql server 5432 global.resultsdb.postgresqlUsername Username of the Postgresql server codacy global.resultsdb.postgresqlDatabase Database name of the Postgresql server results global.resultsdb.postgresqlPassword Hostname of the Postgresql server None global.resultsdb.host Hostname of the Postgresql server None global.resultsdb.service.port Port of the Postgresql server 5432 global.resultsdb201709.postgresqlUsername Username of the Postgresql server codacy global.resultsdb201709.postgresqlDatabase Database name of the Postgresql server results201709 global.resultsdb201709.postgresqlPassword Hostname of the Postgresql server None global.resultsdb201709.host Hostname of the Postgresql server None global.resultsdb201709.service.port Port of the Postgresql server 5432 global.metricsdb.postgresqlUsername Username of the Postgresql server codacy global.metricsdb.postgresqlDatabase Database name of the Postgresql server metrics global.metricsdb.postgresqlPassword Hostname of the Postgresql server None global.metricsdb.host Hostname of the Postgresql server None global.metricsdb.service.port Port of the Postgresql server 5432 global.filestoredb.postgresqlUsername Username of the Postgresql server codacy global.filestoredb.postgresqlDatabase Database name of the Postgresql server filestore global.filestoredb.postgresqlPassword Hostname of the Postgresql server None global.filestoredb.host Hostname of the Postgresql server None global.filestoredb.service.port Port of the Postgresql server 5432 global.jobsdb.postgresqlUsername Username of the Postgresql server codacy global.jobsdb.postgresqlDatabase Database name of the Postgresql server jobs global.jobsdb.postgresqlPassword Hostname of the Postgresql server None global.jobsdb.host Hostname of the Postgresql server None global.jobsdb.service.port Port of the Postgresql server 5432 The following parameters are specific to each Codacy component. Parameter Description Default portal.replicaCount Number of replicas 1 portal.image.repository Image repository from dependency portal.image.tag Image tag from dependency portal.service.type Portal service type ClusterIP portal.service.annotations Annotations to be added to the Portal service {} remote-provider-service.replicaCount Number of replicas 1 remote-provider-service.image.repository Image repository from dependency remote-provider-service.image.tag Image tag from dependency remote-provider-service.service.type Remote Provider service type ClusterIP remote-provider-service.service.annotations Annotations to be added to the Remote Provider service {} activities.replicaCount Number of replicas 1 activities.image.repository Image repository from dependency activities.image.tag Image tag from dependency activities.service.type Service type ClusterIP activities.service.annotations Annotations to be added to the service {} activities.activitiesdb.postgresqlUsername Username of the Postgresql server codacy activities.activitiesdb.postgresqlDatabase Database name of the Postgresql server jobs activities.activitiesdb.postgresqlPassword Hostname of the Postgresql server None activities.activitiesdb.host Hostname of the Postgresql server None activities.activitiesdb.service.port Port of the Postgresql server 5432 hotspots-api.replicaCount Number of replicas 1 hotspots-api.image.repository Image repository from dependency hotspots-api.image.tag Image tag from dependency hotspots-api.service.type Service type ClusterIP hotspots-api.service.annotations Annotations to be added to the service {} hotspots-api.hotspotsdb.postgresqlUsername Username of the Postgresql server codacy hotspots-api.hotspotsdb.postgresqlDatabase Database name of the Postgresql server hotspots hotspots-api.hotspotsdb.postgresqlPassword Hostname of the Postgresql server None hotspots-api.hotspotsdb.host Hostname of the Postgresql server None hotspots-api.hotspotsdb.service.port Port of the Postgresql server 5432 hotspots-worker.replicaCount Number of replicas 1 hotspots-worker.image.repository Image repository from dependency hotspots-worker.image.tag Image tag from dependency hotspots-worker.service.type Service type ClusterIP hotspots-worker.service.annotations Annotations to be added to the service {} listener.replicaCount Number of replicas 1 listener.image.repository Image repository from dependency listener.image.tag Image tag from dependency listener.service.type Service type ClusterIP listener.service.annotations Annotations to be added to the service {} listener.persistence.claim.size Each pod mounts and NFS disk and claims this size. 100Gi listener.nfsserverprovisioner.enabled Creates an NFS server and a storage class to mount volumes in that server. true listener.nfsserverprovisioner.persistence.enabled Creates an NFS provisioner true listener.nfsserverprovisioner.persistence.size Size of the NFS server disk 120Gi listener.listenerdb.postgresqlUsername Username of the Postgresql server codacy listener.listenerdb.postgresqlDatabase Database name of the Postgresql server listener listener.listenerdb.postgresqlPassword Hostname of the Postgresql server PLEASE_CHANGE_ME listener.listenerdb.host Hostname of the Postgresql server codacy-listenerdb.codacy.svc.cluster.local listener.listenerdb.service.port Port of the Postgresql server 5432 core.replicaCount Number of replicas 1 core.image.repository Image repository from dependency core.image.tag Image tag from dependency core.service.type Service type ClusterIP engine.replicaCount Number of replicas 1 engine.image.repository Image repository from dependency engine.image.tag Image tag from dependency engine.service.type Service type ClusterIP engine.service.annotations Annotations to be added to the service {} engine.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.image.repository Image repository from dependency codacy-api.image.tag Image tag from dependency codacy-api.service.type Service type ClusterIP codacy-api.config.license Codacy license for your installation None codacy-api.service.annotations Annotations to be added to the service {} codacy-api.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.metrics.grafana_dashboards.enabled Create the ConfigMap with the dashboard of this component. Can be imported through grafana sidecar. false worker-manager.config.workers.genericMax TBD 100 worker-manager.config.workers.dedicatedMax TBD 100 crow.replicaCount Number of replicas 1 crow.image.repository Image repository from dependency crow.image.tag Image tag from dependency The following parameters refer to components that are not internal to Codacy, but go as part of this bundle so that you can bootstrap Codacy faster. Parameter Description Default fluentdoperator.enable Enable fluentd operator. It gathers logs from Codacy so that you can send it to our support if needed. false fluentdoperator.expirationDays Number of days to retain logs. More time uses more disk on minio. 14 rabbitmq-ha.rabbitmqUsername Username for the bundled RabbitMQ. rabbitmq rabbitmq-ha.rabbitmqPassword Password for the bundled RabbitMQ. rabbitmq You can also configure values for the PostgreSQL database via the Postgresql README.md For overriding variables see: Customizing the chart","title":"Codacy Chart"},{"location":"index.html#codacy-chart","text":"With the Codacy chart it is possible to run all Codacy components and its dependencies with a single command line. edit image Each service/component in Codacy has its chart published to https://charts.codacy.com/stable . This chart bundles all the components and their dependencies. For the bundle, we make use of the requirements capability of Helm.","title":"Codacy Chart"},{"location":"index.html#work-in-progress","text":"This chart is still a Work In Progress and is not ready for general usage. Our docker images are currently private and you will not be able to run the chart by yourself. If you are interested in trying out Codacy contact our support at support@codacy.com.","title":"Work in progress"},{"location":"index.html#charts","text":"Documentation on a per-chart basis is listed here. Some of these repositories are private and accessible to Codacy engineers only. Minio RabbitMQ-HA Postgres kube-fluentd-operator Codacy/ Token Management Codacy/ Website Codacy/ API Codacy/ Ragnaros Codacy/ Activities Codacy/ Repository Listener Codacy/ Portal Codacy/ Worker Manager Codacy/ Engine Codacy/ Core Codacy/ Hotspots API Codacy/ Hotspots Worker","title":"Charts"},{"location":"index.html#configuration","text":"The following table lists the configurable parameters of the Codacy chart and their default values. Global parameters apply to all sub-charts and make it easier to configure resources across different components. Parameter Description Default global.codacy.url Hostname to your Codacy installation None global.codacy.backendUrl Hostname to your Codacy installation None global.play.cryptoSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.filestore.contentsSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.filestore.uuidSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.cacheSecret Secrets used internally for encryption. Generate one with cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 128 | head -n 1 None global.minio.create Create minio internally None global.rabbitmq.create Create rabbitmq internally None global.rabbitmq.rabbitmqUsername Username for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqUsername also. None global.rabbitmq.rabbitmqPassword Password for rabbitmq. If you are using the bundled version, change the rabbitmq-ha.rabbitmqPassword also. None global.defaultdb.postgresqlUsername Username of the Postgresql server codacy global.defaultdb.postgresqlDatabase Database name of the Postgresql server default global.defaultdb.postgresqlPassword Hostname of the Postgresql server None global.defaultdb.host Hostname of the Postgresql server None global.defaultdb.service.port Port of the Postgresql server 5432 global.analysisdb.postgresqlUsername Username of the Postgresql server codacy global.analysisdb.postgresqlDatabase Database name of the Postgresql server analysis global.analysisdb.postgresqlPassword Hostname of the Postgresql server None global.analysisdb.host Hostname of the Postgresql server None global.analysisdb.service.port Port of the Postgresql server 5432 global.resultsdb.postgresqlUsername Username of the Postgresql server codacy global.resultsdb.postgresqlDatabase Database name of the Postgresql server results global.resultsdb.postgresqlPassword Hostname of the Postgresql server None global.resultsdb.host Hostname of the Postgresql server None global.resultsdb.service.port Port of the Postgresql server 5432 global.resultsdb201709.postgresqlUsername Username of the Postgresql server codacy global.resultsdb201709.postgresqlDatabase Database name of the Postgresql server results201709 global.resultsdb201709.postgresqlPassword Hostname of the Postgresql server None global.resultsdb201709.host Hostname of the Postgresql server None global.resultsdb201709.service.port Port of the Postgresql server 5432 global.metricsdb.postgresqlUsername Username of the Postgresql server codacy global.metricsdb.postgresqlDatabase Database name of the Postgresql server metrics global.metricsdb.postgresqlPassword Hostname of the Postgresql server None global.metricsdb.host Hostname of the Postgresql server None global.metricsdb.service.port Port of the Postgresql server 5432 global.filestoredb.postgresqlUsername Username of the Postgresql server codacy global.filestoredb.postgresqlDatabase Database name of the Postgresql server filestore global.filestoredb.postgresqlPassword Hostname of the Postgresql server None global.filestoredb.host Hostname of the Postgresql server None global.filestoredb.service.port Port of the Postgresql server 5432 global.jobsdb.postgresqlUsername Username of the Postgresql server codacy global.jobsdb.postgresqlDatabase Database name of the Postgresql server jobs global.jobsdb.postgresqlPassword Hostname of the Postgresql server None global.jobsdb.host Hostname of the Postgresql server None global.jobsdb.service.port Port of the Postgresql server 5432 The following parameters are specific to each Codacy component. Parameter Description Default portal.replicaCount Number of replicas 1 portal.image.repository Image repository from dependency portal.image.tag Image tag from dependency portal.service.type Portal service type ClusterIP portal.service.annotations Annotations to be added to the Portal service {} remote-provider-service.replicaCount Number of replicas 1 remote-provider-service.image.repository Image repository from dependency remote-provider-service.image.tag Image tag from dependency remote-provider-service.service.type Remote Provider service type ClusterIP remote-provider-service.service.annotations Annotations to be added to the Remote Provider service {} activities.replicaCount Number of replicas 1 activities.image.repository Image repository from dependency activities.image.tag Image tag from dependency activities.service.type Service type ClusterIP activities.service.annotations Annotations to be added to the service {} activities.activitiesdb.postgresqlUsername Username of the Postgresql server codacy activities.activitiesdb.postgresqlDatabase Database name of the Postgresql server jobs activities.activitiesdb.postgresqlPassword Hostname of the Postgresql server None activities.activitiesdb.host Hostname of the Postgresql server None activities.activitiesdb.service.port Port of the Postgresql server 5432 hotspots-api.replicaCount Number of replicas 1 hotspots-api.image.repository Image repository from dependency hotspots-api.image.tag Image tag from dependency hotspots-api.service.type Service type ClusterIP hotspots-api.service.annotations Annotations to be added to the service {} hotspots-api.hotspotsdb.postgresqlUsername Username of the Postgresql server codacy hotspots-api.hotspotsdb.postgresqlDatabase Database name of the Postgresql server hotspots hotspots-api.hotspotsdb.postgresqlPassword Hostname of the Postgresql server None hotspots-api.hotspotsdb.host Hostname of the Postgresql server None hotspots-api.hotspotsdb.service.port Port of the Postgresql server 5432 hotspots-worker.replicaCount Number of replicas 1 hotspots-worker.image.repository Image repository from dependency hotspots-worker.image.tag Image tag from dependency hotspots-worker.service.type Service type ClusterIP hotspots-worker.service.annotations Annotations to be added to the service {} listener.replicaCount Number of replicas 1 listener.image.repository Image repository from dependency listener.image.tag Image tag from dependency listener.service.type Service type ClusterIP listener.service.annotations Annotations to be added to the service {} listener.persistence.claim.size Each pod mounts and NFS disk and claims this size. 100Gi listener.nfsserverprovisioner.enabled Creates an NFS server and a storage class to mount volumes in that server. true listener.nfsserverprovisioner.persistence.enabled Creates an NFS provisioner true listener.nfsserverprovisioner.persistence.size Size of the NFS server disk 120Gi listener.listenerdb.postgresqlUsername Username of the Postgresql server codacy listener.listenerdb.postgresqlDatabase Database name of the Postgresql server listener listener.listenerdb.postgresqlPassword Hostname of the Postgresql server PLEASE_CHANGE_ME listener.listenerdb.host Hostname of the Postgresql server codacy-listenerdb.codacy.svc.cluster.local listener.listenerdb.service.port Port of the Postgresql server 5432 core.replicaCount Number of replicas 1 core.image.repository Image repository from dependency core.image.tag Image tag from dependency core.service.type Service type ClusterIP engine.replicaCount Number of replicas 1 engine.image.repository Image repository from dependency engine.image.tag Image tag from dependency engine.service.type Service type ClusterIP engine.service.annotations Annotations to be added to the service {} engine.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.image.repository Image repository from dependency codacy-api.image.tag Image tag from dependency codacy-api.service.type Service type ClusterIP codacy-api.config.license Codacy license for your installation None codacy-api.service.annotations Annotations to be added to the service {} codacy-api.metrics.serviceMonitor.enabled Create the ServiceMonitor resource type to be read by prometheus operator. false codacy-api.metrics.grafana_dashboards.enabled Create the ConfigMap with the dashboard of this component. Can be imported through grafana sidecar. false worker-manager.config.workers.genericMax TBD 100 worker-manager.config.workers.dedicatedMax TBD 100 crow.replicaCount Number of replicas 1 crow.image.repository Image repository from dependency crow.image.tag Image tag from dependency The following parameters refer to components that are not internal to Codacy, but go as part of this bundle so that you can bootstrap Codacy faster. Parameter Description Default fluentdoperator.enable Enable fluentd operator. It gathers logs from Codacy so that you can send it to our support if needed. false fluentdoperator.expirationDays Number of days to retain logs. More time uses more disk on minio. 14 rabbitmq-ha.rabbitmqUsername Username for the bundled RabbitMQ. rabbitmq rabbitmq-ha.rabbitmqPassword Password for the bundled RabbitMQ. rabbitmq You can also configure values for the PostgreSQL database via the Postgresql README.md For overriding variables see: Customizing the chart","title":"Configuration"},{"location":"configuration/logging.html","text":"Logging \u00b6 Log collection using fluentd \u00b6 To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Logging"},{"location":"configuration/logging.html#logging","text":"","title":"Logging"},{"location":"configuration/logging.html#log-collection-using-fluentd","text":"To collect logs from your Codacy installation using fluentd change fluentdoperator.enabled to true. --set fluentdoperator.enabled = true The fluentd daemonset will send the logs to minio which is also installed by this chart. In order to send them to our support in case of problems, run the following command locally (replacing the <namespace> with the namespace in which codacy was installed): kubectl cp <namespace>/ $( kubectl get pods -n <namespace> -l app = minio -o jsonpath = '{.items[*].metadata.name}' ) :/export/fluentd-bucket ./logs","title":"Log collection using fluentd"},{"location":"configuration/monitoring.html","text":"Monitoring \u00b6 In older versions, codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana. Setting up monitoring using Grafana & Prometheus \u00b6 The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true Setting up monitoring using Crow [deprecated] \u00b6 Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed along side Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Monitoring"},{"location":"configuration/monitoring.html#monitoring","text":"In older versions, codacy was monitored using an application we build in-house called \"Crow\". Going forward, we are deprecating Crow and setting up \"Prometheus style\" metrics in all our components that can be accessed using the Prometheus + Grafana ecosystem. Initially Crow will still be bundled in Codacy helm chart, until we have all functionality available on Grafana.","title":"Monitoring"},{"location":"configuration/monitoring.html#setting-up-monitoring-using-grafana-prometheus","text":"The simplest way to setup prometheus in your cluster is by using the Prometheus-Operator bundle. kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/podmonitor.crd.yaml \u200b sleep 5 # wait for crds to be created \u200b helm upgrade --install monitoring stable/prometheus-operator \\ --version 6 .9.3 \\ --namespace monitoring \\ --set prometheusOperator.createCustomResource = false \\ --set grafana.service.type = LoadBalancer \\ --set grafana.adminPassword = \"CHANGE_HERE\" \u200b kubectl get svc monitoring-grafana -n monitoring Now that you have prometheus and grafana you can enable ServiceMonitors and GrafanaDashboards for Codacy components: codacy-api : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true engine : metrics : serviceMonitor : enabled : true grafana_dashboards : enabled : true","title":"Setting up monitoring using Grafana &amp; Prometheus"},{"location":"configuration/monitoring.html#setting-up-monitoring-using-crow-deprecated","text":"Crow is a visualization tool that displays information about the projects and jobs that are pending for analysis, as well as the ones running in the Codacy platform. The Crow tool is installed along side Codacy after the helm chart is deployed to the cluster. Important note This tool is in the process of being replaced by Grafana (which will handle authentication) and therefore the credentials to login can be freely shared since there is no sensitive information displayed in the platform. The current credentials are the following: username : codacy password : C0dacy123","title":"Setting up monitoring using Crow [deprecated]"},{"location":"installation/index.html","text":"Installing Codacy using Helm \u00b6 Install Codacy on Kubernetes with the cloud native Codacy Helm chart. Environment setup \u00b6 Before proceeding to deploying Codacy, you need to prepare your environment. Tools \u00b6 helm and kubectl need to be installed. Deploying Codacy \u00b6 With the environment set up and configuration generated, you can now proceed to the deployment of Codacy . Upgrading Codacy \u00b6 If you are upgrading an existing Kubernetes installation, follow the upgrade documentation instead. Codacy Self-hosted Installation \u00b6 To have your setup fully functional, Codacy will need to download the code analyzers after running for the first time. This process can take up to an extra hour, depending on network speed. Installing Codacy using the Helm Chart \u00b6 The codacy chart includes all required dependencies, and takes a few minutes to deploy: Preparation Deployment Updating Codacy using the Helm Chart \u00b6 Once your Codacy Chart is installed, configuration changes and chart updates should be done using helm upgrade : helm repo add codacy https://charts.codacy.com/stable/ helm repo update helm get values codacy > codacy.yaml helm upgrade codacy codacy/codacy -f codacy.yaml For more detailed information see Upgrading . Uninstalling Codacy using the Helm Chart \u00b6 To uninstall the Codacy Chart, run the following: helm delete codacy Requirements \u00b6 All requirements Resource Requirements Installation methods \u00b6 All installation guides Configuration \u00b6 Logging Monitoring","title":"Installing Codacy using Helm"},{"location":"installation/index.html#installing-codacy-using-helm","text":"Install Codacy on Kubernetes with the cloud native Codacy Helm chart.","title":"Installing Codacy using Helm"},{"location":"installation/index.html#environment-setup","text":"Before proceeding to deploying Codacy, you need to prepare your environment.","title":"Environment setup"},{"location":"installation/index.html#tools","text":"helm and kubectl need to be installed.","title":"Tools"},{"location":"installation/index.html#deploying-codacy","text":"With the environment set up and configuration generated, you can now proceed to the deployment of Codacy .","title":"Deploying Codacy"},{"location":"installation/index.html#upgrading-codacy","text":"If you are upgrading an existing Kubernetes installation, follow the upgrade documentation instead.","title":"Upgrading Codacy"},{"location":"installation/index.html#codacy-self-hosted-installation","text":"To have your setup fully functional, Codacy will need to download the code analyzers after running for the first time. This process can take up to an extra hour, depending on network speed.","title":"Codacy Self-hosted Installation"},{"location":"installation/index.html#installing-codacy-using-the-helm-chart","text":"The codacy chart includes all required dependencies, and takes a few minutes to deploy: Preparation Deployment","title":"Installing Codacy using the Helm Chart"},{"location":"installation/index.html#updating-codacy-using-the-helm-chart","text":"Once your Codacy Chart is installed, configuration changes and chart updates should be done using helm upgrade : helm repo add codacy https://charts.codacy.com/stable/ helm repo update helm get values codacy > codacy.yaml helm upgrade codacy codacy/codacy -f codacy.yaml For more detailed information see Upgrading .","title":"Updating Codacy using the Helm Chart"},{"location":"installation/index.html#uninstalling-codacy-using-the-helm-chart","text":"To uninstall the Codacy Chart, run the following: helm delete codacy","title":"Uninstalling Codacy using the Helm Chart"},{"location":"installation/index.html#requirements","text":"All requirements Resource Requirements","title":"Requirements"},{"location":"installation/index.html#installation-methods","text":"All installation guides","title":"Installation methods"},{"location":"installation/index.html#configuration","text":"Logging Monitoring","title":"Configuration"},{"location":"installation/deployment.html","text":"Deployment Guide \u00b6 Before running helm install , you need to make some decisions about how you will run Codacy. This guide will cover the required values and common options. TL;DR \u00b6 Quickly install Codacy for demo without any persistence. kubectl create namespace codacy kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy export SHARED_PLAY_CRYPTO_SECRET = $( openssl rand -base64 32 | tr -dc 'a-zA-Z0-9' ) echo \"Store this secret: $SHARED_PLAY_CRYPTO_SECRET \" export CODACY_URL = \"http://codacy.example.com\" helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --set global.imagePullSecrets [ 0 ] .name = docker-credentials \\ --set global.play.cryptoSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.contentsSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.uuidSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.cacheSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.codacy.url = ${ CODACY_URL } \\ --set global.codacy.backendUrl = ${ CODACY_URL } \\ --set codacy-api.service.type = \"LoadBalancer\" If you want to enable persistence please config Selecting configuration options \u00b6 In each section collect the options that will be combined to use with helm install . Secrets \u00b6 Some secrets need to be created (eg. cryptosecret) Also, some Codacy images are currently private. For this, you need to create a secret in the same namespace were you will install Codacy. You should receive these credentials together with your license. kubectl create secret docker-registry docker-credentials --docker-username=$DOCKER_USERNAME --docker-password=$DOCKER_PASSWORD --namespace $NAMESPACE Monitoring the Deployment \u00b6 This will output the list of resources installed once the deployment finishes which may take 5-10 minutes. The status of the deployment can be checked by running helm status codacy which can also be done while the deployment is taking place if you run the command in another terminal.","title":"Deployment Guide"},{"location":"installation/deployment.html#deployment-guide","text":"Before running helm install , you need to make some decisions about how you will run Codacy. This guide will cover the required values and common options.","title":"Deployment Guide"},{"location":"installation/deployment.html#tldr","text":"Quickly install Codacy for demo without any persistence. kubectl create namespace codacy kubectl create secret docker-registry docker-credentials --docker-username = $DOCKER_USERNAME --docker-password = $DOCKER_PASSWORD --namespace codacy export SHARED_PLAY_CRYPTO_SECRET = $( openssl rand -base64 32 | tr -dc 'a-zA-Z0-9' ) echo \"Store this secret: $SHARED_PLAY_CRYPTO_SECRET \" export CODACY_URL = \"http://codacy.example.com\" helm repo add codacy-stable https://charts.codacy.com/stable/ helm repo update helm upgrade --install codacy codacy-stable/codacy \\ --namespace codacy \\ --set global.imagePullSecrets [ 0 ] .name = docker-credentials \\ --set global.play.cryptoSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.contentsSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.filestore.uuidSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.cacheSecret = ${ SHARED_PLAY_CRYPTO_SECRET } \\ --set global.codacy.url = ${ CODACY_URL } \\ --set global.codacy.backendUrl = ${ CODACY_URL } \\ --set codacy-api.service.type = \"LoadBalancer\" If you want to enable persistence please config","title":"TL;DR"},{"location":"installation/deployment.html#selecting-configuration-options","text":"In each section collect the options that will be combined to use with helm install .","title":"Selecting configuration options"},{"location":"installation/deployment.html#secrets","text":"Some secrets need to be created (eg. cryptosecret) Also, some Codacy images are currently private. For this, you need to create a secret in the same namespace were you will install Codacy. You should receive these credentials together with your license. kubectl create secret docker-registry docker-credentials --docker-username=$DOCKER_USERNAME --docker-password=$DOCKER_PASSWORD --namespace $NAMESPACE","title":"Secrets"},{"location":"installation/deployment.html#monitoring-the-deployment","text":"This will output the list of resources installed once the deployment finishes which may take 5-10 minutes. The status of the deployment can be checked by running helm status codacy which can also be done while the deployment is taking place if you run the command in another terminal.","title":"Monitoring the Deployment"},{"location":"installation/upgrade.html","text":"Upgrade Guide \u00b6 NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values Steps \u00b6 The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Upgrade Guide"},{"location":"installation/upgrade.html#upgrade-guide","text":"NOTE: Note: You can retrieve your previous --set arguments cleanly, with helm get values <release name> . If you direct this into a file ( helm get values <release name> > codacy.yaml ), you can safely pass this file via -f . Thus helm upgrade codacy codacy/codacy -f codacy.yaml . This safely replaces the behavior of --reuse-values","title":"Upgrade Guide"},{"location":"installation/upgrade.html#steps","text":"The following are the steps to upgrade Codacy to a newer version: Extract your previous --set arguments with helm get values codacy > codacy.yaml Decide on all the values you need to set Perform the upgrade, with all --set arguments extracted in step 2 helm upgrade codacy codacy/codacy \\ -f codacy.yaml \\ --set ...","title":"Steps"},{"location":"migration/database.html","text":"Database Migration Guide \u00b6 Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results results201709 Requirements \u00b6 The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases. Dumping your current data out of a running Postgres \u00b6 You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password. pg_dump \u00b6 The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation. pg_restore \u00b6 To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar For more information and additional options, please check the official documentation. Sample script \u00b6 Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results results201709 ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" PGPASSWORD= $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Database Migration Guide"},{"location":"migration/database.html#database-migration-guide","text":"Migrating databases between pods is a straightforward process with 3 steps: Dump the databases to a dump file. Apply the dump file. Delete the dump file. You will have to dump all the following databases: accounts analysis filestore jobs metrics results results201709","title":"Database Migration Guide"},{"location":"migration/database.html#requirements","text":"The following operations must be executed by a user which has elevated access ( SUPERUSER ) in the Postgres databases.","title":"Requirements"},{"location":"migration/database.html#dumping-your-current-data-out-of-a-running-postgres","text":"You will need to know the following: $HOSTNAME - the hostname where the database is located. $DB_USER - the username with privileged access to the database that will perform the dump. $DB - the database that you would like to export. $DB_PASSWORD - the database password.","title":"Dumping your current data out of a running Postgres"},{"location":"migration/database.html#pg_dump","text":"The following command lets you extract a given database into a dump file: PGPASSWORD = $DB_PASSWORD pg_dump -h $HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" This will dump the file with the .sql.tar extension into the /tmp folder. For more information and additional options, please check the official documentation.","title":"pg_dump"},{"location":"migration/database.html#pg_restore","text":"To restore a database, you can run a pg_restore command to consume the dump file and replicate the data onto Postgres: PGPASSWORD = $DB_PASSWORD pg_restore -h $HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar For more information and additional options, please check the official documentation.","title":"pg_restore"},{"location":"migration/database.html#sample-script","text":"Assuming you have the same $DB_USER and $DB_PASSWORD , and that you want to migrate all the databases from the same hostname to the same destination hostname, you could easily migrate your databases with the following sample script: SRC_HOSTNAME = $1 DEST_HOSTNAME = $2 DB_USER = $3 DB_PASSWORD = $4 declare -a dbs =( accounts analysis filestore jobs metrics results results201709 ) for db in ${ dbs [@] } do PGPASSWORD = $DB_PASSWORD pg_dump -h $SRC_HOSTNAME -U $DB_USER -d $DB -F t -f /tmp/ $DB .sql.tar \" PGPASSWORD= $DB_PASSWORD pg_restore -h $DEST_HOSTNAME -U $DB_USER -d $DB -F t /tmp/ $DB .sql.tar done You could simply invoke it with: migrateDBs.sh postgres\u2013instance1.us-east-1.rds.amazonaws.com postgres\u2013instance1.eu-west-1.rds.amazonaws.com super_user secret_password","title":"Sample script"},{"location":"quickstart/README.html","text":"Codacy quickstart \u00b6 Quickstart includes a set of templates which will help you create a properly configured kubernetes cluster, ready to run codacy.","title":"Codacy quickstart"},{"location":"quickstart/README.html#codacy-quickstart","text":"Quickstart includes a set of templates which will help you create a properly configured kubernetes cluster, ready to run codacy.","title":"Codacy quickstart"},{"location":"quickstart/aks_quickstart.html","text":"AKS cluster quickstart \u00b6 Setup the Azure CLI \u00b6 https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code. Create the backend for Terraform \u00b6 https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ). Create a cluster \u00b6 To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster Setup \u00b6 To create run: cd setup/ terraform init && terraform apply","title":"AKS cluster quickstart"},{"location":"quickstart/aks_quickstart.html#aks-cluster-quickstart","text":"","title":"AKS cluster quickstart"},{"location":"quickstart/aks_quickstart.html#setup-the-azure-cli","text":"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest for mac brew install azure-cli login with: az login --use-device-code this is more stable than not using device code.","title":"Setup the Azure CLI"},{"location":"quickstart/aks_quickstart.html#create-the-backend-for-terraform","text":"https://www.terraform.io/docs/backends/index.html To create the backend run: cd backend/ terraform init && terraform apply Save the output data and use it to configure the backends of the main/ and setup/ stacks (in config.tf ).","title":"Create the backend for Terraform"},{"location":"quickstart/aks_quickstart.html#create-a-cluster","text":"To create run: cd backend/ terraform init && terraform apply get kubeconfig: az aks get-credentials --resource-group codacy-cluster --name codacy-aks-cluster","title":"Create a cluster"},{"location":"quickstart/aks_quickstart.html#setup","text":"To create run: cd setup/ terraform init && terraform apply","title":"Setup"},{"location":"quickstart/eks_quickstart.html","text":"EKS cluster setup using terraform \u00b6 This folder includes the terraform template needed to create an EKS cluster from scratch, including all the necessary underlying infrastructure. It includes the following infrastructure stacks: backend - (optional) the S3 bucket for storing the terraform state and the DynamoDB table for state locking. main - the EKS cluster, including all the network and nodes setup needed to go from zero to a fully functional EKS cluster. setup - additional setup you must perform prior to installing codacy on your vanilla EKS cluster. Requirements \u00b6 In order to setup the infrastructure you'll need recent versions of: awscli terraform kubectl helm Please follow the documentation in the above links to setup these tools for your OS. They can usually be easily installed using your package manager. For instance, on macOS, if you are using Homebrew , you can just run: brew install awscli terraform kubernetes-helm kubectl You'll also need to setup the CLI credentials for your AWS account. See how to do it the AWS and Terraform documentation. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly. Deployment \u00b6 1. backend - setup terraform state storage \u00b6 The backend stores the current and historical state of your infrastructure. Deploying and using it is optional, but it might make your life easier if you are planning to use these templates to make modifications to the cluster in the future. To deploy it run: terraform init && terraform apply inside the backend/ folder and follow terraform's instructions. An S3 bucket with an unique name to save your state will be created. Note this bucket's name and set it on the config.tf file of the main/ and setup/ stacks where indicated. 2. main - create a vanilla EKS cluster \u00b6 To create a cluster, along with all the necessary network and nodes setup it requires, run: terraform init && terraform apply inside the main/ folder and follow terraform's instructions. This takes a while (~10min). The cluster configuration (e.g., type/number of nodes, network CIDRs,...) are exposed as variables in the variables.tf file. You may tailor the cluster to your needs by editing the defaults that file, by using CLI options, viz. terraform apply -var = \"some_key=some_value\" or by writing them on a file named terraform.tfvars in the same folder as the infrastructure code, which is loaded by default when you run apply. For instance: some_key = \"a_string_value\" another_key = 3 someting_else = true To connect to the cluster get its kubeconfig and set it the as default context with: aws eks update-kubeconfig --name codacy-cluster If you now run kubectl get pods -A you'll see that nothing is scheduled. That's because you haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more info). We'll do that on step 3, but if you want to stop here with a vanilla EKS cluster you can run terraform output aws_auth_configmap | kubectl apply -f - to create the necessary aws-auth kubernetes ConfigMap. See the nodes' status with kubectl get nodes 3. setup - configure your EKS cluster to deploy codacy \u00b6 Some additional setup is necessary to run Codacy on the cluster you just created. For one, you need to allow workers to join the cluster (If you did it manually in the previous step run kubectl delete configmaps aws-auth -n kube-system to clean it up). Docker pull secrets need to be added to the codacy namespace, which will also be added here, and the following helm charts will be installed: kubernetes-dashboard , nginx-ingress , and cert-manager . To do it run terraform init && terraform apply You'll be prompted to input Codacy's docker hub repo password. If you'd like to connect to the kubernetes dashboard, get the admin token by running terraform output admin_token and copy the outputed value. To connect to the dashboard run kubectl proxy and then connect to the dashboard url , select token and paste the value you saved above. Installing Codacy \u00b6 To install Codacy please see the installation documentation . Uninstalling \u00b6 To remove the infrastructure created by these stacks as well as Codacy you have to follow these steps: WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS 1. Remove Codacy \u00b6 To ensure a clean removal you should uninstall Codacy before cleaning destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODs to delte:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' 2. Remove the cluster setup required to install Codacy \u00b6 To cleanup your cluster back to a vanilla state you can now run, in the setup/ folder, the following command terraform destroy 3. Remove the cluster \u00b6 After removing all the above stacks and setup, you may now delete the kubernetes cluster by running: terraform destroy in the main/ directory. This takes a while (~10min). 4. Removing the terraform backend \u00b6 If you created the terraform backend with the above stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly the easiest path is to disable these extra settings. Go to the backend/ folder and change the state_and_lock.tf file as instructed therein. Afterwards, you can now destroy it by running terraform apply && terraform destroy Note that you first have to apply to change the bucket settings, and only then will destroy work. TL;DR \u00b6 Seting up an EKS cluster for Codacy \u00b6 Assuming AWS is well configured and you fulfill all the prerequisites, the setup without state storage, can be done with: export AWS_SDK_LOAD_CONFIG = 1 cd main/ terraform init && terraform apply cd ../setup/ terraform init && terraform apply aws eks update-kubeconfig --name codacy-cluster Uninstalling Codacy and destroying the EKS cluster \u00b6 WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS Assuming you are not using the optional terraform backend, you can uninstall Codacy and remove the cluster by running: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 cd setup/ terraform destroy cd ../main terraform destroy","title":"EKS cluster setup using terraform"},{"location":"quickstart/eks_quickstart.html#eks-cluster-setup-using-terraform","text":"This folder includes the terraform template needed to create an EKS cluster from scratch, including all the necessary underlying infrastructure. It includes the following infrastructure stacks: backend - (optional) the S3 bucket for storing the terraform state and the DynamoDB table for state locking. main - the EKS cluster, including all the network and nodes setup needed to go from zero to a fully functional EKS cluster. setup - additional setup you must perform prior to installing codacy on your vanilla EKS cluster.","title":"EKS cluster setup using terraform"},{"location":"quickstart/eks_quickstart.html#requirements","text":"In order to setup the infrastructure you'll need recent versions of: awscli terraform kubectl helm Please follow the documentation in the above links to setup these tools for your OS. They can usually be easily installed using your package manager. For instance, on macOS, if you are using Homebrew , you can just run: brew install awscli terraform kubernetes-helm kubectl You'll also need to setup the CLI credentials for your AWS account. See how to do it the AWS and Terraform documentation. Note that, as stated in the documentation, if your .aws / credentials are fairly complex you might need to set AWS_SDK_LOAD_CONFIG=1 for Terraform to work correctly.","title":"Requirements"},{"location":"quickstart/eks_quickstart.html#deployment","text":"","title":"Deployment"},{"location":"quickstart/eks_quickstart.html#1-backend-setup-terraform-state-storage","text":"The backend stores the current and historical state of your infrastructure. Deploying and using it is optional, but it might make your life easier if you are planning to use these templates to make modifications to the cluster in the future. To deploy it run: terraform init && terraform apply inside the backend/ folder and follow terraform's instructions. An S3 bucket with an unique name to save your state will be created. Note this bucket's name and set it on the config.tf file of the main/ and setup/ stacks where indicated.","title":"1. backend - setup terraform state storage"},{"location":"quickstart/eks_quickstart.html#2-main-create-a-vanilla-eks-cluster","text":"To create a cluster, along with all the necessary network and nodes setup it requires, run: terraform init && terraform apply inside the main/ folder and follow terraform's instructions. This takes a while (~10min). The cluster configuration (e.g., type/number of nodes, network CIDRs,...) are exposed as variables in the variables.tf file. You may tailor the cluster to your needs by editing the defaults that file, by using CLI options, viz. terraform apply -var = \"some_key=some_value\" or by writing them on a file named terraform.tfvars in the same folder as the infrastructure code, which is loaded by default when you run apply. For instance: some_key = \"a_string_value\" another_key = 3 someting_else = true To connect to the cluster get its kubeconfig and set it the as default context with: aws eks update-kubeconfig --name codacy-cluster If you now run kubectl get pods -A you'll see that nothing is scheduled. That's because you haven't yet allowed the worker nodes to join the cluster (see the EKS docs for more info). We'll do that on step 3, but if you want to stop here with a vanilla EKS cluster you can run terraform output aws_auth_configmap | kubectl apply -f - to create the necessary aws-auth kubernetes ConfigMap. See the nodes' status with kubectl get nodes","title":"2. main - create a vanilla EKS cluster"},{"location":"quickstart/eks_quickstart.html#3-setup-configure-your-eks-cluster-to-deploy-codacy","text":"Some additional setup is necessary to run Codacy on the cluster you just created. For one, you need to allow workers to join the cluster (If you did it manually in the previous step run kubectl delete configmaps aws-auth -n kube-system to clean it up). Docker pull secrets need to be added to the codacy namespace, which will also be added here, and the following helm charts will be installed: kubernetes-dashboard , nginx-ingress , and cert-manager . To do it run terraform init && terraform apply You'll be prompted to input Codacy's docker hub repo password. If you'd like to connect to the kubernetes dashboard, get the admin token by running terraform output admin_token and copy the outputed value. To connect to the dashboard run kubectl proxy and then connect to the dashboard url , select token and paste the value you saved above.","title":"3. setup - configure your EKS cluster to deploy codacy"},{"location":"quickstart/eks_quickstart.html#installing-codacy","text":"To install Codacy please see the installation documentation .","title":"Installing Codacy"},{"location":"quickstart/eks_quickstart.html#uninstalling","text":"To remove the infrastructure created by these stacks as well as Codacy you have to follow these steps: WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS","title":"Uninstalling"},{"location":"quickstart/eks_quickstart.html#1-remove-codacy","text":"To ensure a clean removal you should uninstall Codacy before cleaning destroying the cluster. To do so run: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 Note that the deletion of pvc s in the above command has to run in the background due to a cyclic dependency in one of the components. If you are unsure of these commands' effect please run the each of the bash subcommands and validate their output, viz. echo \"PVCs to delete:\" kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' echo \"PODs to delte:\" kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}'","title":"1. Remove Codacy"},{"location":"quickstart/eks_quickstart.html#2-remove-the-cluster-setup-required-to-install-codacy","text":"To cleanup your cluster back to a vanilla state you can now run, in the setup/ folder, the following command terraform destroy","title":"2. Remove the cluster setup required to install Codacy"},{"location":"quickstart/eks_quickstart.html#3-remove-the-cluster","text":"After removing all the above stacks and setup, you may now delete the kubernetes cluster by running: terraform destroy in the main/ directory. This takes a while (~10min).","title":"3. Remove the cluster"},{"location":"quickstart/eks_quickstart.html#4-removing-the-terraform-backend","text":"If you created the terraform backend with the above stack you can now safely delete it. The backend is purposely created with extra settings to prevent its accidental destruction. To destroy it cleanly the easiest path is to disable these extra settings. Go to the backend/ folder and change the state_and_lock.tf file as instructed therein. Afterwards, you can now destroy it by running terraform apply && terraform destroy Note that you first have to apply to change the bucket settings, and only then will destroy work.","title":"4. Removing the terraform backend"},{"location":"quickstart/eks_quickstart.html#tldr","text":"","title":"TL;DR"},{"location":"quickstart/eks_quickstart.html#seting-up-an-eks-cluster-for-codacy","text":"Assuming AWS is well configured and you fulfill all the prerequisites, the setup without state storage, can be done with: export AWS_SDK_LOAD_CONFIG = 1 cd main/ terraform init && terraform apply cd ../setup/ terraform init && terraform apply aws eks update-kubeconfig --name codacy-cluster","title":"Seting up an EKS cluster for Codacy"},{"location":"quickstart/eks_quickstart.html#uninstalling-codacy-and-destroying-the-eks-cluster","text":"WARNING: IF YOU PROCEED BEYOND THIS POINT YOU'LL PERMANENTLY DELETE AND BREAK THINGS Assuming you are not using the optional terraform backend, you can uninstall Codacy and remove the cluster by running: helm del --purge codacy kubectl delete pvc -n codacy $( kubectl get pvc -n codacy -o jsonpath = '{.items[*].metadata.name}' ) & kubectl delete pods -n codacy $( kubectl get pods -n codacy -o jsonpath = '{.items[*].metadata.name}' ) --force --grace-period = 0 cd setup/ terraform destroy cd ../main terraform destroy","title":"Uninstalling Codacy and destroying the EKS cluster"},{"location":"requirements/index.html","text":"Requirements \u00b6 If you want to deploy Codacy on Kubernetes, the following requirements must be met: kubectl 1.13 or higher, compatible with your cluster ( +/- 1 minor release from your cluster ). Helm 2.14> <3.0 A Kubernetes cluster, between version 1.13 and 1.15. 16vCPU and 64GB of RAM is recommended. Nginx ingress (or some other ingress controller) CertManager (if you want to setup https) Postgres database ( https://support.codacy.com/hc/en-us/articles/360002902573-Installing-postgres-for-Codacy-Enterprise ) Resources \u00b6 Requirements per analysis \u00b6 In order to have a good experience with Codacy, you should have the following calculations in mind when setting the number of concurrent analysis. Each analysis runs a maximum number 4 plugins in parallel (not configurable) Note: All the following configurations are nexted inside the worker-manager.config configutation object, but for simplicity sake we decided to omit the full path. CPU \u00b6 workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory \u00b6 workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis \u00b6 workers.genericMax + workers.dedicatedMax Total resources \u00b6 Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis. Example \u00b6 Maximum of 6 concurrent analysis \u00b6 worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB","title":"Requirements"},{"location":"requirements/index.html#requirements","text":"If you want to deploy Codacy on Kubernetes, the following requirements must be met: kubectl 1.13 or higher, compatible with your cluster ( +/- 1 minor release from your cluster ). Helm 2.14> <3.0 A Kubernetes cluster, between version 1.13 and 1.15. 16vCPU and 64GB of RAM is recommended. Nginx ingress (or some other ingress controller) CertManager (if you want to setup https) Postgres database ( https://support.codacy.com/hc/en-us/articles/360002902573-Installing-postgres-for-Codacy-Enterprise )","title":"Requirements"},{"location":"requirements/index.html#resources","text":"","title":"Resources"},{"location":"requirements/index.html#requirements-per-analysis","text":"In order to have a good experience with Codacy, you should have the following calculations in mind when setting the number of concurrent analysis. Each analysis runs a maximum number 4 plugins in parallel (not configurable) Note: All the following configurations are nexted inside the worker-manager.config configutation object, but for simplicity sake we decided to omit the full path.","title":"Requirements per analysis"},{"location":"requirements/index.html#cpu","text":"workerResources.requests.cpu + (pluginResources.requests.cpu * 4)","title":"CPU"},{"location":"requirements/index.html#memory","text":"workerResources.requests.memory + (pluginResources.requests.memory * 4)","title":"Memory"},{"location":"requirements/index.html#number-of-concurrent-analysis","text":"workers.genericMax + workers.dedicatedMax","title":"Number of concurrent analysis"},{"location":"requirements/index.html#total-resources","text":"Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis.","title":"Total resources"},{"location":"requirements/index.html#example","text":"","title":"Example"},{"location":"requirements/index.html#maximum-of-6-concurrent-analysis","text":"worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB","title":"Maximum of 6 concurrent analysis"},{"location":"requirements/resources.html","text":"Resource requirements \u00b6 In order to have a good experience with Codacy, you should have the following calculations in mind when setting the number of concurrent analysis. Note: All the following configurations are nexted inside the worker-manager.config configutation object, but for simplicity sake we decided to omit the full path. Requirements per analysis \u00b6 Each analysis runs a maximum number 4 plugins in parallel (not configurable) CPU \u00b6 workerResources.requests.cpu + (pluginResources.requests.cpu * 4) Memory \u00b6 workerResources.requests.memory + (pluginResources.requests.memory * 4) Number of concurrent analysis \u00b6 workers.genericMax + workers.dedicatedMax Total resources \u00b6 Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis. Example \u00b6 Maximum of 6 concurrent analysis \u00b6 worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB","title":"Resource requirements"},{"location":"requirements/resources.html#resource-requirements","text":"In order to have a good experience with Codacy, you should have the following calculations in mind when setting the number of concurrent analysis. Note: All the following configurations are nexted inside the worker-manager.config configutation object, but for simplicity sake we decided to omit the full path.","title":"Resource requirements"},{"location":"requirements/resources.html#requirements-per-analysis","text":"Each analysis runs a maximum number 4 plugins in parallel (not configurable)","title":"Requirements per analysis"},{"location":"requirements/resources.html#cpu","text":"workerResources.requests.cpu + (pluginResources.requests.cpu * 4)","title":"CPU"},{"location":"requirements/resources.html#memory","text":"workerResources.requests.memory + (pluginResources.requests.memory * 4)","title":"Memory"},{"location":"requirements/resources.html#number-of-concurrent-analysis","text":"workers.genericMax + workers.dedicatedMax","title":"Number of concurrent analysis"},{"location":"requirements/resources.html#total-resources","text":"Given the previous values, the total number of resources required should be the \"per-analysis\" amount times the number of concurrent analysis.","title":"Total resources"},{"location":"requirements/resources.html#example","text":"","title":"Example"},{"location":"requirements/resources.html#maximum-of-6-concurrent-analysis","text":"worker-manager : config : workers : genericMax : 3 dedicatedMax : 3 workerResources : limits : cpu : 1 memory : \"2Gi\" pluginResources : requests : cpu : 0.5 memory : 1000000000 # 1GB In this example the minimum number of resources required would be: CPU: 6 * (1 + (0.5 * 4)) = 18 CPU Memory: 6 * (2 + (1 * 4)) = 36 GB","title":"Maximum of 6 concurrent analysis"}]}